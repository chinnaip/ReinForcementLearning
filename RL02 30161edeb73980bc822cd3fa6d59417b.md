# RL02

WEBVTT

1
00:19:31.175 –> 00:19:32.225
Good afternoon, sir.

2
00:19:33.265 –> 00:19:37.905
Good afternoon. This, uh, good afternoon sir.

3
00:19:37.935 –> 00:19:41.715
Good afternoon. Yeah, good afternoon sir.

4
00:19:47.015 –> 00:19:49.475
So last plus I did not share to you the code.

5
00:19:49.525 –> 00:19:50.715
Would I share it to you?

6
00:19:51.425 –> 00:19:54.815
That is for, uh, ULA

7
00:19:56.005 –> 00:19:59.415
algorithm four reinforcement policy

8
00:19:59.495 –> 00:20:02.185
gradient today.

9
00:20:02.535 –> 00:20:04.025
Very little topic I covered

10
00:20:04.025 –> 00:20:05.545
that is called actor critic method.

11
00:20:06.455 –> 00:20:09.685
After that, the last topic is going to be, um, next week

12
00:20:10.235 –> 00:20:12.485
it’s called proximal policy optimization.

13
00:20:15.405 –> 00:20:18.585
So that will be like one week.

14
00:20:20.975 –> 00:20:25.675
So by next week I will complete the course.

15
00:20:27.475 –> 00:20:29.285
Following that week, I’ll be complete.

16
00:20:29.485 –> 00:20:33.205
I will be giving you some kind of, uh, thing on that is on,

17
00:20:35.755 –> 00:20:38.685
next week is from four

18
00:20:39.145 –> 00:20:43.845
and so 4th of February next week will be 11th February.

19
00:20:43.905 –> 00:20:46.525
So 11th February or not, it’ll be just, I’ll show you some

20
00:20:46.525 –> 00:20:48.565
of the example of large language model where

21
00:20:49.165 –> 00:20:50.325
reinforcement learning is used.

22
00:20:50.715 –> 00:20:54.425
Okay, so open. Sure.

23
00:20:54.425 –> 00:20:59.015
So, uh, so basically you are getting bored, not getting any,

24
00:21:00.035 –> 00:21:03.695
uh, what is called uh, application of this.

25
00:21:05.915 –> 00:21:07.175
Let me share the screen.

26
00:21:30.915 –> 00:21:32.055
So that is your,

27
00:21:40.225 –> 00:21:41.645
so you are able to see the screen.

28
00:21:42.805 –> 00:21:47.545
Uh, I am into the li, yeah. Yes, sir. Okay.

29
00:21:48.855 –> 00:21:52.125
So last class I had shown you that how to

30
00:21:54.325 –> 00:21:56.065
use this reinforce algorithm.

31
00:21:56.375 –> 00:21:58.705
Okay? This is a manual algorithm for

32
00:21:59.375 –> 00:22:01.185
fast policy gradient method.

33
00:22:04.475 –> 00:22:08.295
So the method is for every policy gradient technique,

34
00:22:08.315 –> 00:22:11.215
we have to find out what is the objective function

35
00:22:11.215 –> 00:22:15.015
and then how to take the gradient of the objective function

36
00:22:15.015 –> 00:22:17.935
with respect to the parameters of the neural network.

37
00:22:19.215 –> 00:22:23.345
So the, the objective function is this expectation,

38
00:22:24.805 –> 00:22:25.775
expectation of

39
00:22:31.225 –> 00:22:36.215
your log probability

40
00:22:36.315 –> 00:22:39.255
of action given as multiplied by the reward.

41
00:22:39.395 –> 00:22:42.495
So reward, you can represent it in terms of Q value

42
00:22:43.035 –> 00:22:46.615
or you can represent it in terms of uh,

43
00:22:47.365 –> 00:22:49.495
just some of discounted reward also.

44
00:22:50.345 –> 00:22:53.205
Okay, so the gradient is going to be nothing

45
00:22:53.305 –> 00:22:57.505
but this expectation with respect to the gradient

46
00:22:57.505 –> 00:23:01.825
of log probability of A given S multiplied by the return.

47
00:23:01.905 –> 00:23:04.585
The return can be, it can be expected,

48
00:23:04.905 –> 00:23:05.985
somewhat discounted reward.

49
00:23:06.175 –> 00:23:11.035
Okay? So for granular reinforcement algorithm,

50
00:23:12.345 –> 00:23:15.885
you have to fast for each

51
00:23:15.885 –> 00:23:19.325
and every episode you have T capital T number of times step

52
00:23:20.775 –> 00:23:22.545
over which you are stepping through.

53
00:23:22.815 –> 00:23:25.585
That means you’re changing the states for every

54
00:23:26.115 –> 00:23:28.345
state transition, you are having some reward

55
00:23:29.915 –> 00:23:33.835
and you get that reward, okay?

56
00:23:33.975 –> 00:23:38.035
And you just multiply that reward

57
00:23:39.995 –> 00:23:43.925
with the gradient of law, probability of a givenness.

58
00:23:44.665 –> 00:23:49.615
So this lock prob of A given SI have shown you last

59
00:23:49.615 –> 00:23:51.535
class that it is a neural network

60
00:23:51.715 –> 00:23:53.735
by which you are basically sampling your

61
00:23:53.795 –> 00:23:55.095
action given a state.

62
00:23:56.005 –> 00:23:57.385
So whenever you are doing sampling,

63
00:23:58.525 –> 00:24:01.905
it is returning a probability of the action to be chosen.

64
00:24:02.765 –> 00:24:06.975
So from that probability, you can get a log probability of

65
00:24:06.975 –> 00:24:11.535
that and that log probability is alone taken here.

66
00:24:11.605 –> 00:24:15.255
This gradient has not been operated on. Okay?

67
00:24:15.635 –> 00:24:18.695
So this log probability they are taking, so

68
00:24:19.295 –> 00:24:20.495
whenever we are implementing, we have

69
00:24:20.495 –> 00:24:21.735
only implemented this log.

70
00:24:21.735 –> 00:24:26.695
Probability of action given is multiplied by the reward.

71
00:24:27.605 –> 00:24:30.975
Okay? And you multiply that with your gamma factor,

72
00:24:32.285 –> 00:24:35.935
keep doing submission for every timestamp that you take.

73
00:24:36.795 –> 00:24:40.055
So you get a total reward for all the timestamps.

74
00:24:40.685 –> 00:24:43.735
Then you use that value for your back progression.

75
00:24:46.605 –> 00:24:50.345
Okay? So I will show you both the program today.

76
00:24:51.285 –> 00:24:54.615
Then the, uh, today’s topic is act, duty,

77
00:24:55.115 –> 00:24:56.295
act duty policy.

78
00:24:57.565 –> 00:25:00.735
Okay? So there was a big, uh, problem in uh, case

79
00:25:00.755 –> 00:25:02.375
of reinforcement.

80
00:25:02.915 –> 00:25:05.135
So here is the reinforcement thing.

81
00:25:07.455 –> 00:25:11.705
The problem was there was lot

82
00:25:11.705 –> 00:25:13.985
of variance of the reward that you are getting.

83
00:25:14.825 –> 00:25:19.015
So I have already done it here, it’ll show you the value.

84
00:25:19.325 –> 00:25:22.095
Okay? So this is what the last plus I had shown you.

85
00:25:24.115 –> 00:25:27.855
So if you are going over so many episodes, look at

86
00:25:29.005 –> 00:25:33.665
your uh, reward that is expected.

87
00:25:33.695 –> 00:25:36.825
Some of discounted reward, which you are maximizing.

88
00:25:37.525 –> 00:25:39.545
It is having a lot of variance. It is going up.

89
00:25:39.935 –> 00:25:44.865
Look at if going up 1 55 going down again, again,

90
00:25:45.005 –> 00:25:47.755
up, down, down,

91
00:25:48.245 –> 00:25:50.425
going up, okay?

92
00:25:50.805 –> 00:25:54.825
At some position you have got 214, then you’re going down

93
00:25:55.365 –> 00:25:57.425
and suddenly at the episode of thousand,

94
00:25:57.425 –> 00:25:58.745
you are getting 450.

95
00:26:00.795 –> 00:26:02.615
So this is a lot of variance in there.

96
00:26:04.285 –> 00:26:08.365
Expected sum of discounted reward. So we need to improve it.

97
00:26:08.385 –> 00:26:11.805
So actor critic method, those who have found out, those

98
00:26:11.805 –> 00:26:13.965
who have invented that method, they said that we need

99
00:26:13.965 –> 00:26:15.645
to decrease this variance.

100
00:26:16.345 –> 00:26:20.085
So for that, the mechanism that they proposed was basically

101
00:26:22.435 –> 00:26:24.815
we need to go for two kind of neural net.

102
00:26:26.405 –> 00:26:28.745
One is called actor, other one is called critique.

103
00:26:28.815 –> 00:26:33.345
Just like Q learning, we have seen two Q2 network like that.

104
00:26:35.805 –> 00:26:39.905
So actor is basically the policy optimization,

105
00:26:41.025 –> 00:26:42.085
uh, parameters.

106
00:26:42.505 –> 00:26:45.445
So given a state, what is the action you want to take,

107
00:26:45.635 –> 00:26:47.725
that is the main job of your policy.

108
00:26:49.275 –> 00:26:53.685
And value function means given the state,

109
00:26:53.955 –> 00:26:58.685
what is the expected sum of discounted reward at

110
00:26:58.995 –> 00:27:00.165
that particular state?

111
00:27:00.785 –> 00:27:03.645
It doesn’t take, it doesn’t draw the value function.

112
00:27:03.645 –> 00:27:06.005
Do not talk about what action you must take

113
00:27:06.355 –> 00:27:07.485
with what probability.

114
00:27:08.075 –> 00:27:10.965
Okay? So value function talks about irrespective

115
00:27:10.985 –> 00:27:14.485
of any action that you take, it’ll do a total aging

116
00:27:14.485 –> 00:27:16.805
of all the action that you can take for every state,

117
00:27:17.265 –> 00:27:20.285
and then it’ll find out the total expected sum

118
00:27:20.285 –> 00:27:22.565
of discounted reward under all possible action

119
00:27:22.785 –> 00:27:23.965
for that particular state.

120
00:27:24.995 –> 00:27:26.735
So that is called the critique method.

121
00:27:27.055 –> 00:27:30.755
Critique method means it only, it is a expert.

122
00:27:30.775 –> 00:27:32.995
Uh, we consider that is to be expert.

123
00:27:33.095 –> 00:27:37.135
So if you have taken any action, suppose you are learning

124
00:27:37.845 –> 00:27:41.815
drawing, if you go to any drawing teacher

125
00:27:41.915 –> 00:27:46.165
to learn your drawing for first few days, he will train you.

126
00:27:46.165 –> 00:27:48.005
After that, he will give you some small task.

127
00:27:48.705 –> 00:27:52.605
And what happens to us is basically we

128
00:27:53.545 –> 00:27:54.925
do a little part of that task

129
00:27:54.985 –> 00:27:57.805
and then we ask the tutor

130
00:27:58.905 –> 00:28:01.635
whether it was good stroke or bad stroke like that.

131
00:28:02.365 –> 00:28:04.985
So the tutor will give you some kind of feedback, okay,

132
00:28:05.095 –> 00:28:07.625
this stroke was good, we carry on this,

133
00:28:07.625 –> 00:28:10.385
otherwise you erase this and do it again like that.

134
00:28:11.485 –> 00:28:12.945
So with the same philosophy,

135
00:28:12.945 –> 00:28:15.905
they have this thing called actor is the person

136
00:28:15.925 –> 00:28:20.385
who is learning and critic is a person who can evaluate

137
00:28:20.415 –> 00:28:21.705
what action you have taken.

138
00:28:23.485 –> 00:28:27.775
So the critic network will use hello function, okay?

139
00:28:28.245 –> 00:28:29.455
Some kind of hello function

140
00:28:29.635 –> 00:28:33.175
or click Q function, an actor network.

141
00:28:33.605 –> 00:28:36.655
They will use the policy gradient technique to learn

142
00:28:36.725 –> 00:28:40.215
what action do be taken for a particular state

143
00:28:40.555 –> 00:28:43.355
and what will be the probability for action, okay?

144
00:28:43.425 –> 00:28:46.235
That is, is leading over here. Okay?

145
00:28:47.365 –> 00:28:52.185
Now for doing that, again, the same algorithm is

146
00:28:55.155 –> 00:28:58.975
look at this part, this part remains same Azure

147
00:29:00.565 –> 00:29:03.205
reinforce algorithm of the ela ELA policy agreement.

148
00:29:03.315 –> 00:29:04.925
Okay? Like this part remains same

149
00:29:05.325 –> 00:29:07.095
because it is having a neural network

150
00:29:07.235 –> 00:29:09.535
by which you are just sampling an action.

151
00:29:10.155 –> 00:29:13.575
You take that sample value, take the lock prob of it,

152
00:29:13.975 –> 00:29:17.015
multiply that with it with a reward value,

153
00:29:17.015 –> 00:29:20.175
that reward you are taking from the queue function

154
00:29:20.175 –> 00:29:21.775
value, okay?

155
00:29:24.745 –> 00:29:26.675
Look at the difference between policy, uh,

156
00:29:26.825 –> 00:29:27.875
your reinforcement,

157
00:29:27.895 –> 00:29:30.595
and this is the reward that you’re taking here.

158
00:29:30.905 –> 00:29:33.315
This reward is the raw reward

159
00:29:33.315 –> 00:29:36.125
that you are getting from the environment, okay?

160
00:29:36.125 –> 00:29:38.525
And at every time step we can do the summation

161
00:29:41.705 –> 00:29:44.205
in case of active critic method, they’re saying that

162
00:29:45.905 –> 00:29:47.365
the reward that you’re going to take,

163
00:29:47.475 –> 00:29:50.085
that should come from the Q function network

164
00:29:50.225 –> 00:29:51.565
of the critic part.

165
00:29:52.965 –> 00:29:54.135
That is a difference there.

166
00:29:54.635 –> 00:29:57.765
Second thing is, so there is a two network.

167
00:29:57.985 –> 00:30:00.045
One is called policy, other one is a value network.

168
00:30:00.785 –> 00:30:01.805
So the policy network,

169
00:30:02.035 –> 00:30:04.925
they will use the reward coming from the value network

170
00:30:05.665 –> 00:30:08.365
and uh, your value network.

171
00:30:08.585 –> 00:30:11.645
Now value network, it is going to update

172
00:30:12.385 –> 00:30:14.245
its value function or queue function.

173
00:30:15.265 –> 00:30:16.765
So the update of queue function

174
00:30:16.765 –> 00:30:19.165
or value function, you already, you have done it using

175
00:30:20.565 –> 00:30:22.955
TD learning or Q learning, right?

176
00:30:23.015 –> 00:30:26.425
You have remembered those steps, okay?

177
00:30:26.845 –> 00:30:30.745
So use that formula for doing the update

178
00:30:30.965 –> 00:30:32.785
of your queue function weight.

179
00:30:33.885 –> 00:30:36.465
So that same, there is no new theory coming up.

180
00:30:37.695 –> 00:30:40.955
Now we are actually in a position to sum together all

181
00:30:40.955 –> 00:30:42.595
of things that you have learned for last

182
00:30:43.455 –> 00:30:47.965
or last, uh, that is beyond the model base.

183
00:30:47.985 –> 00:30:50.005
So now whatever we’re doing is model free.

184
00:30:50.705 –> 00:30:52.605
So in that you have learned already Q learning

185
00:30:52.745 –> 00:30:54.645
and your TD learning that that is this.

186
00:30:54.645 –> 00:30:56.525
This update is called TD learning.

187
00:30:57.185 –> 00:31:01.805
So you want to estimate the value of Q function

188
00:31:02.305 –> 00:31:04.125
or state and action payer.

189
00:31:04.705 –> 00:31:08.685
So what do you do? You take one random action from the new,

190
00:31:08.915 –> 00:31:11.885
from the present state, you go into new state.

191
00:31:12.255 –> 00:31:15.115
Under the new state, you are assuming that

192
00:31:16.185 –> 00:31:18.375
under the new state you may have

193
00:31:18.595 –> 00:31:20.735
so many different possibility of action for each

194
00:31:20.735 –> 00:31:23.455
and every possibility of you have already the old estimate

195
00:31:23.455 –> 00:31:25.815
of the queue, hello, use that.

196
00:31:26.045 –> 00:31:27.735
That is reward plus of

197
00:31:28.475 –> 00:31:31.055
new value coming from the queue function minus

198
00:31:31.115 –> 00:31:33.535
of your present value of this is basically TD learning

199
00:31:33.535 –> 00:31:35.615
applied on queue function.

200
00:31:36.655 –> 00:31:39.315
Use that. That is your error on this error.

201
00:31:39.415 –> 00:31:42.635
You are doing a back propagation. Okay?

202
00:31:43.435 –> 00:31:46.775
So the, the way we do back propagation

203
00:31:48.605 –> 00:31:51.865
for the mathematical writing part, we write that, okay,

204
00:31:52.285 –> 00:31:54.985
you update your weight with the previous weight plus a

205
00:31:55.865 –> 00:31:58.725
faster derivative of this Q function.

206
00:31:59.865 –> 00:32:01.285
But the first order derivative is

207
00:32:01.285 –> 00:32:02.405
not going to be available to you.

208
00:32:02.475 –> 00:32:04.325
What you do is just take the raw value

209
00:32:04.385 –> 00:32:08.185
of this Q function value, multiply that

210
00:32:08.185 –> 00:32:10.255
with the error.

211
00:32:10.605 –> 00:32:14.975
This error is the next state estimate the difference

212
00:32:14.975 –> 00:32:16.535
between next state estimate, current state,

213
00:32:17.245 –> 00:32:20.095
that area you multiply with your current value of Q

214
00:32:20.475 –> 00:32:24.595
and then you give it to your loss backward function.

215
00:32:24.905 –> 00:32:26.755
They’ll do the job, okay?

216
00:32:27.055 –> 00:32:30.475
You don’t have to do this manual faster derivative.

217
00:32:31.975 –> 00:32:35.575
So it is basically alternating update of two network.

218
00:32:36.195 –> 00:32:37.855
The first network is a policy network.

219
00:32:38.565 –> 00:32:40.045
When you update the policy network,

220
00:32:40.185 –> 00:32:41.845
you don’t update your value network.

221
00:32:42.425 –> 00:32:46.445
You use the value coming from the value network, okay?

222
00:32:46.445 –> 00:32:48.565
Then you completed your update of your policy network.

223
00:32:49.275 –> 00:32:50.645
Then you go to value network.

224
00:32:51.345 –> 00:32:52.805
And in the value network you apply

225
00:32:52.805 –> 00:32:53.845
the studio learning mechanism.

226
00:32:54.265 –> 00:32:56.045
You are, you are updating a value network

227
00:32:56.725 –> 00:32:58.685
estimate whatever value we are getting.

228
00:32:59.465 –> 00:33:01.245
You just keep it as it is.

229
00:33:01.465 –> 00:33:03.405
Use that value net towards value

230
00:33:03.465 –> 00:33:05.535
to update your policy network.

231
00:33:06.205 –> 00:33:09.305
This is the mechanism. Okay?

232
00:33:10.085 –> 00:33:11.985
Now I will show you the program,

233
00:33:19.075 –> 00:33:19.365
okay?

234
00:33:19.505 –> 00:33:21.365
So this is the program for your

235
00:33:25.495 –> 00:33:26.555
ELA imports algorithm.

236
00:33:27.115 –> 00:33:28.235
I will share it to you today.

237
00:33:30.915 –> 00:33:33.095
Let me just start it from there.

238
00:33:33.395 –> 00:33:36.735
Ela, after getting all of these,

239
00:33:40.115 –> 00:33:43.455
so you have a state, uh, you have a environment from

240
00:33:43.455 –> 00:33:46.295
where you’re getting the total number of states possible

241
00:33:47.275 –> 00:33:49.295
and for every state, what are the actions possible?

242
00:33:49.295 –> 00:33:51.575
There are two actions for your carpool environment.

243
00:33:51.955 –> 00:33:55.055
Either you go front or go back. Okay?

244
00:33:56.195 –> 00:33:57.975
So this is a vanilla algorithm.

245
00:33:58.045 –> 00:34:02.265
This is a policy class in which you are specifying.

246
00:34:02.635 –> 00:34:05.305
There are two linear layers, okay?

247
00:34:05.925 –> 00:34:09.825
The first one is taking into a hidden size. Hidden size.

248
00:34:09.885 –> 00:34:11.385
The second one is taking from hidden

249
00:34:11.455 –> 00:34:12.705
size to the action space.

250
00:34:14.145 –> 00:34:16.945
A size is basically what a size is your state space

251
00:34:17.535 –> 00:34:20.105
from the state space to the action space, action space.

252
00:34:20.105 –> 00:34:22.645
That mapping. So you have

253
00:34:22.675 –> 00:34:26.495
this function.

254
00:34:26.495 –> 00:34:29.805
After that you have a act function.

255
00:34:29.945 –> 00:34:32.205
Act function is basically the job

256
00:34:32.225 –> 00:34:35.325
of act function is if you initialize this policy class,

257
00:34:35.745 –> 00:34:40.005
the moment you call policy dot act, it’ll give you a sample

258
00:34:40.065 –> 00:34:42.055
of the action, okay?

259
00:34:42.055 –> 00:34:45.215
And the way it is done is it’ll ask for the state

260
00:34:45.555 –> 00:34:47.775
for which you want to know what action to be chosen.

261
00:34:48.515 –> 00:34:53.135
You take the state and give it to the forward function.

262
00:34:53.755 –> 00:34:56.015
Who, what you going to give that state?

263
00:34:56.625 –> 00:34:58.375
State you’re giving to forward function.

264
00:34:59.075 –> 00:35:00.135
We are going, the output

265
00:35:00.135 –> 00:35:03.175
of the forward function is basically the probability

266
00:35:03.275 –> 00:35:04.615
for every action possible.

267
00:35:05.405 –> 00:35:07.055
Okay? So it is returning a probability.

268
00:35:07.125 –> 00:35:10.055
Then you can use categorical to convert that into a

269
00:35:10.845 –> 00:35:12.535
softmax level categorization.

270
00:35:13.505 –> 00:35:17.165
So your aim is going to contain the probability, okay?

271
00:35:17.435 –> 00:35:19.525
From the probability, if you call this aim dot sample.

272
00:35:20.705 –> 00:35:23.525
So look at this aim is coming from categorical.

273
00:35:23.525 –> 00:35:24.605
What is categorical?

274
00:35:24.635 –> 00:35:27.725
Categorical is already defined to distribution.

275
00:35:28.025 –> 00:35:30.485
So under this class you have a categorical function.

276
00:35:30.865 –> 00:35:34.245
So naturally under categorical class also, you are going

277
00:35:34.245 –> 00:35:35.885
to have a sample function available.

278
00:35:36.645 –> 00:35:41.165
So this sample will give you sample of

279
00:35:42.115 –> 00:35:43.805
that distribution, one sample,

280
00:35:44.225 –> 00:35:45.725
and that sample is called action.

281
00:35:47.535 –> 00:35:51.115
Okay? And the moment in this categorical,

282
00:35:51.115 –> 00:35:52.195
there is another function called

283
00:35:52.255 –> 00:35:53.435
log under scope probability.

284
00:35:53.485 –> 00:35:55.675
It’ll return you the log probability of that action,

285
00:35:57.335 –> 00:35:59.525
which action, that action you’re getting from the sample.

286
00:36:00.855 –> 00:36:03.515
So this is we, we are going to call every time

287
00:36:05.315 –> 00:36:08.255
to query our neur network that if I give you a state,

288
00:36:08.345 –> 00:36:09.655
it’ll give you a action.

289
00:36:10.265 –> 00:36:12.415
Along with that, it’ll give a lock code of the action.

290
00:36:14.025 –> 00:36:15.375
There is the only class we have,

291
00:36:15.485 –> 00:36:17.055
then we have a redeem for S.

292
00:36:22.665 –> 00:36:26.245
So for inforce algorithm, we are accepting

293
00:36:27.025 –> 00:36:30.975
the policy class, a object of policy, class,

294
00:36:31.645 –> 00:36:33.575
optimizer, whatever optimizer you’re using.

295
00:36:33.635 –> 00:36:37.335
It may be Adam, it may be RMS, prop, anything, number

296
00:36:37.395 –> 00:36:38.415
of your episode

297
00:36:39.275 –> 00:36:41.855
and maximum number of timestamp within an episode.

298
00:36:41.855 –> 00:36:43.095
That is max hundred t.

299
00:36:43.825 –> 00:36:46.285
Then you have a discount factor, okay?

300
00:36:47.285 –> 00:36:49.825
So when you are within an episode, like

301
00:36:49.925 –> 00:36:53.835
inside this follow loop, you are going to have a list

302
00:36:53.835 –> 00:36:55.395
of rewards for every episode.

303
00:36:55.395 –> 00:36:57.995
You’re counting the reward. You start your

304
00:36:57.995 –> 00:37:00.115
game at this state.

305
00:37:01.945 –> 00:37:04.595
Now you are going to do the stepping through

306
00:37:04.595 –> 00:37:07.075
what all the states inside episode, okay?

307
00:37:08.155 –> 00:37:09.855
So when you’re doing stepping through,

308
00:37:10.275 –> 00:37:12.215
you are first giving one state,

309
00:37:12.875 –> 00:37:15.975
we want the action coming out from your policy, okay?

310
00:37:15.975 –> 00:37:20.825
So you need this policy to attack to give you one

311
00:37:21.445 –> 00:37:22.865
action from the policy.

312
00:37:23.195 –> 00:37:27.045
Along with that, it’ll give you a law probability, okay?

313
00:37:28.925 –> 00:37:31.215
This law probability, you are basically appending.

314
00:37:31.445 –> 00:37:33.615
This is a least law. Probability is a least.

315
00:37:35.405 –> 00:37:37.275
Sorry, this lock prob is not a list.

316
00:37:37.275 –> 00:37:40.115
This is a single fellow probability fellow. This is a list.

317
00:37:41.185 –> 00:37:43.745
This list you are appending just

318
00:37:43.825 –> 00:37:46.465
to store all the lock property for

319
00:37:47.005 –> 00:37:48.865
the state action period that you have chosen.

320
00:37:50.265 –> 00:37:51.275
Then you’re stepping through,

321
00:37:51.275 –> 00:37:53.075
you’re getting a new state reward

322
00:37:53.095 –> 00:37:57.005
and all, if the new state is going to be termination state,

323
00:37:57.115 –> 00:38:00.595
then you are breaking out, okay?

324
00:38:00.735 –> 00:38:03.555
If not, then you keep on adding the reward

325
00:38:03.575 –> 00:38:08.275
and all This is

326
00:38:08.275 –> 00:38:10.675
how we are calculating your, you are maintaining a queue

327
00:38:10.695 –> 00:38:14.005
for the returns, okay?

328
00:38:14.665 –> 00:38:15.525
And for that,

329
00:38:21.675 –> 00:38:24.855
The return, uh, it is now a

330
00:38:25.435 –> 00:38:26.905
queue, okay?

331
00:38:27.565 –> 00:38:28.745
For a particular episode.

332
00:38:30.195 –> 00:38:33.375
So for every time step you take, you are going

333
00:38:33.375 –> 00:38:36.125
to have a reward coming up at that reward.

334
00:38:36.125 –> 00:38:40.825
You are basically attaching every time, okay?

335
00:38:42.315 –> 00:38:44.165
This is only initialization happening.

336
00:38:44.165 –> 00:38:47.965
After that, you take this, multiply it with the gamma,

337
00:38:47.965 –> 00:38:52.935
and then attaching at every timestamp, okay?

338
00:38:52.935 –> 00:38:54.495
So that is how you store your reward.

339
00:38:55.155 –> 00:38:58.295
So your reward, this return is basically is going to have

340
00:38:58.995 –> 00:39:02.125
capital T number of element for every element.

341
00:39:02.185 –> 00:39:06.085
It is having the return for that times step alone.

342
00:39:06.425 –> 00:39:09.125
And for every times step, you are going to do this,

343
00:39:09.755 –> 00:39:11.205
this operation, okay?

344
00:39:12.205 –> 00:39:16.445
I will show you for every time step, for every times step,

345
00:39:16.445 –> 00:39:17.845
we are going to do this operation

346
00:39:20.365 –> 00:39:23.905
and this update of your weight parameter by the help

347
00:39:23.925 –> 00:39:27.105
of the reward that is in available to you at

348
00:39:27.105 –> 00:39:29.905
that particular timestamp, multiply

349
00:39:30.855 –> 00:39:32.345
with the log prob of action.

350
00:39:32.485 –> 00:39:34.305
So this gradient will not be there, okay?

351
00:39:34.305 –> 00:39:35.865
In the, in the program.

352
00:39:38.845 –> 00:39:41.145
So you are taking this return value, you are,

353
00:39:41.145 –> 00:39:42.185
you are making the return value.

354
00:39:42.245 –> 00:39:43.705
Now we are going to reuse them

355
00:39:43.985 –> 00:39:45.185
whenever we are going to do the

356
00:39:48.145 –> 00:39:49.635
loss function update.

357
00:39:50.895 –> 00:39:52.035
So this return

358
00:39:53.705 –> 00:39:54.895
after that, what they’re doing

359
00:39:54.915 –> 00:39:57.685
to make it more stabilized, okay?

360
00:39:57.685 –> 00:40:00.605
They, they’re reducing stable, uh, reducing instability.

361
00:40:00.755 –> 00:40:04.045
Look at their to avoid instability.

362
00:40:04.595 –> 00:40:08.555
What they’re doing, this return is basically your uh,

363
00:40:09.375 –> 00:40:11.155
add for every time step.

364
00:40:11.155 –> 00:40:14.275
What is the return you are having? So it is now normalizing.

365
00:40:14.275 –> 00:40:17.915
It. Normalizing means every element will be subtracted from

366
00:40:17.915 –> 00:40:19.155
the average, average value divided

367
00:40:19.255 –> 00:40:21.635
by this standard deviation.

368
00:40:22.135 –> 00:40:24.435
So every element will become now normalized,

369
00:40:25.095 –> 00:40:27.115
so no one will have too much fluctuation.

370
00:40:28.035 –> 00:40:30.775
So it is just a normalized step here.

371
00:40:30.845 –> 00:40:33.665
This step after that, we are having this return.

372
00:40:33.695 –> 00:40:37.645
This is now added for total number of timestamp.

373
00:40:37.645 –> 00:40:40.475
These are the values of your return, okay?

374
00:40:40.655 –> 00:40:44.395
Now, once you have this return available, this to you,

375
00:40:45.155 –> 00:40:47.605
then you have this save lock probability

376
00:40:47.635 –> 00:40:50.605
that you are getting from here, save lock probability.

377
00:40:54.245 –> 00:40:56.135
Here you have saving the lock probability.

378
00:40:56.505 –> 00:40:58.575
Again, this is also added, okay?

379
00:40:58.605 –> 00:41:01.495
This area stands for lock probability.

380
00:41:01.555 –> 00:41:04.455
At every individual timestamp you have total

381
00:41:04.455 –> 00:41:05.975
capital T number of timestamp.

382
00:41:06.205 –> 00:41:08.255
Okay? So now we have two areas.

383
00:41:08.395 –> 00:41:11.175
One area is, so lock probability, other one is return area,

384
00:41:11.275 –> 00:41:13.495
and all of these two areas are having

385
00:41:13.495 –> 00:41:14.895
capital T number of element.

386
00:41:15.795 –> 00:41:18.005
Each element stands for lock probability

387
00:41:18.345 –> 00:41:19.525
and corresponding return.

388
00:41:19.705 –> 00:41:21.085
And next element will be lock

389
00:41:21.085 –> 00:41:22.325
probability, corresponding return.

390
00:41:23.275 –> 00:41:24.725
Just we want to do this job.

391
00:41:24.905 –> 00:41:29.465
Now, this part, so a return multiplied by lock probability

392
00:41:29.465 –> 00:41:30.465
of action, givenness.

393
00:41:30.565 –> 00:41:33.045
So this will be going on and,

394
00:41:33.155 –> 00:41:37.165
and then element by element,

395
00:41:37.605 –> 00:41:38.685
multiplication and addition.

396
00:41:38.685 –> 00:41:41.765
Because for all the state action where I want

397
00:41:41.765 –> 00:41:42.845
to find out the total

398
00:41:45.865 –> 00:41:46.905
multiplied value, okay?

399
00:41:48.085 –> 00:41:50.465
So this, this, I’m not going to do the do this

400
00:41:50.485 –> 00:41:52.105
for at every time step this update.

401
00:41:55.195 –> 00:41:58.285
Now coming back here, so these are the two

402
00:41:58.445 –> 00:41:59.685
areas you have already stored.

403
00:41:59.785 –> 00:42:02.125
One is for your log probability, other one is

404
00:42:02.125 –> 00:42:03.725
for your return, right?

405
00:42:04.105 –> 00:42:05.965
You are storing them. You,

406
00:42:08.245 –> 00:42:11.245
whenever you’re calling them for a inside a fall loop,

407
00:42:11.305 –> 00:42:14.405
you need to write this jeep so that it’ll now become two

408
00:42:16.435 –> 00:42:18.735
couple, okay?

409
00:42:20.525 –> 00:42:22.865
And tap by tap, it’ll take this values,

410
00:42:22.895 –> 00:42:25.425
this lock probability and all this thing, okay?

411
00:42:26.485 –> 00:42:28.825
So now we are multiplying them element

412
00:42:28.925 –> 00:42:30.185
by element multiplication

413
00:42:31.225 –> 00:42:34.405
and then offending that value into your policy loss.

414
00:42:35.115 –> 00:42:35.405
Okay?

415
00:42:40.295 –> 00:42:43.475
So this policy loss is going to be again, one list.

416
00:42:44.215 –> 00:42:46.875
It is a, that is why we have, they have initialized

417
00:42:46.875 –> 00:42:48.035
with this, okay?

418
00:42:48.815 –> 00:42:51.955
So this is containing this multiplied value for element

419
00:42:52.095 –> 00:42:53.155
by element wise.

420
00:42:55.115 –> 00:42:56.825
After finishing up this follow-up,

421
00:42:56.825 –> 00:43:00.385
then you are making the average of your loss this,

422
00:43:01.085 –> 00:43:05.335
but this policy loss is your area.

423
00:43:06.355 –> 00:43:07.975
You call this concatenation.

424
00:43:07.975 –> 00:43:09.615
That means you’re concatenating everybody.

425
00:43:09.615 –> 00:43:13.445
And then doing the submission while doing this submission,

426
00:43:13.545 –> 00:43:15.645
you can also divide despite the length

427
00:43:15.645 –> 00:43:17.085
of this policy loss also

428
00:43:17.235 –> 00:43:19.965
that will match more, stabilize your training.

429
00:43:21.575 –> 00:43:24.465
Because this submission means at every time step,

430
00:43:24.785 –> 00:43:26.265
whatever loss is there, you’re doing all

431
00:43:26.265 –> 00:43:27.425
the submission, okay?

432
00:43:27.885 –> 00:43:29.585
The submission might be bigger help.

433
00:43:32.045 –> 00:43:35.535
However, if you do this division with the total number

434
00:43:35.535 –> 00:43:37.855
of elements, then it’ll fall that become a smaller help.

435
00:43:38.575 –> 00:43:41.835
You can do that, no problem. Okay? So that is a policy loss.

436
00:43:43.775 –> 00:43:48.075
So this loss is calculated for one episode, okay?

437
00:43:50.205 –> 00:43:54.175
Then you take this policy law gradient step.

438
00:43:54.235 –> 00:43:59.095
Now what I, why I’m telling this, uh, program line

439
00:43:59.095 –> 00:44:02.295
by line is because no, where you find I am using

440
00:44:03.395 –> 00:44:05.795
a gradient multiplied by lock quality of action.

441
00:44:06.455 –> 00:44:08.515
If you look at here this lock quality, I’m getting

442
00:44:09.385 –> 00:44:10.755
this from this add

443
00:44:10.935 –> 00:44:12.595
and this add, I have already shown you

444
00:44:12.595 –> 00:44:16.515
what is coming from your sampling, your action function.

445
00:44:16.515 –> 00:44:21.355
From there, this

446
00:44:21.355 –> 00:44:23.435
area I’m forming, forming by the help

447
00:44:23.435 –> 00:44:25.555
of law prob, what is law?

448
00:44:25.555 –> 00:44:27.515
Prob law is coming from your

449
00:44:29.065 –> 00:44:30.845
action, this action plus.

450
00:44:31.735 –> 00:44:34.515
So know where I’m taking this into account,

451
00:44:34.515 –> 00:44:36.395
this gradient, gradient of this.

452
00:44:36.395 –> 00:44:38.315
So that is why I’m telling you these are all

453
00:44:38.705 –> 00:44:39.875
approximation method.

454
00:44:40.945 –> 00:44:42.645
Mathematically, if you want to do,

455
00:44:42.665 –> 00:44:44.285
you must take the gradient of them.

456
00:44:44.345 –> 00:44:46.365
But gradient is not a going to be possible

457
00:44:46.365 –> 00:44:49.605
because there is no um, close formulation.

458
00:44:49.615 –> 00:44:52.205
Close formulation means you don’t have any equation

459
00:44:53.195 –> 00:44:54.445
that, okay, action.

460
00:44:54.725 –> 00:44:57.125
A at the state of ace can be expressed

461
00:44:57.185 –> 00:45:01.765
by some function like theta one square multiplied

462
00:45:01.765 –> 00:45:05.125
by S one theta two square multi theta two

463
00:45:05.205 –> 00:45:06.845
Q multiplied by S two.

464
00:45:06.845 –> 00:45:11.435
Like that. There is no such explicit relation, okay?

465
00:45:11.535 –> 00:45:15.075
So we cannot mathematically write any gradient.

466
00:45:15.975 –> 00:45:20.515
So neural network is a great approximation tool.

467
00:45:21.255 –> 00:45:24.075
If you don’t have any gradient, still the value,

468
00:45:24.175 –> 00:45:27.725
if you get it, the value might be around us.

469
00:45:28.305 –> 00:45:33.125
If you have the value, you give it to a back prop algorithm.

470
00:45:34.395 –> 00:45:36.925
Only thing you would require is you look at large number

471
00:45:36.945 –> 00:45:40.925
of samples so that whatever value we are having,

472
00:45:41.975 –> 00:45:44.555
you’ll be able to get the proper estimation.

473
00:45:47.705 –> 00:45:49.805
So this is reinforced algorithm.

474
00:45:51.805 –> 00:45:53.785
Now, I will share this to you today

475
00:45:54.325 –> 00:45:57.865
and yes, now for the actor critic thing.

476
00:45:58.365 –> 00:46:02.995
For the actor critic thing, this one, this for this program,

477
00:46:04.625 –> 00:46:07.585
I have a program that I have got it from hugging face.

478
00:46:09.625 –> 00:46:10.955
This is the hugging face program

479
00:46:11.415 –> 00:46:12.835
for which I’m not satisfied.

480
00:46:12.855 –> 00:46:14.715
The value because I’m getting a reward

481
00:46:14.715 –> 00:46:15.875
at the end is negative.

482
00:46:17.065 –> 00:46:19.035
This is a standard deviation, okay?

483
00:46:19.035 –> 00:46:20.035
This is a standard deviation,

484
00:46:20.095 –> 00:46:22.515
but the actual reward is coming up to me negative, which

485
00:46:22.515 –> 00:46:23.715
that’s why I’m not satisfied.

486
00:46:24.605 –> 00:46:27.225
And this program I have taken from hugging face itself,

487
00:46:28.315 –> 00:46:31.185
which is not my program, and see the documentation

488
00:46:31.245 –> 00:46:32.985
and all available, it is available.

489
00:46:33.375 –> 00:46:37.905
Yeah, it is having first program. This is called A two C.

490
00:46:38.055 –> 00:46:40.305
That was Advantage Act critic.

491
00:46:43.505 –> 00:46:45.465
I will share this program to you also,

492
00:46:45.525 –> 00:46:47.185
but I’m not satisfied with this program.

493
00:46:49.595 –> 00:46:52.605
However, this at method is just for your knowledge purpose.

494
00:46:52.795 –> 00:46:55.365
This will not, this programming is not going to be coming

495
00:46:55.425 –> 00:46:58.315
to your exam, okay?

496
00:47:00.425 –> 00:47:05.295
Okay. But fortunately this Lillian Wayne, you look at this,

497
00:47:05.295 –> 00:47:07.935
Lillian, I will give you this link also.

498
00:47:08.765 –> 00:47:12.385
She has developed the program using, uh,

499
00:47:15.535 –> 00:47:17.075
let me go to your chat window.

500
00:47:20.245 –> 00:47:21.495
This is using TensorFlow.

501
00:47:21.695 –> 00:47:23.415
Actually, I’m not comfortable in TensorFlow.

502
00:47:23.555 –> 00:47:27.165
That’s why I will not be able to show you the execution.

503
00:47:27.165 –> 00:47:30.555
However, you can just upload this to your,

504
00:47:31.695 –> 00:47:35.545
your copy paste and paste it to your Google collab.

505
00:47:35.545 –> 00:47:40.365
You will be able to do that. I have shared it to you.

506
00:47:41.765 –> 00:47:42.905
She has done a great job

507
00:47:43.485 –> 00:47:45.585
and all the programs she has made in

508
00:47:49.715 –> 00:47:50.325
what happened,

509
00:48:01.635 –> 00:48:01.925
okay?

510
00:48:02.765 –> 00:48:05.145
So this is the program for your actor critic

511
00:48:05.245 –> 00:48:06.465
and reinforce everything.

512
00:48:07.455 –> 00:48:10.305
This is the multicolor policy gradient.

513
00:48:10.305 –> 00:48:11.625
This is basically reinforce.

514
00:48:16.315 –> 00:48:18.655
So let me show you from this blog itself.

515
00:48:18.715 –> 00:48:22.095
So whatever I have taught to you so far, Q learning

516
00:48:23.165 –> 00:48:26.095
deep Q learning, everything is available here.

517
00:48:27.005 –> 00:48:31.855
Lillian Wang’s blog. But the program starting can flow.

518
00:48:32.395 –> 00:48:33.455
You can also do one thing.

519
00:48:33.455 –> 00:48:37.735
You can copy this program, give it to a chat GPT.

520
00:48:37.735 –> 00:48:41.635
They can convert this program to your PY touch, okay?

521
00:48:43.215 –> 00:48:44.555
But the logic remains same.

522
00:48:44.555 –> 00:48:48.665
Only the way it is written is different in PyTorch.

523
00:48:48.665 –> 00:48:52.475
And so you can very well go through this link, okay?

524
00:48:53.025 –> 00:48:55.775
This very, very well maintained.

525
00:48:59.685 –> 00:49:03.895
Everything is available in your TensorFlow.

526
00:49:06.105 –> 00:49:09.635
This is the knife Q learning,

527
00:49:10.125 –> 00:49:11.315
which you already know.

528
00:49:15.775 –> 00:49:20.025
Then the DQ learning for which we regard to neural network.

529
00:49:20.075 –> 00:49:21.545
Everything is given here.

530
00:49:28.905 –> 00:49:33.545
Okay? So here is your granular policy gradient.

531
00:49:33.695 –> 00:49:34.185
This one

532
00:49:44.205 –> 00:49:47.575
or act, you can copy this, you

533
00:49:48.265 –> 00:49:49.465
can give it to chat gt.

534
00:49:49.505 –> 00:49:50.985
They can convert into pythons.

535
00:49:55.155 –> 00:49:57.815
Now look at here, she has explained it very well.

536
00:50:00.325 –> 00:50:02.895
Collect for our, we have two network.

537
00:50:03.075 –> 00:50:06.335
One is your actor network, other one is your critic network.

538
00:50:06.945 –> 00:50:09.015
Actor network is on the policy

539
00:50:09.915 –> 00:50:13.255
and critic network is on the value value function.

540
00:50:13.645 –> 00:50:17.795
Okay? So the policy objection rule is basically

541
00:50:20.315 –> 00:50:25.145
you have to get your, you have

542
00:50:25.145 –> 00:50:29.535
to get your log probability of action, multiply

543
00:50:29.535 –> 00:50:30.535
with the gt.

544
00:50:30.725 –> 00:50:31.855
This, this formula only.

545
00:50:32.835 –> 00:50:35.725
This formula, this is what basically GT means here.

546
00:50:36.465 –> 00:50:38.765
Reward at the time of T multiply

547
00:50:38.765 –> 00:50:40.045
with the lock order of action.

548
00:50:40.675 –> 00:50:42.565
Okay? Only thing is this reward.

549
00:50:43.305 –> 00:50:46.005
You are not going to calculate just like we have done it in

550
00:50:46.005 –> 00:50:47.725
case of manual policy grad.

551
00:50:48.985 –> 00:50:52.155
Now you should get it from your critic network,

552
00:50:52.385 –> 00:50:53.915
this reward, okay?

553
00:50:54.225 –> 00:50:56.035
That is what this reward,

554
00:50:56.035 –> 00:50:57.355
you should get it from critic network.

555
00:50:57.775 –> 00:51:01.945
So for that, the reward

556
00:51:01.945 –> 00:51:03.385
that they’re taking is basically

557
00:51:09.085 –> 00:51:13.705
you have to apply TD learning error to find out your error

558
00:51:13.775 –> 00:51:15.425
that you are getting at the time.

559
00:51:15.425 –> 00:51:17.745
Step T use that error

560
00:51:18.445 –> 00:51:22.065
to update your value function at the critic critic network.

561
00:51:23.005 –> 00:51:27.265
And you have to take a sample from the uh,

562
00:51:27.805 –> 00:51:29.825
critic network for the particular state.

563
00:51:30.165 –> 00:51:32.305
You will get a value function coming

564
00:51:32.405 –> 00:51:33.865
out that value function.

565
00:51:33.865 –> 00:51:36.385
Have to use it in here in the actor network.

566
00:51:37.765 –> 00:51:41.555
So here is that. There is uh,

567
00:51:41.805 –> 00:51:43.155
these are the inputs.

568
00:51:43.485 –> 00:51:46.435
Input requires your state action definition

569
00:51:47.135 –> 00:51:50.325
and uh, reward

570
00:51:50.625 –> 00:51:52.245
for every state action pair,

571
00:51:52.245 –> 00:51:53.765
which is coming from your environment.

572
00:51:54.605 –> 00:51:57.265
So you have a act network dense under.

573
00:51:57.365 –> 00:52:01.985
So NN means it is a dense layer of the neur network,

574
00:52:02.195 –> 00:52:04.925
which has initial state is this.

575
00:52:06.385 –> 00:52:09.655
These are the your hidden states. Okay?

576
00:52:11.735 –> 00:52:14.945
Finally, it’ll convert to the action space.

577
00:52:15.165 –> 00:52:17.795
So if you look at, this is a list, okay?

578
00:52:19.085 –> 00:52:21.175
This list has three element.

579
00:52:21.315 –> 00:52:24.175
The first one is the dimension of your past hidden state.

580
00:52:24.555 –> 00:52:26.695
Second was the dimension of the second hidden state.

581
00:52:27.385 –> 00:52:29.775
Third one is a dimension of your action space

582
00:52:31.245 –> 00:52:34.955
because the act actor network is a

583
00:52:34.955 –> 00:52:36.075
policy network, policy network.

584
00:52:36.135 –> 00:52:37.515
Man, it’ll accept state.

585
00:52:37.645 –> 00:52:41.395
It’ll give you the output of action space, okay?

586
00:52:42.215 –> 00:52:44.995
Critic network is a value network is a Q function network.

587
00:52:44.995 –> 00:52:47.515
Basically, it is also a dense layer network,

588
00:52:47.805 –> 00:52:50.235
which has same definition.

589
00:52:50.615 –> 00:52:52.155
Now look at the differences here.

590
00:52:54.255 –> 00:52:56.795
The third element inside the list,

591
00:52:56.795 –> 00:53:00.565
because the first element is the dimension

592
00:53:00.565 –> 00:53:01.685
of your hidden layer.

593
00:53:04.235 –> 00:53:07.065
The first hidden layer, this 32 is a dimension

594
00:53:07.065 –> 00:53:09.345
of second hidden layer of the critic network.

595
00:53:09.885 –> 00:53:12.225
The one is basically there is only one neuron

596
00:53:13.125 –> 00:53:14.825
in the output of your critic network.

597
00:53:14.855 –> 00:53:17.385
Basically you are outputting your Q Health.

598
00:53:19.095 –> 00:53:21.055
Q Hal means it is a scaler value, okay?

599
00:53:21.715 –> 00:53:26.475
Qs, MI, that one you are outputting, so it’ll accept state,

600
00:53:27.845 –> 00:53:30.265
but they are not accepting SI they’re just

601
00:53:30.265 –> 00:53:31.385
taking states alone.

602
00:53:31.965 –> 00:53:34.105
It is producing the value for state.

603
00:53:34.165 –> 00:53:36.905
So basically now, although they have written Q value,

604
00:53:36.965 –> 00:53:39.505
but it is actually VV value

605
00:53:40.435 –> 00:53:42.895
and is Q value is a two dimensional thing,

606
00:53:43.225 –> 00:53:44.495
state action payer.

607
00:53:45.275 –> 00:53:46.975
But here they’re producing only one value.

608
00:53:46.975 –> 00:53:49.615
That means basically it is accepting state,

609
00:53:49.795 –> 00:53:50.855
it is producing one.

610
00:53:53.755 –> 00:53:54.845
Okay? So

611
00:53:54.845 –> 00:53:59.575
after that, when they’re using that network,

612
00:54:04.715 –> 00:54:07.255
so this one, now that training, it is

613
00:54:07.255 –> 00:54:09.255
inside critic train, okay?

614
00:54:10.285 –> 00:54:14.145
There’s, then it is actor train.

615
00:54:14.345 –> 00:54:16.745
Actually, I’m not sure about all the tens loading

616
00:54:16.765 –> 00:54:20.145
how they’re, which particular network they’re calling.

617
00:54:20.175 –> 00:54:23.855
That is TF Square under uh, TF train.

618
00:54:23.855 –> 00:54:25.255
They’re calling tf.

619
00:54:40.175 –> 00:54:41.675
You can do one thing, I am,

620
00:54:42.495 –> 00:54:43.755
you are not having that much time.

621
00:54:43.775 –> 00:54:45.595
You just copy this, give it to,

622
00:54:46.115 –> 00:54:48.275
they can transform this into, you can give it to Germany.

623
00:54:48.275 –> 00:54:51.215
They can translate to uh, your,

624
00:54:53.555 –> 00:54:54.695
you can run it, okay?

625
00:54:56.635 –> 00:55:00.205
However, I have this, uh, from hugging face, which

626
00:55:00.385 –> 00:55:02.565
for which they have not written the

627
00:55:04.815 –> 00:55:05.975
treating method by themselves

628
00:55:05.975 –> 00:55:07.615
because they’re using the library called

629
00:55:08.155 –> 00:55:09.495
stable baseline three.

630
00:55:10.915 –> 00:55:12.015
Stable baseline three.

631
00:55:12.015 –> 00:55:15.225
This library is available from

632
00:55:15.225 –> 00:55:19.535
where they’re calling just uh, this thing, this,

633
00:55:20.335 –> 00:55:22.395
you have to download this PIP install.

634
00:55:22.925 –> 00:55:26.565
Under this, you have act critic function available A

635
00:55:26.565 –> 00:55:28.035
to C, okay?

636
00:55:29.175 –> 00:55:31.555
You can call this function with the model name

637
00:55:31.575 –> 00:55:33.195
of your NEUR network.

638
00:55:33.445 –> 00:55:36.515
It’ll give you the act result.

639
00:55:41.945 –> 00:55:43.105
I have a little function,

640
00:55:43.105 –> 00:55:44.945
but I’m not have completed that program.

641
00:55:45.005 –> 00:55:47.545
That’s why I just, I was hesitating to show that

642
00:55:49.795 –> 00:55:52.835
this act by touch,

643
00:55:59.735 –> 00:56:01.555
but my program is not a completed.

644
00:56:02.715 –> 00:56:05.045
This is also, again, Cardpool example

645
00:56:09.645 –> 00:56:10.625
for act critic.

646
00:56:10.925 –> 00:56:13.145
The policy class I’m showing now,

647
00:56:13.775 –> 00:56:15.145
look at here the policy class.

648
00:56:15.665 –> 00:56:20.415
I have a linear layer, which is taking my

649
00:56:21.805 –> 00:56:25.965
input four dimension, going to hidden set of 1 28 dimension.

650
00:56:26.475 –> 00:56:30.325
Okay? And then from 1 28 dimension, I am mapping it

651
00:56:30.325 –> 00:56:33.325
to a only two dimensional action space.

652
00:56:33.395 –> 00:56:37.715
This is called action head. Then I have a critic layer.

653
00:56:39.255 –> 00:56:42.635
Pretty layer is, it is from 1 28 dimension.

654
00:56:42.655 –> 00:56:46.435
It is going to only one neuron.

655
00:56:46.435 –> 00:56:48.195
That means you’re producing one value.

656
00:56:48.775 –> 00:56:50.835
Now look at, in this case, the only problem

657
00:56:50.835 –> 00:56:55.195
that I was facing is I am having a shared neural network

658
00:56:56.435 –> 00:56:59.775
in which I have two neuron at the output space

659
00:57:00.895 –> 00:57:04.175
representing my action and one neuron in my

660
00:57:04.175 –> 00:57:05.655
output representing the value.

661
00:57:06.275 –> 00:57:08.175
So it is not a two separate neur network.

662
00:57:08.275 –> 00:57:11.015
It is one neuro network in the output space,

663
00:57:11.095 –> 00:57:12.815
I have three neurons.

664
00:57:12.995 –> 00:57:15.535
Two are for actor, one is for value.

665
00:57:16.345 –> 00:57:18.675
Whereas in this implementation, if look at

666
00:57:18.985 –> 00:57:22.455
that having two different network after three.

667
00:57:25.115 –> 00:57:29.605
So you have two different neur network here, okay,

668
00:57:34.485 –> 00:57:37.025
I’m getting some result, but I’m not satisfied the result.

669
00:57:37.025 –> 00:57:38.465
That’s what I was resisting before.

670
00:57:39.205 –> 00:57:40.585
So then I have forward class.

671
00:57:40.845 –> 00:57:44.695
In the forward class i, I am getting action probability.

672
00:57:45.115 –> 00:57:49.425
If I call action head, I’m getting state value.

673
00:57:49.565 –> 00:57:52.385
If I call value head, the action head value head,

674
00:57:52.385 –> 00:57:55.675
they are two different this

675
00:57:56.475 –> 00:57:57.535
and this, okay?

676
00:57:58.205 –> 00:58:01.925
They are sharing this input space to the hidden space.

677
00:58:01.925 –> 00:58:03.845
They’re sharing only the output

678
00:58:05.375 –> 00:58:08.485
having two different direction.

679
00:58:09.185 –> 00:58:11.525
One goes to action head, other goes to value head.

680
00:58:13.655 –> 00:58:14.995
So like that I’m having this

681
00:58:15.135 –> 00:58:19.515
and this is the value amp printing from them.

682
00:58:20.615 –> 00:58:22.145
Okay? So this is the model policy.

683
00:58:22.775 –> 00:58:24.785
This is the model parameters and all.

684
00:58:25.135 –> 00:58:27.465
Then I have this select action function

685
00:58:28.345 –> 00:58:30.885
and I have finish episode function,

686
00:58:31.545 –> 00:58:32.765
the select action function.

687
00:58:32.785 –> 00:58:35.485
The job is if you provide a state,

688
00:58:35.775 –> 00:58:40.205
it’ll give you the action sample from the neural network.

689
00:58:40.695 –> 00:58:43.005
Along with that, it’s probably of that action, okay?

690
00:58:44.665 –> 00:58:47.955
So if you give this state, okay,

691
00:58:48.085 –> 00:58:49.675
it’ll run the model.

692
00:58:50.295 –> 00:58:52.835
The model is what model is defined here, run the model.

693
00:58:52.885 –> 00:58:57.525
It’ll give you the probability and the state value, okay?

694
00:58:57.995 –> 00:59:00.765
From the probability, if you call categorical,

695
00:59:00.945 –> 00:59:05.165
if you give you, it’ll give you the distribution function.

696
00:59:05.475 –> 00:59:08.925
Then if you call sample, it’ll give you us action.

697
00:59:12.465 –> 00:59:14.395
Okay? And you have the saved action.

698
00:59:15.205 –> 00:59:17.595
Saved action is basically one function.

699
00:59:19.525 –> 00:59:22.565
The saved action is basically one function in which you’re

700
00:59:22.565 –> 00:59:24.605
saving the log probability of action and stable.

701
00:59:24.825 –> 00:59:27.405
That’s it. Saved action is already dup, defined

702
00:59:30.585 –> 00:59:30.875
save

703
00:59:40.275 –> 00:59:41.685
will be saved, added.

704
00:59:41.755 –> 00:59:43.205
Here it is added here.

705
00:59:46.425 –> 00:59:47.635
Save is name double.

706
00:59:56.175 –> 01:00:00.335
So after that, after doing this selection of action,

707
01:00:00.525 –> 01:00:04.535
then you use that action, give it to your step function.

708
01:00:04.535 –> 01:00:06.095
It’ll give a new reward

709
01:00:06.115 –> 01:00:08.695
and new state

710
01:00:10.655 –> 01:00:13.885
use those reward and state repetitively

711
01:00:14.935 –> 01:00:17.985
to find out your new reward

712
01:00:17.985 –> 01:00:20.345
and state complete the episode.

713
01:00:20.765 –> 01:00:23.705
So this finish episode is basically where you are doing this

714
01:00:24.895 –> 01:00:27.895
gradient update of your policy, network and value network.

715
01:00:28.045 –> 01:00:32.755
Both. Okay? The way we are running this is basically

716
01:00:33.735 –> 01:00:36.915
for every episode, but every episode

717
01:00:39.645 –> 01:00:44.325
you are starting from a

718
01:00:44.745 –> 01:00:45.765
any initial state.

719
01:00:46.835 –> 01:00:50.315
You have this may times step to be taken.

720
01:00:50.385 –> 01:00:51.515
This is a card pool game.

721
01:00:51.575 –> 01:00:55.765
So we have limited number of step 10,000 times step.

722
01:00:55.765 –> 01:00:57.525
We can take under every times step.

723
01:00:58.025 –> 01:01:00.885
We are only taking the action for every action,

724
01:01:01.165 –> 01:01:02.925
whatever the next state and next reward

725
01:01:03.005 –> 01:01:04.205
that we’re going to get.

726
01:01:04.865 –> 01:01:08.525
You are making this add to append the reward.

727
01:01:09.105 –> 01:01:13.085
If the state has been termination, then you come out of this

728
01:01:14.505 –> 01:01:16.435
traversal readiness episode.

729
01:01:17.055 –> 01:01:18.315
So one episode is completed.

730
01:01:18.315 –> 01:01:19.755
If you have encountered this terminated,

731
01:01:19.975 –> 01:01:24.505
so when the episode is completed, then you must find out

732
01:01:25.135 –> 01:01:28.985
your total sum of discounted reward.

733
01:01:29.775 –> 01:01:32.105
Then on that reward, you are going

734
01:01:32.105 –> 01:01:34.065
to apply this finish episode function.

735
01:01:35.005 –> 01:01:36.615
This finish episode. What it is doing,

736
01:01:36.675 –> 01:01:38.215
it doesn’t take any argument, okay?

737
01:01:39.885 –> 01:01:44.535
So it calculate the statewise

738
01:01:45.155 –> 01:01:49.775
reward from that reward, the same model of your, uh,

739
01:01:54.195 –> 01:01:55.135
policy gradient.

740
01:01:57.605 –> 01:01:59.625
And then it is finding the average reward

741
01:01:59.775 –> 01:02:00.985
with the average average reward.

742
01:02:01.605 –> 01:02:03.465
Now look at here, this average reward,

743
01:02:04.185 –> 01:02:05.745
I have got the average reward.

744
01:02:06.345 –> 01:02:08.185
I have got my saved action from here.

745
01:02:08.845 –> 01:02:11.575
So look at this value I’m get

746
01:02:11.765 –> 01:02:12.975
from where I’m getting this value.

747
01:02:13.285 –> 01:02:15.935
This value is something coming from our value network.

748
01:02:17.425 –> 01:02:20.285
That is what I wanted to show you. The difference.

749
01:02:20.905 –> 01:02:25.525
After you get all of their reward from this variable,

750
01:02:26.105 –> 01:02:29.165
you have to call the value network.

751
01:02:31.025 –> 01:02:33.735
Now, this value is basically your

752
01:02:36.095 –> 01:02:39.305
Advantage, um, computing advantage I think,

753
01:02:39.325 –> 01:02:43.605
but your difference

754
01:02:43.605 –> 01:02:48.315
between Q function and your value function, okay?

755
01:02:48.855 –> 01:02:51.515
And Q function for which we are using just the reward

756
01:02:51.515 –> 01:02:52.955
that we have accumulated so far.

757
01:02:55.895 –> 01:03:00.345
So if you remember your advantage computation,

758
01:03:00.345 –> 01:03:04.665
what is the definition of advantage has been,

759
01:03:05.805 –> 01:03:08.425
but in my P-P-T-I-I have long back written

760
01:03:09.005 –> 01:03:13.535
the advantage means, yeah,

761
01:03:13.535 –> 01:03:17.885
this is the advantage, okay?

762
01:03:17.885 –> 01:03:22.245
Advantage means what is the benefit

763
01:03:22.385 –> 01:03:23.725
of taking a particular action?

764
01:03:24.005 –> 01:03:28.405
A from the same state of ace. Okay?

765
01:03:28.745 –> 01:03:30.885
So that means difference between the Q function

766
01:03:30.995 –> 01:03:34.515
that is under state and action pair with the difference

767
01:03:34.535 –> 01:03:35.595
of value function.

768
01:03:35.775 –> 01:03:37.915
So look at the value function of the state of is

769
01:03:38.685 –> 01:03:40.145
can you delete between this and this?

770
01:03:40.145 –> 01:03:41.385
What is the relation between them?

771
01:03:47.945 –> 01:03:50.725
So total episode level versus individual actions. Now

772
01:03:53.025 –> 01:03:55.365
No Q function also two compute Q function.

773
01:03:55.385 –> 01:03:57.725
You need to go to the end of episode also.

774
01:04:06.235 –> 01:04:08.755
Uh, okay, within, uh,

775
01:04:10.575 –> 01:04:12.395
all the possible actions within the state

776
01:04:15.455 –> 01:04:17.725
value based on all the possible actions within the state

777
01:04:20.025 –> 01:04:21.025
Mm-hmm.

778
01:04:23.175 –> 01:04:26.875
Within S state, okay? I’ll tell you the differences.

779
01:04:28.275 –> 01:04:30.475
I don’t have the stylist. I kept it in office.

780
01:04:31.655 –> 01:04:33.755
The value function is

781
01:04:35.825 –> 01:04:38.405
you can get the value function from this queue also.

782
01:04:39.125 –> 01:04:41.625
Okay? How it is

783
01:04:42.875 –> 01:04:47.555
value function at the state of means, you have

784
01:04:48.705 –> 01:04:50.955
four or five number of possible actions

785
01:04:50.955 –> 01:04:53.635
to be taken at every state, okay?

786
01:04:54.255 –> 01:04:58.995
For every state you have, uh, for every state you have

787
01:05:00.385 –> 01:05:04.115
define possible action along with the probability.

788
01:05:04.335 –> 01:05:08.235
So take one probability of a particular action, multiply

789
01:05:08.235 –> 01:05:10.595
with the Q function of that state action pair.

790
01:05:11.925 –> 01:05:16.485
Then do the submission across all possible action.

791
01:05:17.425 –> 01:05:18.885
So view of a nothing

792
01:05:19.065 –> 01:05:23.265
but some over action,

793
01:05:23.585 –> 01:05:25.185
a some over action.

794
01:05:25.425 –> 01:05:28.465
A pi A given is multiplied

795
01:05:28.465 –> 01:05:31.375
by Q is combined, okay?

796
01:05:32.535 –> 01:05:35.635
So pi A given is probability of choosing action at the state

797
01:05:35.635 –> 01:05:37.715
of a, multiplied by

798
01:05:38.415 –> 01:05:41.115
the value we are getting under this state action

799
01:05:41.115 –> 01:05:42.355
pair at is QSA.

800
01:05:42.855 –> 01:05:46.995
So PI A given is multiplied by this, take this submission

801
01:05:47.015 –> 01:05:48.195
for all possible action.

802
01:05:48.195 –> 01:05:50.115
That is your V function, okay?

803
01:05:50.855 –> 01:05:52.795
So in other way, V function is the F average

804
01:05:53.415 –> 01:05:56.995
across all action possible in a particular state.

805
01:05:57.745 –> 01:05:59.085
And this is a particular health.

806
01:05:59.935 –> 01:06:02.945
This is a particular health and this is a, okay?

807
01:06:03.605 –> 01:06:05.665
So the difference between them is called advantage.

808
01:06:06.975 –> 01:06:11.525
Suppose you have a hundred students in your class

809
01:06:12.595 –> 01:06:15.175
and every student has its own particular school

810
01:06:15.175 –> 01:06:16.295
in a particular subject.

811
01:06:17.495 –> 01:06:19.515
If you compute average, you get a single value.

812
01:06:19.735 –> 01:06:22.755
Now for every student, you take this difference, that means

813
01:06:22.815 –> 01:06:24.035
how far he is from the average.

814
01:06:25.805 –> 01:06:29.865
He might be below average, he might be above average.

815
01:06:30.725 –> 01:06:33.985
So that difference, your only storing as your advantage.

816
01:06:35.225 –> 01:06:37.645
So the advantage value can be positive,

817
01:06:37.705 –> 01:06:40.165
can be negative, okay?

818
01:06:40.855 –> 01:06:43.235
It has a great benefit

819
01:06:44.395 –> 01:06:46.565
because it is only ODing the difference.

820
01:06:46.585 –> 01:06:49.245
And not only that, it is also ODing positive or negative.

821
01:06:49.505 –> 01:06:52.365
The moment it is negative, that means this action.

822
01:06:53.105 –> 01:06:54.645
You need to reduce the probability

823
01:06:54.955 –> 01:06:57.045
because this action a good action it is.

824
01:06:57.685 –> 01:06:59.205
So you need to reduce the probability,

825
01:06:59.265 –> 01:07:01.165
and that is what all of our

826
01:07:01.945 –> 01:07:04.285
policy gradient mechanism objective is.

827
01:07:05.265 –> 01:07:07.845
We will increase the probability of the action.

828
01:07:08.185 –> 01:07:10.885
If the action is leading to a positive deter,

829
01:07:11.695 –> 01:07:13.805
we’ll reduce the probability of the action.

830
01:07:13.865 –> 01:07:16.085
If the action is leading to a negative return,

831
01:07:16.845 –> 01:07:18.095
this will greatly help us

832
01:07:18.555 –> 01:07:21.255
and that is getting computed over there.

833
01:07:22.505 –> 01:07:24.165
It is R minus of value.

834
01:07:25.125 –> 01:07:27.345
And this value, actually it is coming from

835
01:07:32.795 –> 01:07:36.605
your value head here, this value head,

836
01:07:37.445 –> 01:07:41.795
this value head value is stored here at state value, okay?

837
01:07:41.905 –> 01:07:42.955
From state value,

838
01:07:45.875 –> 01:07:48.495
it is already getting stored in saved action.

839
01:07:49.725 –> 01:07:51.545
So this saved, action added,

840
01:07:53.385 –> 01:07:55.045
we have this saved action, added

841
01:07:57.975 –> 01:07:59.805
saved action added is given.

842
01:08:00.105 –> 01:08:02.685
So there is saved action here. There is a reward here.

843
01:08:03.665 –> 01:08:06.485
So in all of this adding, we are storing

844
01:08:07.735 –> 01:08:09.675
and this value will be fetch from them.

845
01:08:11.385 –> 01:08:16.165
This value is basically this value is fetch from them.

846
01:08:18.955 –> 01:08:20.675
Actually, I’m making these changes actually.

847
01:08:22.595 –> 01:08:24.975
So that program is not completed so far

848
01:08:24.975 –> 01:08:29.605
because here I’m taking the value from return, return,

849
01:08:29.745 –> 01:08:32.005
I’m gain, uh, forming from here, this return

850
01:08:33.985 –> 01:08:35.045
and this return.

851
01:08:37.205 –> 01:08:39.925
I am having it from the model reward.

852
01:08:42.435 –> 01:08:43.885
This model reward is nothing.

853
01:08:43.905 –> 01:08:47.775
But again, I have

854
01:08:47.775 –> 01:08:51.555
this, this state value.

855
01:08:51.815 –> 01:08:53.995
I’m not able to use it so hard.

856
01:08:54.435 –> 01:08:56.235
Actually, that’s why I have not shared this program too.

857
01:08:59.705 –> 01:09:03.045
So overall ideas like this, it is reduced, it is working,

858
01:09:03.345 –> 01:09:05.125
but it is not producing a good result.

859
01:09:05.125 –> 01:09:08.975
That’s why I’m not giving you the program.

860
01:09:09.485 –> 01:09:11.295
However, I will share you this program

861
01:09:13.895 –> 01:09:16.435
and then you have this hugging phase.

862
01:09:17.805 –> 01:09:19.705
You can this, I will share it to you.

863
01:09:20.175 –> 01:09:22.785
Yeah, and you can also take this program,

864
01:09:23.415 –> 01:09:26.615
convert this into your, this thing

865
01:09:28.465 –> 01:09:29.965
by by touch.

866
01:09:32.775 –> 01:09:35.315
So the heads here value head, attention head,

867
01:09:35.725 –> 01:09:36.955
sorry, the action head.

868
01:09:37.215 –> 01:09:39.035
Is it, is it something like this?

869
01:09:39.035 –> 01:09:42.905
Attention heads, uh, this head.

870
01:09:43.455 –> 01:09:46.145
What, what this head means action head and value head.

871
01:09:48.175 –> 01:09:50.585
Just let me download and send it to, I will tell you

872
01:10:00.025 –> 01:10:00.185
download.

873
01:10:10.215 –> 01:10:12.315
Let me send it by email. Then I’ll give you the answer.

874
01:11:52.645 –> 01:11:54.125
Hmm. What did you

875
01:11:54.125 –> 01:11:59.005
ask The heads, sir?

876
01:11:59.965 –> 01:12:03.145
Hmm? The head? Yeah. What, what, what they mean?

877
01:12:03.425 –> 01:12:07.925
Actually sir, well this is just a naming convention, okay?

878
01:12:08.195 –> 01:12:09.565
This just a naming con. It is not

879
01:12:10.795 –> 01:12:13.205
your uh, transformer.

880
01:12:14.075 –> 01:12:18.435
I see. Okay. Multi attention head. It is not that.

881
01:12:18.825 –> 01:12:22.475
Okay, here, just basically,

882
01:12:25.125 –> 01:12:26.225
so that is one thing.

883
01:12:27.815 –> 01:12:31.655
I have not been referring back to the textbook long time.

884
01:12:37.915 –> 01:12:41.855
Go to the textbook chapter.

885
01:12:50.895 –> 01:12:52.915
So we are currently in the chapter 11,

886
01:12:57.805 –> 01:12:58.735
chapter 11.

887
01:13:00.455 –> 01:13:04.465
This program I’m developing by going through this book,

888
01:13:08.765 –> 01:13:11.755
like for hugging phase, they have, they have

889
01:13:12.385 –> 01:13:13.955
this act activity program available,

890
01:13:15.155 –> 01:13:18.745
but it is under the library called, uh,

891
01:13:20.315 –> 01:13:21.415
stable baseline.

892
01:13:22.175 –> 01:13:23.745
Yeah, inside the stable baseline.

893
01:13:23.765 –> 01:13:28.605
If you go, uh, you’ll be able to see that. Let me show you.

894
01:13:30.555 –> 01:13:35.085
This is the place from where I’m building up. Yeah.

895
01:13:37.525 –> 01:13:37.745
So

896
01:13:43.505 –> 01:13:44.565
my idea was here,

897
01:13:47.975 –> 01:13:49.155
single neural network.

898
01:13:50.335 –> 01:13:52.635
The first two, the outputs space,

899
01:13:52.735 –> 01:13:54.955
I’m talking about the first two neuro policy

900
01:13:57.015 –> 01:13:58.275
output six.

901
01:13:58.345 –> 01:14:01.335
Last output is called value out. Okay?

902
01:14:01.405 –> 01:14:03.215
From here I’m trying to develop,

903
01:14:03.565 –> 01:14:07.975
however, there are various kind of different implementation.

904
01:14:08.225 –> 01:14:10.975
Every reinforcement learning research that I,

905
01:14:11.395 –> 01:14:13.205
this one from li Lilian Wayne,

906
01:14:14.305 –> 01:14:15.795
this is not a shared neural network.

907
01:14:15.795 –> 01:14:19.905
It’s two separate instance of neur network. Yeah.

908
01:14:20.285 –> 01:14:22.465
So what you do, you can go here,

909
01:14:25.365 –> 01:14:26.745
my program is not yet completed.

910
01:14:26.745 –> 01:14:28.705
So you go to here and share

911
01:14:37.625 –> 01:14:41.035
Just from the architecture I’m trying to write the code

912
01:14:41.545 –> 01:14:43.435
code is not completed.

913
01:14:45.605 –> 01:14:47.545
So you can ask this question

914
01:14:52.415 –> 01:14:56.665
on what 10 flow

915
01:15:18.665 –> 01:15:20.885
inflow is difficulty.

916
01:15:21.185 –> 01:15:22.485
So much of variable spoke

917
01:15:22.505 –> 01:15:27.305
and all, it’ll do some job sometimes has done it.

918
01:15:32.085 –> 01:15:33.975
Yeah. So it has done this job

919
01:15:39.695 –> 01:15:39.915
or

920
01:15:45.565 –> 01:15:46.935
yeah, it has done it very well.

921
01:15:46.935 –> 01:15:50.655
So you can just take this, copy them,

922
01:15:52.265 –> 01:15:55.195
give your, so here look at two separate instance,

923
01:16:00.665 –> 01:16:02.725
two separate instance of your neur network

924
01:16:15.785 –> 01:16:16.075
here.

925
01:16:16.255 –> 01:16:20.655
Are they using cardpool or what anything can

926
01:16:34.125 –> 01:16:35.185
to give the input?

927
01:16:39.355 –> 01:16:43.185
There are active network. There are critic network here.

928
01:16:43.255 –> 01:16:46.985
Okay? Yeah.

929
01:16:47.725 –> 01:16:51.765
Two different network. Then when you’re using forward

930
01:16:52.205 –> 01:16:55.165
function, you have logics coming out from actor network.

931
01:16:55.385 –> 01:16:59.125
You have Q hello coming out from critic network. Okay?

932
01:17:00.245 –> 01:17:03.095
Then after all the input you have to add,

933
01:17:04.695 –> 01:17:08.425
this is the only job we have to do after that.

934
01:17:08.855 –> 01:17:12.075
This model,

935
01:17:12.535 –> 01:17:15.715
you are in instant setting from the critique, which is going

936
01:17:15.715 –> 01:17:19.195
to give you the policy output and the queue function.

937
01:17:24.515 –> 01:17:28.255
So then you have this uh, critique output.

938
01:17:31.485 –> 01:17:34.815
From the critique output you’re using TD target, okay?

939
01:17:36.435 –> 01:17:40.125
Now, this TD target, you have to make it from

940
01:17:40.125 –> 01:17:41.405
where you’re going to get TD target.

941
01:17:42.275 –> 01:17:44.205
That is where here you need to write the code.

942
01:17:45.535 –> 01:17:47.235
So from there you get TD error.

943
01:17:47.455 –> 01:17:49.835
On the TD error you can apply,

944
01:17:50.615 –> 01:17:52.475
uh, cost function.

945
01:17:52.505 –> 01:17:56.685
That means of two, you get a loss on this loss.

946
01:17:56.705 –> 01:17:59.365
If you do a back provocation, you are going to do the update

947
01:17:59.505 –> 01:18:01.405
of your critic network level.

948
01:18:02.575 –> 01:18:06.775
This is critic network. Uh, no loss.

949
01:18:07.125 –> 01:18:08.895
Loss sees for critic, network

950
01:18:10.145 –> 01:18:13.355
loss under code is the loss in, in your actor network

951
01:18:15.375 –> 01:18:18.235
or each one of them have to update separately,

952
01:18:27.055 –> 01:18:30.145
so it’ll become too much heavy for you.

953
01:18:30.145 –> 01:18:31.705
If I give you this as a home assignment,

954
01:18:33.915 –> 01:18:35.535
how many assignment I have given so far?

955
01:18:37.865 –> 01:18:40.305
I have given one Monte learning assignment.

956
01:18:40.585 –> 01:18:44.045
I remember only that. Yes.

957
01:18:44.055 –> 01:18:46.385
Monte assessment, only one.

958
01:18:50.055 –> 01:18:54.695
Okay. As

959
01:18:54.955 –> 01:18:57.175
do we able to do it to become too much hay volume?

960
01:18:57.175 –> 01:19:00.035
You, yes, I would say Visa, sir.

961
01:19:04.575 –> 01:19:07.195
So we can practice it by ourselves. It’s fine.

962
01:19:08.325 –> 01:19:09.615
Yeah, you can practice. Okay?

963
01:19:09.915 –> 01:19:11.735
I’m not giving this as your home assignment.

964
01:19:12.325 –> 01:19:13.815
Only thing you need to, you can give,

965
01:19:13.835 –> 01:19:15.535
but not as a graded assessment.

966
01:19:21.185 –> 01:19:25.575
One suggestion, okay?

967
01:19:25.575 –> 01:19:28.135
You do by yourself at your home. Okay?

968
01:19:34.625 –> 01:19:35.845
Do, do you have any question?

969
01:19:48.145 –> 01:19:49.165
No question, sir. Thank you.

970
01:19:51.065 –> 01:19:53.605
My implementation that I was doing is basically from the

971
01:19:53.605 –> 01:19:55.685
shared network is from the book.

972
01:19:56.695 –> 01:19:59.665
From this, this is also one strategy.

973
01:19:59.765 –> 01:20:02.065
It is called synchronous policy update

974
01:20:03.335 –> 01:20:05.465
because they’re shared neuro network.

975
01:20:05.725 –> 01:20:08.165
So all of them are getting updated at the same time.

976
01:20:08.265 –> 01:20:09.645
It is synchronously updated

977
01:20:10.345 –> 01:20:12.805
and what, this is basically two separating stuff.

978
01:20:13.075 –> 01:20:15.885
They’re called asynchronous advantage critic is

979
01:20:15.885 –> 01:20:20.155
what is called a three C asynchronous advantage,

980
01:20:22.155 –> 01:20:23.615
uh, critic.

981
01:20:24.155 –> 01:20:25.455
So three A out there.

982
01:20:27.725 –> 01:20:29.905
So there are two ways to do synchronous

983
01:20:29.905 –> 01:20:30.705
and synchronous,

984
01:20:35.165 –> 01:20:35.455
okay?

985
01:20:36.195 –> 01:20:40.095
So you can log off them if you have no question.

986
01:20:43.795 –> 01:20:47.145
Thank you, sir. Thank you, sir. Thank you, sir.

987
01:20:47.775 –> 01:20:52.585
Okay, thank you, sir. Yeah, thank you. Yeah, thank you sir.

[vimeo- [https://vimeo.com/1160450987?fl=pl&fe=vl](https://vimeo.com/1160450987?fl=pl&fe=vl)](https://www.notion.so/vimeo-https-vimeo-com-1160450987-fl-pl-fe-vl-30161edeb73980c0a9e9c0a0838ee451?pvs=21)

---

# CLASS NOTES: RL02 - Actor-Critic Methods

## Metadata

- **Video URL**: [https://vimeo.com/1160450987](https://vimeo.com/1160450987)
- **Duration**: 80:52 (MM:SS)
- **Date**: February 2026
- **Topic**: Actor-Critic Methods
- **Environment**: CartPole (OpenAI Gym)
- **Framework**: PyTorch

## 1. Visual Index

| ID | Timestamp | Type | Description | Priority |
| --- | --- | --- | --- | --- |
| V1 | 19:30 | Diagram | Actor-Critic architecture (2 networks) | High |
| V2 | 32:00 | Equation | TD error and advantage function | High |
| V3 | 34:00 | Code | Policy network (actor) implementation | High |
| V4 | 42:00 | Code | Value network (critic) implementation | High |
| V5 | 50:00 | Diagram | Training loop structure | High |

---

## 2. Key Concepts

### Actor-Critic Architecture

**Two Networks Working Together:**

```
ACTOR NETWORK (Policy π_θ)
┌────────────────────────┐
│ Input: State s              │
│ Output: Action probabilities │
│ Update: Policy gradient      │
│ Role: "Actor" - takes actions│
└────────────────────────┘
        │
        ↓ provides actions
        │
┌────────────────────────┐
│ CRITIC NETWORK (Value V_φ) │
│                          │
│ Input: State s            │
│ Output: State value V(s)  │
│ Update: TD learning       │
│ Role: "Critic" - evaluates│
└────────────────────────┘
        ↑
        │ provides value estimate
```

### Advantage Function

**TD Error as Advantage:**

```
Advantage A(s,a) = Q(s,a) - V(s)

Using TD error:
δ_t = r_t + γV(s_{t+1}) - V(s_t)

This δ_t approximates the advantage!
```

### Key Equations

**Actor Update (Policy Gradient):**

```
∇_θ J(θ) ≈ ∇_θ log π_θ(a_t|s_t) · δ_t

θ ← θ + α_actor · δ_t · ∇_θ log π_θ(a_t|s_t)
```

**Critic Update (TD Learning):**

```
Loss_critic = δ_t² = [r_t + γV(s_{t+1}) - V(s_t)]²

φ ← φ - α_critic · ∇_φ Loss_critic
```

---

## 3. Implementation

**Actor Network:**

```python
class Actor(nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        return F.softmax(self.fc2(x), dim=-1)
```

**Critic Network:**

```python
class Critic(nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)  # Single value output

    def forward(self, state):
        x = F.relu(self.fc1(state))
        return self.fc2(x)  # V(s)
```

**Training Loop:**

```python
for episode in range(num_episodes):
    state = env.reset()

    while not done:
        # Actor selects action
        action_probs = actor(state)
        action = sample_from(action_probs)

        # Environment step
        next_state, reward, done = env.step(action)

        # Critic evaluates
        V_s = critic(state)
        V_s_next = critic(next_state)

        # TD error (advantage)
        delta = reward + gamma * V_s_next - V_s

        # Update critic
        critic_loss = delta**2
        critic_optimizer.zero_grad()
        critic_loss.backward()
        critic_optimizer.step()

        # Update actor
        log_prob = log(action_probs[action])
        actor_loss = -log_prob * delta.detach()
        actor_optimizer.zero_grad()
        actor_loss.backward()
        actor_optimizer.step()
```

---

## 4. Key Takeaways

1. **Hybrid Approach**: Combines policy gradient (actor) + value function (critic)
2. **Variance Reduction**: Critic reduces variance compared to pure REINFORCE
3. **Online Learning**: Updates after each step (not full episodes)
4. **TD Error = Advantage**: δ_t approximates how much better action was than expected
5. **Two Learning Rates**: Separate α for actor and critic

---

## 5. Advantages over REINFORCE

- **Lower Variance**: Value function baseline reduces gradient variance
- **Online Updates**: Can learn from incomplete episodes
- **Faster Convergence**: More stable and efficient training
- **Bootstrap**: Uses V(s') estimate instead of Monte Carlo returns

## 6. REINFORCE vs Actor-Critic: Detailed Comparison

### Problem with REINFORCE:

The professor demonstrated that REINFORCE suffers from HIGH VARIANCE in rewards across episodes. Looking at the CartPole training graph:

- Episode 100: reward ~155
- Episode 200: reward drops to ~50
- Episode 300: reward ~214
- Episode 1000: reward spikes to 450
This instability makes training unreliable and slow.

### How Actor-Critic Solves This:

**Key Insight**: Instead of using raw environment rewards (which are noisy), use a learned VALUE FUNCTION to estimate returns.

**Architectural Difference**:

```
REINFORCE:
  Policy Network → Action → Environment Reward
  (reward from environment is noisy)

ACTOR-CRITIC:
  Actor (Policy) → Action
  Critic (Value) → Estimated Return
  (critic provides smoother, learned estimate)
  
  ## 7. The Teacher-Student Analogy (Professor's Explanation)

The professor used a great analogy from around 27:00-28:30 in the lecture:

### Learning to Draw:
- **Student (Actor)**: Tries different drawing strokes
- **Teacher (Critic)**: Evaluates each stroke - "This was good, keep it" or "This was bad, try again"

In RL terms:
- **Actor Network**: Takes actions (like the student trying strokes)
- **Critic Network**: Evaluates actions (like the teacher providing feedback)
- The student doesn't wait until the entire drawing is done to get feedback
- The teacher provides immediate feedback after each stroke

This is why Actor-Critic can do **step-by-step updates** instead of waiting for full episodes!

## 8. Mathematical Foundation: Value Function vs Q-Function

### Definition Clarification (from 1:04:00-1:07:00):

The professor explained the relationship between V(s) and Q(s,a):

```
Q(s, a) = Expected return for state-action pair (s,a)
V(s) = Expected return for state s (averaged over all possible actions)

Mathematical relationship:
V(s) = Σ π(a|s) × Q(s,a)
     = Sum over all actions of [probability of action × Q-value]
```

### Student Example (from lecture):
- 100 students in class
- Each student has individual score: Q(student, subject)
- Class average: V(class)
- **Advantage**: How much better/worse a student is compared to average
  - A(student) = Q(student) - V(class)
  - If positive: above average
  - If negative: below average

### Why This Matters:
```
Advantage A(s,a) = Q(s,a) - V(s)
                 ≈ δ_t (TD error)
                 = r_t + γV(s_{t+1}) - V(s_t)
```

**The TD error naturally gives us the advantage without computing Q explicitly!**

## 9. Detailed Code Walkthrough (REINFORCE)

### Key Components from Lecture (33:00-45:00):

#### 1. Policy Network Structure:
```python
class Policy(nn.Module):
    def __init__(self, state_dim=4, hidden_dim=128, action_dim=2):
        self.fc1 = nn.Linear(state_dim, hidden_dim)  # State -> Hidden
        self.fc2 = nn.Linear(hidden_dim, action_dim)  # Hidden -> Action probs
```

**Critical Point**: Output is ACTION PROBABILITIES, not raw actions.

#### 2. Action Selection (act function):
```python
def act(self, state):
    probs = self.forward(state)  # Get probabilities
    m = Categorical(probs)       # Create distribution
    action = m.sample()          # Sample one action
    return action, m.log_prob(action)
```

**Professor's emphasis**: `log_prob` is AUTOMATICALLY computed - no manual gradient needed!

#### 3. Return Calculation:
The professor explained (39:00-42:00) that returns are stored in a queue:
```python
returns = []
for r in rewards[::-1]:  # Reverse iteration
    G = r + gamma * G    # Accumulate discounted reward
    returns.insert(0, G)
```

#### 4. The Loss Function (42:00-44:00):
**THIS IS THE MOST IMPORTANT PART:**

```python
policy_loss = []
for log_prob, R in zip(saved_log_probs, returns):
    policy_loss.append(-log_prob * R)
    
loss = torch.cat(policy_loss).sum()
loss.backward()  # PyTorch does the gradient automatically!
```

**Key Insight from Professor**: We don't manually compute ∇ log π. The negative sign and multiplication by R is enough for PyTorch's autograd!

## 10. Actor-Critic Implementation Details

### Two Network Architectures Discussed:

#### Architecture 1: Shared Network (from textbook, 1:13:00)
```python
class SharedActorCritic(nn.Module):
    def __init__(self):
        self.shared = nn.Linear(state_dim, 128)  # Shared layer
        self.action_head = nn.Linear(128, action_dim)  # Actor
        self.value_head = nn.Linear(128, 1)            # Critic
    
    def forward(self, state):
        x = F.relu(self.shared(state))
        action_probs = F.softmax(self.action_head(x))
        state_value = self.value_head(x)
        return action_probs, state_value
```
**Synchronous updates** - both networks share parameters

#### Architecture 2: Separate Networks (Lilian Weng's blog, 1:15:00)
```python
actor = PolicyNetwork(state_dim, 32, 32, action_dim)
critic = ValueNetwork(state_dim, 32, 32, 1)  # Output: single value
```
**Asynchronous updates (A3C)** - independent networks

### The Update Process (29:00-32:00):

```python
# Step 1: Get values from both networks
action_probs = actor(state)
V_s = critic(state)
V_s_next = critic(next_state)

# Step 2: Compute TD error (advantage)
delta = reward + gamma * V_s_next - V_s

# Step 3: Update Critic (TD Learning)
critic_loss = delta ** 2
critic_loss.backward()

# Step 4: Update Actor (Policy Gradient)
actor_loss = -log_prob * delta.detach()  # DETACH to stop gradient flow!
actor_loss.backward()
```

**Critical**: `delta.detach()` prevents gradients from flowing back to critic during actor update!

## 11. Important Practical Considerations

### Normalization Trick (40:00):
```python
returns = torch.tensor(returns)
returns = (returns - returns.mean()) / (returns.std() + 1e-5)
```
**Why?** Reduces variance and prevents gradient explosions.

### Approximation Nature (44:00-45:00):
Professor emphasized:
> "These are ALL approximation methods. Mathematically we should compute gradients, but there's no closed-form solution. Neural networks are great approximation tools - if you have enough samples, you'll get good estimates."

### Why No Explicit Gradient Computation?
- Action selection uses stochastic sampling
- No equation like: a = θ₁² × s₁ + θ₂² × s₂
- Neural network approximates this relationship
- PyTorch's autograd handles all gradient computation automatically

## 12. Resources Shared in Lecture

### 1. Lilian Weng's Blog (46:00-50:00):
- URL: [Lilian Weng's Policy Gradient Tutorial](mentioned in lecture)
- Contains:
  - DQN implementation
  - REINFORCE (vanilla policy gradient)
  - Actor-Critic methods
  - All in TensorFlow (can convert to PyTorch using ChatGPT)

### 2. Stable Baselines 3 (55:00):
- Library with pre-built Actor-Critic (A2C)
- Professor showed implementation but wasn't satisfied with results
- Good for reference but may need tweaking

### 3. Hugging Face Implementation:
- Has Actor-Critic examples
- Uses stable-baselines3 library

## 13. Next Topics (from lecture intro):

1. ✅ REINFORCE Algorithm (completed last class)
2. ✅ Actor-Critic Method (current class)
3. ⬜ **Proximal Policy Optimization (PPO)** - Next week (Feb 11)
4. ⬜ **Application to LLMs** - Show how RL is used in ChatGPT-like models

## 14. Exam Preparation Notes

### Professor's Statement (46:00):
> "This Actor-Critic programming will NOT be in the exam. This is just for your knowledge."

**What IS expected:**
- Understanding REINFORCE algorithm
- Concept of variance reduction
- Why Actor-Critic helps
- Basic architecture understanding

### Key Concepts to Master:

1. **Policy Gradient Formula**:
   - ∇J(θ) = E[∇ log π(a|s) × R]
   - Understand each component

2. **TD Learning**:
   - δ = r + γV(s') - V(s)
   - This is the critic's job

3. **Advantage Function**:
   - A(s,a) = Q(s,a) - V(s)
   - Why it reduces variance
   - Student example analogy

4. **Alternating Updates**:
   - Critic updates using TD error
   - Actor updates using critic's value
   - Why we detach gradients

## 15. Quick Reference: Algorithm Comparison

| Feature | REINFORCE | Actor-Critic |
|---------|-----------|-------------|
| **Networks** | 1 (Policy only) | 2 (Actor + Critic) |
| **Update Frequency** | End of episode | Every step |
| **Variance** | High | Lower |
| **Convergence** | Slow, unstable | Faster, more stable |
| **Return Estimation** | Monte Carlo | TD (bootstrapped) |
| **Advantage** | Requires full episodes | Can learn from incomplete episodes |

## 16. Variants Mentioned

### A2C (Advantage Actor-Critic):
- Uses advantage function explicitly
- Advantage = δ (TD error)

### A3C (Asynchronous Advantage Actor-Critic):
- Multiple parallel agents
- Separate actor and critic networks
- Each agent explores independently

### Synchronous vs Asynchronous:
- **Synchronous** (book approach): Shared network, simultaneous updates
- **Asynchronous** (Lilian Weng): Separate networks, alternating updates

## 17. Common Issues & Debugging (from Q&A)

### "What are 'heads' in the code?" (1:09:00)
**Student Question**: Are these like attention heads in transformers?
**Professor's Answer**: "No, just naming convention. 'Head' means output layer here."
- `action_head` = output layer for actor
- `value_head` = output layer for critic
- Not related to multi-head attention

### Professor's Implementation Challenges:
```python
# Professor's attempt with shared network (56:00-59:00)
# Problem: Couldn't properly integrate state_value into loss
# Issue: Using returns from environment instead of critic's V(s)
```

**The Fix**: Use critic's value estimate, not raw returns:
```python
# WRONG (what professor was debugging):
advantage = returns - model_rewards  # mixing sources

# CORRECT:
advantage = reward + gamma*V_next - V_current  # pure TD
```

### Why Hugging Face Result Was Negative (46:00):
- Professor got negative final reward
- "I'm not satisfied with this result"
- Issue: May need hyperparameter tuning
- Recommendation: Start with Lilian Weng's code and adapt

## 18. Final Summary: The Big Picture

### Evolution of RL Methods:
```
1. Monte Carlo
   ↓ (too slow, needs full episodes)
   
2. Q-Learning/DQN
   ↓ (discrete actions only)
   
3. Policy Gradients (REINFORCE)
   ↓ (high variance)
   
4. Actor-Critic ✓
   - Combines best of both worlds
   - Continuous actions possible
   - Lower variance
   - Online learning
```

### When to Use Actor-Critic:
- ✅ Continuous action spaces
- ✅ Need faster convergence
- ✅ Want to learn from incomplete episodes
- ✅ Have sufficient computational resources (2 networks)

### When to Use REINFORCE:
- ✅ Simple baseline needed
- ✅ Limited compute
- ✅ Policy optimization sufficient
- ❌ Don't need fast convergence

---

## 📌 Action Items:
1. Review REINFORCE code implementation (shared by professor)
2. Try converting Lilian Weng's TensorFlow code to PyTorch
3. Understand TD error = Advantage relationship
4. Practice explaining teacher-student analogy
5. Prepare for PPO topic next week
```