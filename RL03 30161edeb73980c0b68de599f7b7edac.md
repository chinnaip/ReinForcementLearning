# RL03

WEBVTT

1
00:03:40.655 –> 00:03:41.715
Uh, hi, Hamman.

2
00:03:43.065 –> 00:03:44.725
Hi, sir. Hi, sir.

3
00:03:44.945 –> 00:03:49.685
Uh, the faculty will join on, uh, 6 45. Okay. Oh, okay.

4
00:03:49.985 –> 00:03:51.085
The call from. Okay.

5
00:03:51.115 –> 00:03:55.285
Hundred person, just be waiting the clock. Okay? Sure, sure.

6
00:03:55.285 –> 00:03:56.685
Thank you. Sure. Thank you.

7
00:13:58.125 –> 00:14:01.705
Uh, just Michelle sir, will be joining at around 6 45 sim.

8
00:14:26.375 –> 00:14:28.115
Can someone ping in the WhatsApp then?

9
00:23:40.155 –> 00:23:44.445
I just to join? Um, is there,

10
00:23:45.855 –> 00:23:47.195
is there a class going on?

11
00:23:48.065 –> 00:23:50.325
No. No, sir. Told you join until 6 45.

12
00:23:50.325 –> 00:23:51.925
That’s what we’re waiting for, sir, to come.

13
00:23:52.875 –> 00:23:53.965
Okay, cool. Thanks

14
00:28:35.375 –> 00:28:36.375
Ma’am. Are you here

15
00:28:36.375 –> 00:28:36.375

16
00:29:12.535 –> 00:29:13.555
for any TAs?

17
00:29:21.505 –> 00:29:23.965
You not at any good news refers. Just wait, care.

18
00:29:25.235 –> 00:29:26.335
Oh, the wait is too long.

19
00:29:30.715 –> 00:29:35.415
My ears are waiting for good news. Yes. Everybody’s same.

20
00:29:35.565 –> 00:29:35.855
Come.

21
00:29:40.355 –> 00:29:44.245
Any announcements due? Yes.

22
00:29:46.745 –> 00:29:48.055
Sorry, I was waiting. Any

23
00:29:48.325 –> 00:29:53.325
from, I

24
00:30:14.335 –> 00:30:15.875
We already lost, uh, 40 minutes.

25
00:30:16.895 –> 00:30:18.635
Uh, sorry. 20 minutes. We hardly left

26
00:30:18.635 –> 00:30:19.875
with, uh, 40 minutes. Yeah,

27
00:30:19.875 –> 00:30:20.875
40 minutes, correct.

28
00:30:21.265 –> 00:30:22.275
Time in two, two.

29
00:30:23.285 –> 00:30:26.015
Yeah. Let’s, let’s have a good start in the next class.

30
00:30:29.045 –> 00:30:33.335
Can we have one, one more class at, uh, 7 45? All Yeah.

31
00:30:33.335 –> 00:30:35.015
I need to prepare the, I need to,

32
00:30:36.605 –> 00:30:37.605
Okay. Okay. That is

33
00:30:37.605 –> 00:30:38.455
why I got it.

34
00:30:43.435 –> 00:30:46.435
So you also there today? No, no, no, no.

35
00:30:46.735 –> 00:30:51.575
I’m, I’m, last,

36
00:30:56.485 –> 00:30:56.775
last.

37
00:31:12.375 –> 00:31:17.065
No, no. Mine is February 25th. Oh, after you.

38
00:31:17.065 –> 00:31:18.065
It’s mine.

39
00:31:22.135 –> 00:31:24.735
I had no choice when I started looking at the sheet.

40
00:31:27.245 –> 00:31:31.155
Mistake. I didn’t know that

41
00:31:31.155 –> 00:31:33.355
That Brita was the only one by choice.

42
00:31:33.415 –> 00:31:35.955
She selected the, the top most, uh, slot.

43
00:31:38.185 –> 00:31:40.035
Yeah. Later move on.

44
00:31:43.555 –> 00:31:45.875
Actually, I did not know that. Uh, that list is published.

45
00:31:45.935 –> 00:31:48.235
By the time I knew I opened. All,

46
00:31:48.455 –> 00:31:49.675
All the bottom bottoms slot are gone.

47
00:31:52.295 –> 00:31:56.835
Second beam sensor. Exactly like movie theater.

48
00:31:56.945 –> 00:31:59.675
Like all the backseats are taken. Correct.

49
00:32:07.525 –> 00:32:09.905
You on the camera. I think you want to. Oh my God.

50
00:32:09.935 –> 00:32:14.305
What about the camera and all, just like that.

51
00:32:15.305 –> 00:32:19.425
I had a new No, I just combed my head. That’s why everyone,

52
00:32:28.835 –> 00:32:30.975
so this class became a fashion show?

53
00:32:32.305 –> 00:32:34.435
Exactly. Not fashion show fun

54
00:32:34.755 –> 00:32:38.815
activity. Okay.

55
00:32:38.815 –> 00:32:40.375
For yoga? Yes.

56
00:32:41.165 –> 00:32:44.855
Yeah. You, we would them if we had come

57
00:32:44.855 –> 00:32:46.495
for the the, so yoga,

58
00:32:48.635 –> 00:32:49.975
No, no, not required.

59
00:32:50.025 –> 00:32:52.375
Sorry. No,

60
00:32:53.935 –> 00:32:56.095
I don’t think anyone is looking at, uh, any,

61
00:32:56.115 –> 00:32:58.735
any other person will be simply looking at our own.

62
00:33:02.735 –> 00:33:04.465
Even in, uh, my company,

63
00:33:04.985 –> 00:33:07.065
whenever we are required to turn our cameras,

64
00:33:07.885 –> 00:33:09.825
all my colleagues, they only look at themselves.

65
00:33:10.015 –> 00:33:12.385
Even myself, I only look at myself. How am I looking?

66
00:33:12.925 –> 00:33:14.305
Nobody looks at anybody else.

67
00:33:27.585 –> 00:33:31.005
Anyway, we, we had this, uh, whatever science is going

68
00:33:31.005 –> 00:33:32.805
to teach, it’s going to go above ahead anyway.

69
00:33:36.155 –> 00:33:38.895
And especially for you, if your, your mind is only with, uh,

70
00:33:40.215 –> 00:33:41.935
I think the literature review.

71
00:33:41.935 –> 00:33:45.175
Yeah. And now that you brought the topic, like for me,

72
00:33:45.175 –> 00:33:47.375
at least half of it is already over

73
00:33:47.375 –> 00:33:49.535
my head on Bounce or something. So

74
00:33:50.315 –> 00:33:54.935
Can everybody, Simon, how can you say like that? Come

75
00:33:54.935 –> 00:33:55.695
On. Like,

76
00:33:56.635 –> 00:33:57.535
No. Glad. No.

77
00:33:57.715 –> 00:34:00.575
So can anybody help from that PD learning part to

78
00:34:01.245 –> 00:34:02.505
take Correct.

79
00:34:03.275 –> 00:34:04.705
Maybe collectively we can pay also,

80
00:34:04.705 –> 00:34:09.465
but yeah, just throwing it out there, like maybe off, uh,

81
00:34:09.965 –> 00:34:12.505
off the chat we can have like an hour meeting.

82
00:34:12.645 –> 00:34:16.805
So just go over the topics and just randomly, uh, you’re

83
00:34:16.805 –> 00:34:17.845
Telling among ourselves, right?

84
00:34:18.135 –> 00:34:20.525
Informally. Yeah. Yeah. That we can do that.

85
00:34:20.525 –> 00:34:21.685
We can do actually actually whoever,

86
00:34:21.715 –> 00:34:25.245
whoever understood can actually give a small presentation

87
00:34:25.245 –> 00:34:26.365
and we can all follow it.

88
00:34:26.365 –> 00:34:27.365
That, that will be really good. Yeah.

89
00:34:27.365 –> 00:34:28.485
Let’s not put a high bar. I think

90
00:34:29.945 –> 00:34:31.645
if you say whoever understood the whole thing,

91
00:34:31.885 –> 00:34:33.165
probably everybody will say no.

92
00:34:34.955 –> 00:34:37.045
Correct. From large.

93
00:34:38.145 –> 00:34:40.525
Any comments regarding how the presentation was?

94
00:34:40.595 –> 00:34:43.565
Like any requirements? Anything from last case?

95
00:34:45.825 –> 00:34:48.125
You’re talking about, uh, literature review? Yes.

96
00:34:48.125 –> 00:34:50.885
Yes. I Would love to hear, uh, the input.

97
00:34:52.565 –> 00:34:54.145
The first, last, last session,

98
00:34:54.255 –> 00:34:56.225
last piece was I was the one who was doing

99
00:34:59.315 –> 00:35:02.455
So, so I, I think, I think, uh, beam Bema sensor is there.

100
00:35:02.675 –> 00:35:06.015
He would, he would be better person to give a review

101
00:35:06.015 –> 00:35:08.215
because he is the one gave who gave a presentation last time

102
00:35:08.955 –> 00:35:11.215
and even vibe bu also gave, I think, right?

103
00:35:11.215 –> 00:35:15.215
Yeah. Vibe also gave, he’s,

104
00:35:16.435 –> 00:35:18.295
and then the today’s, uh, this

105
00:35:19.475 –> 00:35:22.075
reinforcement classes canceled, is it?

106
00:35:22.415 –> 00:35:25.235
No, no, it is there, sir. 12, 6 45. Now it is seven o’clock.

107
00:35:25.235 –> 00:35:28.035
They’re telling. So we, we’ll wait and see. Okay.

108
00:35:28.095 –> 00:35:29.275
We are hoping for the thing.

109
00:35:35.525 –> 00:35:39.215
Okay. So in the classroom,

110
00:35:39.595 –> 00:35:40.655
if the service is not there,

111
00:35:40.655 –> 00:35:41.775
we’ll all be chit chatting, right?

112
00:35:41.775 –> 00:35:43.295
That is what is happening right now over here,

113
00:35:45.265 –> 00:35:46.265
Right?

114
00:35:49.215 –> 00:35:54.025
Uh, good evening. Sorry, I got delayed. We had a good sir.

115
00:35:54.055 –> 00:35:56.905
Webinar today. Good evening, sir.

116
00:35:57.095 –> 00:35:58.095
Good evening, Sir.

117
00:35:58.215 –> 00:35:59.865
Evening. Good evening, sir.

118
00:36:00.215 –> 00:36:05.175
Good evening. So we had a webinar today, so

119
00:36:05.175 –> 00:36:08.295
for which I had to attend that, so I got late for that.

120
00:36:08.965 –> 00:36:09.965
Sure, sir.

121
00:36:11.385 –> 00:36:12.765
We were Just chatting before you came.

122
00:36:12.985 –> 00:36:14.005
We had a lot of time. We were

123
00:36:14.005 –> 00:36:15.525
just chatting, chatting to each other.

124
00:36:16.195 –> 00:36:19.725
Okay. So I see a lot

125
00:36:19.725 –> 00:36:21.445
of people are there 41 people.

126
00:36:22.825 –> 00:36:23.825
Okay.

127
00:36:25.045 –> 00:36:27.685
I said before you start, just wanted to confirm one thing

128
00:36:27.685 –> 00:36:31.645
that, uh, these, uh, end semester exam will be on, uh,

129
00:36:31.645 –> 00:36:33.045
second week of March, right?

130
00:36:33.145 –> 00:36:36.205
Mm-hmm. Or, uh, this date is not, uh,

131
00:36:39.805 –> 00:36:41.505
Uh, first week of March.

132
00:36:41.695 –> 00:36:43.025
Okay? I can,

133
00:36:43.775 –> 00:36:45.225
Okay, so the, those dates,

134
00:36:46.015 –> 00:36:48.785
they will be taking care while you have no idea about date.

135
00:36:50.495 –> 00:36:54.195
Uh, we got 22 dates, uh, but we’ll let you know, okay?

136
00:36:55.335 –> 00:36:57.115
Mm-hmm. Yeah.

137
00:36:57.625 –> 00:37:00.195
Okay. That will be on first week of March, is it?

138
00:37:00.775 –> 00:37:04.395
Yes. Yes. Think March 7th. March.

139
00:37:05.025 –> 00:37:06.075
Okay. Yeah.

140
00:37:07.975 –> 00:37:11.875
So your last class is going to be, uh, 21st of February?

141
00:37:15.275 –> 00:37:16.875
Yes, sir. Yes.

142
00:37:18.165 –> 00:37:22.995
Okay. So today’s topic,

143
00:37:23.215 –> 00:37:27.585
uh, is going to be only two topics are remaining.

144
00:37:27.585 –> 00:37:28.905
That is one is I’m going

145
00:37:28.905 –> 00:37:31.305
to cover generalized advantage estimate,

146
00:37:33.505 –> 00:37:37.365
and another one is called, uh, your, uh,

147
00:37:40.855 –> 00:37:42.575
proximal policy optimization.

148
00:37:45.295 –> 00:37:46.725
Today, I will show you about

149
00:37:46.915 –> 00:37:49.245
what is called generalized advantage estimate.

150
00:37:51.045 –> 00:37:52.145
Let me share the screen

151
00:38:01.355 –> 00:38:03.575
So I’ll not take too much of theory here.

152
00:38:04.495 –> 00:38:09.455
I will, I’ll give very brief, uh,

153
00:38:09.885 –> 00:38:11.575
theoretical knowledge about it.

154
00:38:14.795 –> 00:38:18.965
Yeah. So I am going back to again,

155
00:38:19.075 –> 00:38:23.205
Lillian Wang’s, uh, blog policy gradient.

156
00:38:25.095 –> 00:38:26.195
So my intention is

157
00:38:26.195 –> 00:38:27.435
before getting into policy,

158
00:38:27.835 –> 00:38:30.955
proximal P-P-O-P-P-O algorithm is a very advanced algorithm.

159
00:38:31.215 –> 00:38:33.875
Before that, that is on Saturday class.

160
00:38:34.035 –> 00:38:37.125
I will show you one example of the usage

161
00:38:37.225 –> 00:38:41.035
of all this reinforcement learning algorithm, uh,

162
00:38:41.185 –> 00:38:43.395
into your large language model training.

163
00:38:47.315 –> 00:38:51.665
So I have covered here till, uh, actor critic method.

164
00:38:52.825 –> 00:38:54.635
Okay? So today’s topic is going to be

165
00:38:55.335 –> 00:38:57.955
called generalized advantage estimation,

166
00:38:58.605 –> 00:39:02.595
which is one more step ahead of ACT critic method.

167
00:39:04.805 –> 00:39:06.265
So, oh, that is here

168
00:39:13.715 –> 00:39:14.135
theory,

169
00:39:19.925 –> 00:39:21.815
this is DRPO and then PPO.

170
00:40:37.755 –> 00:40:42.745
Did we lose, sir? Yes, I think Got disconnected.

171
00:40:52.175 –> 00:40:53.235
Sir, you are on mute.

172
00:41:06.315 –> 00:41:09.355
Okay, my connection got disconnected.

173
00:41:11.045 –> 00:41:15.215
Okay, so I will just go to the formula for this.

174
00:41:18.935 –> 00:41:20.395
So could you reshare your screen?

175
00:41:22.715 –> 00:41:26.145
Okay, I’m sharing.

176
00:41:45.655 –> 00:41:48.675
So basically this generalized advantage estimate,

177
00:41:48.905 –> 00:41:51.915
this method is implemented already in PPO.

178
00:41:52.615 –> 00:41:54.475
It is proximal policy optimization.

179
00:41:55.295 –> 00:41:59.035
So before that, I, that’s why I plan to show you

180
00:41:59.035 –> 00:42:00.995
what is called generalized advantage estimate.

181
00:42:01.785 –> 00:42:04.115
Okay? So before that, what is called advantage.

182
00:42:05.925 –> 00:42:07.985
So advantage is basically this function.

183
00:42:09.905 –> 00:42:12.925
You see that advantage is written by capital A

184
00:42:13.225 –> 00:42:14.645
for the state and action payer.

185
00:42:14.675 –> 00:42:15.885
That is ST and 80.

186
00:42:16.505 –> 00:42:19.285
So ST and 80 is your current state and current action.

187
00:42:19.945 –> 00:42:22.685
But for every time step that you, uh,

188
00:42:22.685 –> 00:42:24.605
take into consideration that is ST

189
00:42:24.605 –> 00:42:27.165
and 80, you can estimate the advantage function.

190
00:42:28.065 –> 00:42:31.005
So that is advantage, basically difference

191
00:42:31.005 –> 00:42:32.845
between Q function and value function.

192
00:42:33.715 –> 00:42:38.325
Okay? So a Q function, it pro it is the

193
00:42:40.045 –> 00:42:43.285
expected sum of discounted reward, having committed

194
00:42:43.305 –> 00:42:44.565
to a particular action,

195
00:42:46.165 –> 00:42:49.025
and V function is averaging across all action

196
00:42:49.285 –> 00:42:50.465
at that particular state.

197
00:42:51.545 –> 00:42:53.155
Okay? So

198
00:42:55.955 –> 00:42:59.325
naturally this Q function value the tendencies.

199
00:42:59.665 –> 00:43:02.755
If the action is good, then the average,

200
00:43:03.785 –> 00:43:06.155
then this Q function value will be higher than

201
00:43:06.155 –> 00:43:07.235
your value function.

202
00:43:07.415 –> 00:43:09.915
So that is our natural, uh,

203
00:43:09.915 –> 00:43:12.515
assumption, which is true.

204
00:43:12.515 –> 00:43:16.435
Basically value function is averaging across all action

205
00:43:16.455 –> 00:43:17.555
for a particular state.

206
00:43:18.345 –> 00:43:22.925
Okay? So average value will be always somewhat, uh,

207
00:43:24.445 –> 00:43:25.485
somewhat a stable value,

208
00:43:25.485 –> 00:43:29.165
whereas individual value will be jumping, okay?

209
00:43:29.945 –> 00:43:32.805
So if this particular value is a positive thing,

210
00:43:32.875 –> 00:43:34.405
then the action is a good action.

211
00:43:34.425 –> 00:43:36.565
If this particular value is a negative thing,

212
00:43:36.565 –> 00:43:38.005
then the action is a negative action.

213
00:43:38.585 –> 00:43:40.125
Um, the action is a bad action.

214
00:43:40.145 –> 00:43:43.845
So we’ll have to reduce the probability of taking the action

215
00:43:43.945 –> 00:43:45.125
for that particular state.

216
00:43:45.125 –> 00:43:47.125
So that is the general assumption

217
00:43:47.425 –> 00:43:49.845
by which advantage estimation is working.

218
00:43:51.015 –> 00:43:52.265
Okay? So

219
00:43:52.295 –> 00:43:55.065
what is called generalized advantage estimation then?

220
00:43:55.525 –> 00:43:57.585
So this is called normal advantage, okay?

221
00:43:58.085 –> 00:44:00.425
So generalized advantage is basically

222
00:44:01.785 –> 00:44:05.065
whenever you are trying to, you can think of

223
00:44:06.285 –> 00:44:10.305
you have already, uh, familiar with TD learning.

224
00:44:10.335 –> 00:44:15.065
That is TD zero, TD one, TD two, acute TD in learning, okay?

225
00:44:15.165 –> 00:44:19.665
We have in stands for the number of times steps

226
00:44:20.415 –> 00:44:24.905
that you have moved into the future to estimate your, uh,

227
00:44:25.485 –> 00:44:27.105
future state value function.

228
00:44:27.685 –> 00:44:30.905
So that future state value function will be used

229
00:44:31.035 –> 00:44:34.255
to update your the current state value function.

230
00:44:35.185 –> 00:44:36.835
Okay? So that is TDN learning

231
00:44:37.095 –> 00:44:39.955
and then TD Lambda learning is for all

232
00:44:39.955 –> 00:44:44.555
of this future timestamp that you have gone ahead, each one

233
00:44:44.555 –> 00:44:46.715
of them can provide you a value function.

234
00:44:47.415 –> 00:44:50.595
You can average them by using a lambda term.

235
00:44:51.215 –> 00:44:55.635
So every TD learning function that you’re using,

236
00:44:55.635 –> 00:44:58.035
that is value function at the state

237
00:44:58.055 –> 00:45:02.595
of st plus NTA current state st plus n result

238
00:45:03.055 –> 00:45:04.755
in steps into the future.

239
00:45:06.015 –> 00:45:09.105
Okay? So then once you have that value available,

240
00:45:09.725 –> 00:45:13.025
you can multiply all of the individual previous timestamp

241
00:45:13.125 –> 00:45:16.905
of st plus N minus on st plus N minus two

242
00:45:17.205 –> 00:45:20.025
by a factor called Lambda and lambda.

243
00:45:20.365 –> 00:45:22.665
You can take it lambda square,

244
00:45:22.725 –> 00:45:24.785
lambda 2.3 lambda before four like that.

245
00:45:25.015 –> 00:45:28.265
Okay? So that is a weight factor that we’re involving

246
00:45:28.445 –> 00:45:29.825
to estimate your general.

247
00:45:30.005 –> 00:45:31.345
It is called TD lambda learning.

248
00:45:31.885 –> 00:45:33.105
So the TD Lambda learning,

249
00:45:33.425 –> 00:45:37.145
whenever it is converted into this function of advantage,

250
00:45:38.335 –> 00:45:40.465
that is called generalized advantage, okay?

251
00:45:40.465 –> 00:45:43.185
So that is the basic philosophy

252
00:45:43.185 –> 00:45:44.785
behind generalized advantage estimate.

253
00:45:44.785 –> 00:45:47.785
So basically, what is a purpose of generalized advantage?

254
00:45:48.215 –> 00:45:52.145
That they want to have a TD lambda learning kind of concept

255
00:45:52.715 –> 00:45:56.655
applicable to this advantage function.

256
00:45:56.935 –> 00:46:00.995
A normally this, uh, uh,

257
00:46:01.235 –> 00:46:05.395
TD lambda learning that is applied on value function, okay?

258
00:46:05.745 –> 00:46:07.595
Instead of they want to apply out in

259
00:46:08.145 –> 00:46:09.595
onto the advantage function.

260
00:46:10.175 –> 00:46:13.305
So that is the main thing behind t uh,

261
00:46:13.375 –> 00:46:15.065
your generalized advantage destination.

262
00:46:15.605 –> 00:46:19.505
So it is given with a formula

263
00:46:20.255 –> 00:46:21.825
that this is the formula

264
00:46:22.045 –> 00:46:24.425
for generalized advantage destination.

265
00:46:24.575 –> 00:46:27.405
Okay? This is the, these are the formulas.

266
00:46:27.585 –> 00:46:31.085
So whether you want to take one time step into the future,

267
00:46:31.145 –> 00:46:33.445
and whether you want to take two times step into the future

268
00:46:33.625 –> 00:46:36.045
or three times step in the future like that, it’s okay.

269
00:46:37.025 –> 00:46:40.245
So, uh, there is a td uh, error term.

270
00:46:40.435 –> 00:46:44.405
That TD error term is basically td TD zero.

271
00:46:44.475 –> 00:46:47.165
That means currently you’re in a state of st

272
00:46:48.095 –> 00:46:49.275
you have taken one action.

273
00:46:49.275 –> 00:46:51.195
So you’re moving into ST plus one state.

274
00:46:52.165 –> 00:46:54.625
At ST plus one state you have a value function

275
00:46:54.865 –> 00:46:56.415
estimate, okay?

276
00:46:56.415 –> 00:46:58.175
So that value function estimate you’re using

277
00:46:58.195 –> 00:46:59.655
to update your old estimate.

278
00:46:59.655 –> 00:47:03.045
So that is your TD error term.

279
00:47:03.045 –> 00:47:06.485
So TD era term is this part, RT plus of gamma

280
00:47:07.005 –> 00:47:09.205
multiplier value function available at the state

281
00:47:09.205 –> 00:47:10.205
of ST plus one.

282
00:47:11.005 –> 00:47:13.255
This is your TD target. This part is TD target.

283
00:47:13.395 –> 00:47:15.695
If you take a difference to the previous value estimate,

284
00:47:15.695 –> 00:47:17.775
that is based, it’s called error term.

285
00:47:17.775 –> 00:47:21.175
Error term is always noted as delta. Okay?

286
00:47:22.235 –> 00:47:25.125
This delta T is basically that error at a time.

287
00:47:25.125 –> 00:47:28.265
Step T, if you’re considering only one time,

288
00:47:28.265 –> 00:47:30.845
step into the future, this is only one time step.

289
00:47:31.265 –> 00:47:33.965
If you’re considering two times step into the future, in

290
00:47:33.965 –> 00:47:37.005
that case it’ll become, this will be expanded further.

291
00:47:38.425 –> 00:47:41.675
This is V of st plus on you can take one more time,

292
00:47:41.675 –> 00:47:44.555
step into the future so that it’ll give you

293
00:47:45.395 –> 00:47:49.335
a reward at the time of T plus one plus

294
00:47:51.635 –> 00:47:55.445
plus the discounted value that is gamma multiplied by

295
00:47:56.625 –> 00:47:59.375
value function available at time T plus two.

296
00:47:59.605 –> 00:48:03.575
Okay? So this is, and everything is pre multiplied by gamma.

297
00:48:03.575 –> 00:48:05.415
So it’ll be like this, okay?

298
00:48:05.635 –> 00:48:07.895
So this is two times step into the future.

299
00:48:07.995 –> 00:48:11.775
So that is called delta, uh,

300
00:48:11.965 –> 00:48:13.095
your times step T

301
00:48:13.095 –> 00:48:17.515
and delta at times step T plus on this delta times step T

302
00:48:17.515 –> 00:48:20.595
plus on, it’ll be multiplied by a discount factor of gamma.

303
00:48:20.775 –> 00:48:22.275
So this two sum is called your

304
00:48:22.905 –> 00:48:24.835
generalized advantage estimate for the times

305
00:48:25.365 –> 00:48:28.935
43 take two times step into the future, okay?

306
00:48:29.445 –> 00:48:31.255
Like that you can take key number

307
00:48:31.255 –> 00:48:32.495
of timestamp into the future.

308
00:48:32.995 –> 00:48:36.485
So that will become like this formula, okay?

309
00:48:37.585 –> 00:48:40.405
So that is what is called generalized advantage estimation.

310
00:48:41.095 –> 00:48:45.155
So only changes this, the reward function.

311
00:48:45.675 –> 00:48:47.355
Normally we have been keep on using this.

312
00:48:48.415 –> 00:48:51.995
If I go back to uh, Lillian w paper, uh, method

313
00:48:53.675 –> 00:48:55.615
for our actor critic method,

314
00:48:58.495 –> 00:48:59.965
there are a lot of methods out there.

315
00:49:00.705 –> 00:49:04.515
I’m only teaching you basic

316
00:49:04.515 –> 00:49:07.845
methods, okay?

317
00:49:07.945 –> 00:49:08.685
So that is

318
00:49:17.465 –> 00:49:20.885
the general formula for this is basically this one.

319
00:49:21.705 –> 00:49:24.845
The gradient of your cost function is basically expected

320
00:49:25.115 –> 00:49:30.085
overall, the policy expectation, all the policy that is gt,

321
00:49:30.085 –> 00:49:34.405
basically GT stands for your reward at a time.

322
00:49:34.435 –> 00:49:39.285
Step T on word, then gradient log, multi

323
00:49:40.045 –> 00:49:43.525
gradient log prob of a action given is, okay,

324
00:49:44.665 –> 00:49:48.965
now this G is going to be changed with the 80 this,

325
00:49:50.485 –> 00:49:52.935
this 80 will be coming up, whether you want

326
00:49:52.935 –> 00:49:54.895
to take two times step or three times step

327
00:49:54.915 –> 00:49:57.295
or key number of times, step into the future, okay?

328
00:49:57.755 –> 00:50:00.625
So this GT is getting modified with the

329
00:50:02.005 –> 00:50:03.215
generalized advantage.

330
00:50:03.435 –> 00:50:04.735
The rest all will remain same,

331
00:50:05.575 –> 00:50:06.575
Sir. But there

332
00:50:06.575 –> 00:50:08.795
in that a formula, generalized advantage,

333
00:50:08.795 –> 00:50:10.915
there was an R also rt,

334
00:50:11.865 –> 00:50:14.045
and that was the reward, I guess

335
00:50:14.515 –> 00:50:15.885
This ah, yeah.

336
00:50:15.995 –> 00:50:18.005
That is a reward for every timestamp

337
00:50:18.005 –> 00:50:19.285
that you have taken. Ah,

338
00:50:19.825 –> 00:50:24.245
And in that other Lillian’s, uh, blog, this

339
00:50:24.905 –> 00:50:26.605
RT becomes GT or

340
00:50:28.155 –> 00:50:31.095
No, this gt, the formula for GT is

341
00:50:32.935 –> 00:50:35.145
this is the reward

342
00:50:35.145 –> 00:50:38.025
that you opt in from timestamp T onward.

343
00:50:38.335 –> 00:50:41.145
Okay? So that will be RT r plus

344
00:50:41.215 –> 00:50:43.065
Plus comma into RT plus

345
00:50:43.195 –> 00:50:46.425
Gamma one T applied by RT plus one plus gamma

346
00:50:46.765 –> 00:50:49.545
Square R plus, yeah, yeah, it’ll go on, yeah,

347
00:50:49.545 –> 00:50:50.545
Yeah.

348
00:50:51.565 –> 00:50:55.865
So you can terminate this, uh, this gamma multiplied

349
00:50:55.865 –> 00:50:57.145
by RT factor.

350
00:50:57.325 –> 00:50:59.145
You can terminate it by assuming

351
00:50:59.175 –> 00:51:01.785
that you are getting into a new state into the future.

352
00:51:02.605 –> 00:51:04.465
In that future state, you are having some

353
00:51:04.465 –> 00:51:05.705
kind of health estimation.

354
00:51:05.705 –> 00:51:07.185
You can use that value estimation.

355
00:51:08.615 –> 00:51:11.035
So GT formalize is shown hard here,

356
00:51:12.855 –> 00:51:14.635
And that is why we are replacing GT

357
00:51:14.635 –> 00:51:16.715
by 80 in this concept. Mm-hmm.

358
00:51:17.095 –> 00:51:20.265
Yes Sir.

359
00:51:20.265 –> 00:51:22.225
How will we remember all these things in the exam?

360
00:51:22.365 –> 00:51:23.365
I’m really nervous.

361
00:51:25.605 –> 00:51:28.065
In exam, it is not about writing.

362
00:51:28.405 –> 00:51:31.585
Uh, this, the exam question will be conceptual

363
00:51:32.025 –> 00:51:36.925
question, exact equation.

364
00:51:37.225 –> 00:51:41.005
You may not be able to write it as it is, but

365
00:51:41.005 –> 00:51:44.885
however it should be some kind of mistake in time.

366
00:51:44.885 –> 00:51:45.885
Step is okay?

367
00:51:46.895 –> 00:51:51.505
Mm. I’ll show you the GI mean the

368
00:51:51.505 –> 00:51:53.145
calculation of your reward.

369
00:52:01.575 –> 00:52:05.325
This, this factor is called gt the samo.

370
00:52:05.355 –> 00:52:08.795
Yeah, RT plus of L. Okay?

371
00:52:08.795 –> 00:52:09.995
What sir? Where is gamma?

372
00:52:12.125 –> 00:52:16.055
Yeah, gamma, they have not, this is the residual

373
00:52:18.325 –> 00:52:19.355
total reward.

374
00:52:19.505 –> 00:52:23.355
This actually, the GT is basically this gamma to the power

375
00:52:23.355 –> 00:52:26.955
of T multiplied by RT plus of one.

376
00:52:27.545 –> 00:52:29.955
That is called your reward, total reward.

377
00:52:33.575 –> 00:52:35.625
This is called advantage function.

378
00:52:36.425 –> 00:52:38.645
And generalized advantage means you have

379
00:52:38.645 –> 00:52:40.245
to take into account of

380
00:52:43.185 –> 00:52:44.205
off every times step.

381
00:52:44.265 –> 00:52:48.525
You can take into the future from 1, 2, 3, onward.

382
00:52:48.765 –> 00:52:51.085
Anything you can take or every timestamp

383
00:52:51.085 –> 00:52:53.885
that we take into the future, you have a a delta function

384
00:52:53.885 –> 00:52:54.925
associated with that.

385
00:52:55.395 –> 00:52:58.885
This is what basically you have a delta function associated

386
00:52:58.905 –> 00:53:00.325
for every time step into the future.

387
00:53:01.145 –> 00:53:04.685
And again, if you consider this submission, that is delta

388
00:53:05.605 –> 00:53:08.085
T plus gamma to the gamma multiplied by delta.

389
00:53:08.225 –> 00:53:10.965
So basically your, all of your delta term is getting

390
00:53:11.645 –> 00:53:14.395
multiplied by your gamma factor, okay?

391
00:53:15.045 –> 00:53:19.315
So this is basically your instead of reward at a time.

392
00:53:19.475 –> 00:53:22.755
T you can consider delta at a time TT plus on like that

393
00:53:23.665 –> 00:53:25.685
some award them by the factor of gamma.

394
00:53:26.355 –> 00:53:31.345
That is what is called adv, uh, generalized advantage. Okay?

395
00:53:35.955 –> 00:53:39.775
So I will show you my python notebook from here.

396
00:53:40.735 –> 00:53:43.475
So we are going to follow again, a actor critic model.

397
00:53:44.595 –> 00:53:49.245
Only thing is this, uh, delta term.

398
00:53:50.505 –> 00:53:52.445
We are not taking only one time step.

399
00:53:52.665 –> 00:53:54.205
We are taking two times step

400
00:53:54.315 –> 00:53:55.805
that will be multiplied by gamma.

401
00:53:55.805 –> 00:53:58.765
Again, again, we’re taking three times step

402
00:53:58.765 –> 00:54:02.005
that will be multiplied by gamma square, okay?

403
00:54:02.845 –> 00:54:06.635
It is going to be like that. Okay?

404
00:54:06.655 –> 00:54:09.155
So I will show you this Python program now.

405
00:54:41.025 –> 00:54:44.005
So have you people gone through any kind of, uh, like

406
00:54:45.245 –> 00:54:48.165
language model in transformer in the course

407
00:54:48.345 –> 00:54:49.365
or out of the course,

408
00:54:53.485 –> 00:54:54.485
Sir? Very basically

409
00:54:54.485 –> 00:54:57.325
last year we did, um,

410
00:54:58.705 –> 00:55:00.885
we did go through a high level overview

411
00:55:00.885 –> 00:55:03.525
of the transform architecture.

412
00:55:05.015 –> 00:55:09.845
Okay? So basically I will show you one use case

413
00:55:10.615 –> 00:55:12.005
using transform architect.

414
00:55:12.205 –> 00:55:14.965
I mean, uh, this use of reinforcement learning

415
00:55:14.985 –> 00:55:17.565
and language model, which is going

416
00:55:17.565 –> 00:55:21.045
to be on a LM basic thing.

417
00:55:23.515 –> 00:55:26.275
I hope all of you are familiar with these things, right?

418
00:55:27.465 –> 00:55:31.275
How to do the supervised, fine tuning for

419
00:55:32.205 –> 00:55:33.765
LLM transformer and all.

420
00:55:35.545 –> 00:55:38.525
So only some prompting we were taught, so nothing great,

421
00:55:39.505 –> 00:55:42.965
and maybe that, um, some temperature parameter, et cetera.

422
00:55:45.245 –> 00:55:47.935
Okay? Nothing, nothing in deep, at least for me.

423
00:55:49.055 –> 00:55:53.365
Okay? Next semester we have sir,

424
00:55:55.215 –> 00:55:55.715
In the current,

425
00:56:06.995 –> 00:56:09.335
Sir, today’s, uh, this, uh, Lillian Wes link.

426
00:56:09.395 –> 00:56:12.015
You will, uh, post or is it the same link, sir?

427
00:56:13.045 –> 00:56:15.415
Same link. Okay, for one topic,

428
00:56:15.595 –> 00:56:17.335
she has made a large post like that.

429
00:56:17.965 –> 00:56:21.305
Okay, sir, This one I’ll give you,

430
00:56:22.325 –> 00:56:25.015
Uh, this is easy in your notes on ai,

431
00:56:26.615 –> 00:56:28.315
You know already the site, no,

432
00:56:28.375 –> 00:56:31.875
No, uh, it, it was very easy to search ly with you.

433
00:56:33.035 –> 00:56:36.775
Mm, uh, let me give it to you in your chat window itself.

434
00:56:36.885 –> 00:56:39.995
Then I’ll send it. What is that? Uh,

435
00:57:01.795 –> 00:57:03.095
it can open from your own.

436
00:57:15.475 –> 00:57:18.725
Okay, so this is a generalized advantage information.

437
00:57:19.405 –> 00:57:21.605
I think I have already executed this.

438
00:57:21.945 –> 00:57:25.035
Yes, I had already executed.

439
00:57:25.695 –> 00:57:30.195
So yeah, so I am just going to show you

440
00:57:31.545 –> 00:57:34.995
what is the, how we have, uh,

441
00:57:35.095 –> 00:57:37.475
how this has been coded out.

442
00:57:37.545 –> 00:57:41.135
Okay? This 80 formula, you can take any time,

443
00:57:41.135 –> 00:57:45.925
step into the future that is the upon you, okay?

444
00:57:47.075 –> 00:57:49.525
However, the gradient of your

445
00:57:50.075 –> 00:57:52.205
objective function is not changing at all.

446
00:57:52.745 –> 00:57:56.165
The gradient, as I told here, is going to be GT multiplied

447
00:57:56.165 –> 00:57:58.245
by grad gradient of log.

448
00:57:58.245 –> 00:58:00.645
Probability of action given is

449
00:58:02.205 –> 00:58:05.295
this GT is modified for generalized advantage estimation

450
00:58:06.155 –> 00:58:10.615
by taking action A, um, by taking this a

451
00:58:11.135 –> 00:58:13.655
notation of a, okay, this is the only difference.

452
00:58:15.035 –> 00:58:18.735
So I’m just going to show you this formula

453
00:58:18.915 –> 00:58:20.705
for this, okay?

454
00:58:20.775 –> 00:58:22.585
This is what is basically code up.

455
00:58:25.915 –> 00:58:29.815
So this 80 Infiniti means you can take infonet number

456
00:58:29.815 –> 00:58:32.775
of steps into the future for that.

457
00:58:32.995 –> 00:58:36.615
So as I told you, just like if you replace this delta with

458
00:58:37.155 –> 00:58:42.095
reward r, instantaneous reward R, you get the sum

459
00:58:42.095 –> 00:58:43.575
of your discounted reward, right?

460
00:58:44.045 –> 00:58:48.695
Instead of that, here, they’re taking delta T

461
00:58:51.165 –> 00:58:53.545
pre multiplied by kama factor.

462
00:58:53.845 –> 00:58:58.635
So it basically sum of discounted delta thing, okay?

463
00:59:01.825 –> 00:59:04.395
Now the, this sum

464
00:59:04.395 –> 00:59:06.235
of discounted delta is basically

465
00:59:06.235 –> 00:59:07.755
converted into this formula.

466
00:59:09.085 –> 00:59:10.895
This is simplified version of this formula.

467
00:59:11.035 –> 00:59:14.785
So this, this means that you have hello function

468
00:59:15.005 –> 00:59:19.545
for the timestamp st, okay, that you can take it

469
00:59:19.565 –> 00:59:20.785
as a subtraction factor

470
00:59:21.385 –> 00:59:25.475
and the plus factor is your sum

471
00:59:25.475 –> 00:59:26.675
of discounted reward.

472
00:59:26.775 –> 00:59:29.435
So gamma to the part of L, this is gamma

473
00:59:29.435 –> 00:59:30.675
to the part of L, okay?

474
00:59:31.225 –> 00:59:34.275
Then RT plus LL is starting from zero.

475
00:59:35.165 –> 00:59:38.445
So that means this is basically called sum of

476
00:59:46.155 –> 00:59:46.505
thing.

477
00:59:46.615 –> 00:59:48.945
This compute generalized estimation.

478
00:59:49.135 –> 00:59:52.455
This function is doing that job only,

479
00:59:54.265 –> 00:59:58.925
Okay, where we lost you at the time,

480
00:59:58.945 –> 01:00:00.565
we were explaining that sigma thing,

481
01:00:01.635 –> 01:00:02.635
Okay?

482
01:00:02.785 –> 01:00:03.885
Now it is connected back,

483
01:00:04.155 –> 01:00:05.155
Yeah.

484
01:00:05.405 –> 01:00:08.235
Okay. So I’m just going to show you

485
01:00:08.975 –> 01:00:10.035
the change in this program.

486
01:00:10.145 –> 01:00:12.235
That is, I’m going to show you the function

487
01:00:13.095 –> 01:00:16.675
to do this coding part that is value function

488
01:00:17.695 –> 01:00:22.035
for the timestamp st plus of sum of discounted reward.

489
01:00:22.145 –> 01:00:24.925
Okay? So this part is shown over here.

490
01:00:25.275 –> 01:00:26.965
This is the compute generalized

491
01:00:26.965 –> 01:00:28.445
advantage estimation function.

492
01:00:29.185 –> 01:00:31.885
So you have a value added.

493
01:00:32.385 –> 01:00:34.805
So this is value added, but it is storing the value

494
01:00:34.905 –> 01:00:36.405
for every time step T

495
01:00:36.995 –> 01:00:39.845
because we are going to compute this

496
01:00:40.545 –> 01:00:42.365
for a particular times step 80.

497
01:00:42.755 –> 01:00:45.845
That means at the times step T, you want

498
01:00:45.845 –> 01:00:47.645
to compute the advantage estimate

499
01:00:47.665 –> 01:00:50.685
for which you look at the val value available

500
01:00:50.825 –> 01:00:52.885
to you at a times step T.

501
01:00:53.835 –> 01:00:56.455
So this value is basically it is a

502
01:00:58.355 –> 01:01:00.915
MPA area or torch, uh, tensor variable,

503
01:01:01.725 –> 01:01:04.435
which contains a value for every time step.

504
01:01:05.595 –> 01:01:09.365
Okay? And next value is basically it is given

505
01:01:09.425 –> 01:01:12.785
as a function argument itself, okay?

506
01:01:13.725 –> 01:01:18.555
So this generalized advantage estimate is this ga,

507
01:01:20.645 –> 01:01:24.275
this delta is basically for every time step,

508
01:01:24.275 –> 01:01:26.995
basically this is a length of reward means basically

509
01:01:28.215 –> 01:01:31.745
you have, you have

510
01:01:33.045 –> 01:01:37.045
starting from the small T, you can go into capital T number

511
01:01:37.065 –> 01:01:40.125
of times step because that is capital T is a total number of

512
01:01:40.995 –> 01:01:42.225
total number of times step.

513
01:01:42.325 –> 01:01:46.235
You can go into one episode, okay?

514
01:01:46.255 –> 01:01:50.435
So it is basically length of the episode here, okay?

515
01:01:50.935 –> 01:01:53.875
And then for every time step, you have a reward

516
01:01:53.875 –> 01:01:57.555
that is called instantaneous reward, reward step, this plus

517
01:01:58.595 –> 01:02:00.475
multiplied by value.

518
01:02:03.205 –> 01:02:07.165
This value is basically the

519
01:02:08.975 –> 01:02:10.925
value available to you in the future.

520
01:02:11.235 –> 01:02:12.805
Just one time step ahead to you.

521
01:02:12.905 –> 01:02:15.605
So gamma multiplied by this value,

522
01:02:15.795 –> 01:02:19.565
this basically if you consider this is rt, this is RT,

523
01:02:20.035 –> 01:02:24.915
this is value ST plus one because this is called timestamp.

524
01:02:25.015 –> 01:02:27.595
So at the timestamp of T plus one,

525
01:02:28.025 –> 01:02:29.475
this is the value available to you.

526
01:02:29.905 –> 01:02:32.315
That means this part is basically containing your

527
01:02:33.255 –> 01:02:34.895
TD target, okay?

528
01:02:36.115 –> 01:02:39.775
And then the this mask variable is used that

529
01:02:41.035 –> 01:02:43.895
how many times stepss values are available to you.

530
01:02:44.565 –> 01:02:48.855
This mask is a one dimensional area. It’ll be one.

531
01:02:49.675 –> 01:02:53.455
If the values are available to you into this area,

532
01:02:54.125 –> 01:02:55.175
then it’ll be one.

533
01:02:56.885 –> 01:03:00.025
Otherwise, let’s say it is like, uh,

534
01:03:00.125 –> 01:03:04.065
for every instantaneous times step of T, okay,

535
01:03:04.085 –> 01:03:07.065
you are going into the next step of let’s say T plus one.

536
01:03:07.065 –> 01:03:08.225
You are not going beyond that.

537
01:03:08.805 –> 01:03:11.305
So beyond that, those masks will be disable here.

538
01:03:12.895 –> 01:03:16.785
Suppose you are going to consider for times step two, so V

539
01:03:16.785 –> 01:03:19.425
of T plus two, that will be available to you.

540
01:03:20.045 –> 01:03:23.725
And after that, this mask at time,

541
01:03:23.925 –> 01:03:25.765
T time T plus one will be available to you.

542
01:03:26.035 –> 01:03:28.285
Time T plus two, one, what? It’ll be set to zero.

543
01:03:30.065 –> 01:03:33.165
So this mask is just to say, just to convey

544
01:03:34.095 –> 01:03:35.625
into this program that

545
01:03:36.495 –> 01:03:38.585
what is the total number timestamp you

546
01:03:38.585 –> 01:03:39.625
are getting into the future.

547
01:03:39.805 –> 01:03:41.785
You are not going to export the whole times

548
01:03:41.785 –> 01:03:42.905
steps available to you.

549
01:03:44.455 –> 01:03:47.195
And this one is basically your tele function at the

550
01:03:47.195 –> 01:03:48.515
timestamp of st.

551
01:03:49.405 –> 01:03:52.865
So this stands for your, just like your TD function,

552
01:03:53.205 –> 01:03:56.265
TD learning, this stands for your error TD error, okay?

553
01:03:56.765 –> 01:03:59.705
So this is called delta. Look at, this is called delta.

554
01:04:01.185 –> 01:04:04.065
The definition of delta is given here, right?

555
01:04:04.215 –> 01:04:07.135
This one, this is a delta at the time step T,

556
01:04:08.305 –> 01:04:12.845
so RT plus of gamma multiplied by VST plus one minus of VST,

557
01:04:12.865 –> 01:04:16.605
and that is what is pulled over RT plus gamma multiplied.

558
01:04:16.605 –> 01:04:20.005
But this VST plus one minus of value at st.

559
01:04:20.005 –> 01:04:22.885
So this is your delta, and this delta, you are going

560
01:04:22.885 –> 01:04:25.005
to take it for your this submission.

561
01:04:26.995 –> 01:04:29.975
At every time step of delta, you need to say, yeah,

562
01:04:29.975 –> 01:04:32.495
so this delta, this, this submission you need to do.

563
01:04:33.955 –> 01:04:38.585
Okay? So for this submission, what are they doing?

564
01:04:38.585 –> 01:04:42.305
This delta. So this GA is basically now a,

565
01:04:43.115 –> 01:04:45.405
they’re considering to be multiplicative factor

566
01:04:45.405 –> 01:04:49.385
because there is this part coming up,

567
01:04:49.815 –> 01:04:52.545
this gamma multiplied by this delta.

568
01:04:52.925 –> 01:04:55.985
So from taking the multiplicative, if you don’t have gamma,

569
01:04:55.985 –> 01:04:56.985
then you can just do it.

570
01:04:57.655 –> 01:05:01.545
Just normal summation. This J is gonna j plus of this thing.

571
01:05:02.185 –> 01:05:04.195
However, it is going to be multipli factor.

572
01:05:04.215 –> 01:05:07.595
So to pull this J here, multiplicative top, okay?

573
01:05:08.935 –> 01:05:11.315
So that is what is your generalized advantage

574
01:05:11.675 –> 01:05:14.145
estimate, okay?

575
01:05:14.725 –> 01:05:18.445
Now this will be called inside your ACT method.

576
01:05:19.695 –> 01:05:21.715
So this is the act, act method.

577
01:05:22.685 –> 01:05:25.705
In the case of ACT method, what we do is we have a

578
01:05:26.245 –> 01:05:28.265
two separate neural network.

579
01:05:28.525 –> 01:05:30.425
One is for act, other is for critic.

580
01:05:31.165 –> 01:05:33.545
Act is basically standing for your policy network.

581
01:05:33.565 –> 01:05:38.105
Policy network means it’ll take the, it’ll accept the input,

582
01:05:38.735 –> 01:05:40.825
your state variable,

583
01:05:41.285 –> 01:05:43.265
and it’ll produce the output Azure,

584
01:05:44.735 –> 01:05:47.355
all possible actions under every state.

585
01:05:48.255 –> 01:05:51.555
So your output size is basically action dimension.

586
01:05:52.965 –> 01:05:55.745
In case of critic, critic uses poli.

587
01:05:55.765 –> 01:05:59.625
Uh, it is a neur network that is working on value function

588
01:06:01.615 –> 01:06:04.405
actor is a neur network that is working on Q function.

589
01:06:04.715 –> 01:06:07.125
Okay? So for value function, you are going

590
01:06:07.125 –> 01:06:08.485
to produce one single scaler.

591
01:06:08.555 –> 01:06:13.485
That how good is the, I mean, what is the expected utility

592
01:06:14.345 –> 01:06:16.045
for living in a particular state?

593
01:06:17.205 –> 01:06:19.105
So it’ll accept the argument which is

594
01:06:19.105 –> 01:06:20.265
belong the state space.

595
01:06:20.675 –> 01:06:22.105
It’ll produce only single health.

596
01:06:24.115 –> 01:06:25.495
So this is the forward function.

597
01:06:25.495 –> 01:06:28.325
In the forward I’m producing logic.

598
01:06:28.615 –> 01:06:30.005
Logic is coming from the actor.

599
01:06:30.625 –> 01:06:32.445
And Q help is coming from your critic.

600
01:06:35.155 –> 01:06:38.845
This compute GA will be calling every time.

601
01:06:39.465 –> 01:06:41.045
So this is the training module now.

602
01:06:41.945 –> 01:06:45.285
So in the training module, I start initiation from any,

603
01:06:48.355 –> 01:06:51.255
uh, I reset it to initial state.

604
01:06:52.425 –> 01:06:55.595
Then I have two set of optimizer here, two set of optimizer.

605
01:06:59.175 –> 01:07:02.555
Then I am basically starting

606
01:07:02.895 –> 01:07:04.835
for all the episode, okay?

607
01:07:04.835 –> 01:07:07.715
The auto loop is for all the episode, the inner loop is

608
01:07:08.295 –> 01:07:09.875
within a particular episode.

609
01:07:10.075 –> 01:07:12.915
I am stepping through all the states within an episode.

610
01:07:15.405 –> 01:07:17.665
So first one is this is the starting state.

611
01:07:18.855 –> 01:07:20.995
The starting state. I’m putting into the,

612
01:07:22.705 –> 01:07:24.405
I’m giving it to my model.

613
01:07:24.675 –> 01:07:28.605
That is network. It is producing logic and value.

614
01:07:29.175 –> 01:07:31.945
Value is basically your how good it is

615
01:07:31.945 –> 01:07:33.185
to live in a particular state.

616
01:07:33.645 –> 01:07:36.225
How good it is to live in a particular state of this st.

617
01:07:36.885 –> 01:07:40.495
So it’ll produce, it is produce a logic,

618
01:07:40.915 –> 01:07:43.255
it is producing logic that you can convert into a

619
01:07:43.255 –> 01:07:44.455
categorical distribution.

620
01:07:46.645 –> 01:07:48.615
Then this from this categorical distribution,

621
01:07:48.615 –> 01:07:50.655
if you call the sample function, you will get a

622
01:07:51.535 –> 01:07:53.075
action, okay?

623
01:07:53.375 –> 01:07:56.255
You’ll get action. This action, if you stepping

624
01:07:56.255 –> 01:07:58.655
through your, from your environment, you’ll get a new state,

625
01:07:59.835 –> 01:08:02.095
new reward for taking the action

626
01:08:02.205 –> 01:08:03.855
termination state you are reaching.

627
01:08:04.195 –> 01:08:06.135
The new state is a termination state

628
01:08:06.275 –> 01:08:10.245
or a ation state set.

629
01:08:10.245 –> 01:08:13.365
The down variable, if you’re getting into terminal state

630
01:08:13.425 –> 01:08:14.645
or ation state,

631
01:08:15.945 –> 01:08:19.285
can you keep on adding your lock probability of all

632
01:08:19.285 –> 01:08:22.635
of your action For all of this action,

633
01:08:23.865 –> 01:08:26.205
you are transitioning to a new state

634
01:08:26.305 –> 01:08:28.045
for which you have a probability

635
01:08:29.885 –> 01:08:31.425
for transitioning this new state.

636
01:08:31.455 –> 01:08:34.185
This probability, you can get it from this distribution,

637
01:08:35.215 –> 01:08:38.585
this distribution functions law, probability function,

638
01:08:40.505 –> 01:08:42.365
and you keep on adding all of this.

639
01:08:42.545 –> 01:08:45.205
So all of this addition is happening for all of your

640
01:08:46.015 –> 01:08:48.005
state action pair, state action pair.

641
01:08:48.065 –> 01:08:51.485
You just keep on getting your probability of new state.

642
01:08:51.865 –> 01:08:55.725
We add them out. This is happening within a single episode.

643
01:08:56.115 –> 01:08:58.325
That means in a single episode, you have collected all

644
01:08:58.325 –> 01:09:00.685
of your log probability reward.

645
01:09:01.195 –> 01:09:06.075
Okay? So this inner loop is closing here.

646
01:09:06.465 –> 01:09:08.475
This inner loop is closing here.

647
01:09:08.575 –> 01:09:11.355
Now, once one inner loop is closed,

648
01:09:11.355 –> 01:09:12.995
that means you have completed one episode.

649
01:09:13.415 –> 01:09:16.595
Now you can use those collected

650
01:09:16.595 –> 01:09:18.115
information of law probability.

651
01:09:18.115 –> 01:09:21.835
That means probability of next date transition reward,

652
01:09:21.835 –> 01:09:22.955
that you have got everything.

653
01:09:22.955 –> 01:09:25.435
You have got it in a single episode.

654
01:09:25.575 –> 01:09:30.295
Now you are using the training of the model, okay?

655
01:09:30.355 –> 01:09:34.465
So you are having a next state.

656
01:09:36.215 –> 01:09:37.945
This next state is coming from the

657
01:09:38.595 –> 01:09:40.685
next state variable under the environment here.

658
01:09:40.745 –> 01:09:43.325
So this next state, you can give it to your model.

659
01:09:43.425 –> 01:09:44.925
You are going to give a next value.

660
01:09:45.595 –> 01:09:47.485
This next value is going to be used

661
01:09:47.625 –> 01:09:50.765
as a argument into your form two, two GI function.

662
01:09:51.355 –> 01:09:55.805
Okay? And the values are already coming up from your

663
01:09:56.415 –> 01:10:00.615
which place values is coming up, is coming from here

664
01:10:02.735 –> 01:10:07.115
at every timestamp, you have taken a new action, okay?

665
01:10:07.825 –> 01:10:09.795
Your model will return the critic value.

666
01:10:09.795 –> 01:10:13.875
That is this value, this value you keep on appending,

667
01:10:14.615 –> 01:10:17.435
you keep on appending to make this variable called values.

668
01:10:19.165 –> 01:10:24.125
Okay? So by using this area of storage of your value,

669
01:10:25.095 –> 01:10:27.835
the next value, single value, that is a scaler thing.

670
01:10:29.295 –> 01:10:32.875
Now, if you go and look at your compute GA function,

671
01:10:34.755 –> 01:10:37.045
this is a compute GA something, okay?

672
01:10:37.395 –> 01:10:38.525
This is a scaler value.

673
01:10:38.785 –> 01:10:41.685
To make that to a vector, you will just need

674
01:10:41.685 –> 01:10:43.325
to mention it within a third bracket.

675
01:10:43.995 –> 01:10:45.205
Then you can add, then

676
01:10:45.205 –> 01:10:47.565
that means one element is getting appended here,

677
01:10:47.905 –> 01:10:49.005
it is not the submission,

678
01:10:49.065 –> 01:10:50.525
it is one element is getting append.

679
01:10:51.305 –> 01:10:54.695
So you have this value, you can compute this ga.

680
01:10:55.315 –> 01:10:59.075
So this compute GA is going to be

681
01:11:00.525 –> 01:11:04.885
all to find out your return, okay?

682
01:11:05.025 –> 01:11:08.365
And this return is your, it is going to be used

683
01:11:08.505 –> 01:11:09.765
for finding the advantage.

684
01:11:09.985 –> 01:11:14.705
So advantage is going to be return minus of your values.

685
01:11:15.705 –> 01:11:17.835
Okay? This values is the add variable.

686
01:11:17.835 –> 01:11:18.955
This is also add variable.

687
01:11:19.305 –> 01:11:22.115
This stands for at every timestamp what is the return.

688
01:11:23.085 –> 01:11:26.175
This is your advantage. This advantage, you have

689
01:11:26.175 –> 01:11:27.415
to use it for training.

690
01:11:27.715 –> 01:11:29.615
You have to multiply with the log probability

691
01:11:30.045 –> 01:11:33.575
because post function, if you look at the

692
01:11:34.845 –> 01:11:36.165
derivative of the cost function,

693
01:11:38.645 –> 01:11:41.185
it is this part all the time gradient.

694
01:11:41.385 –> 01:11:45.945
I mean your advantage multiplied by gradient of law.

695
01:11:45.945 –> 01:11:48.905
Prob, look at this gradient. We’re not finding it out.

696
01:11:49.485 –> 01:11:50.985
We are, we are hoping,

697
01:11:52.385 –> 01:11:56.485
and we are depending on the auto grad function of PyTorch.

698
01:11:56.705 –> 01:12:01.525
So we are multiplying this advantage with this lock prob

699
01:12:01.625 –> 01:12:05.125
of action A given is, so this lock prob is already stored.

700
01:12:06.775 –> 01:12:08.385
This lock code is coming up from here.

701
01:12:08.735 –> 01:12:12.505
This is the lock probative action at a particular state.

702
01:12:13.055 –> 01:12:16.835
Okay? So that is a lock probability multiplied

703
01:12:16.895 –> 01:12:18.395
by your advantage value.

704
01:12:18.985 –> 01:12:22.805
Take the A of all of them. That is a actor loss.

705
01:12:23.075 –> 01:12:24.205
What is a critic loss?

706
01:12:25.095 –> 01:12:29.975
Critic loss is critic loss is basically this means

707
01:12:29.975 –> 01:12:34.245
square error of this thing that is return

708
01:12:34.895 –> 01:12:39.285
minus of value whole square.

709
01:12:39.585 –> 01:12:41.885
So that’s why this is power to the part two.

710
01:12:41.885 –> 01:12:45.335
And then take the advantage of this a, of this

711
01:12:47.025 –> 01:12:49.155
advantage square variable.

712
01:12:49.185 –> 01:12:50.715
Okay? So that is the critical loss.

713
01:12:51.585 –> 01:12:55.075
Then you do this optimizer standard mechanism

714
01:12:55.175 –> 01:12:57.915
of doing the back propagation, set the gradient to be zero,

715
01:12:58.545 –> 01:13:02.565
then do a loss backward and do a step function.

716
01:13:03.385 –> 01:13:08.165
Mm-hmm. And here is the

717
01:13:08.165 –> 01:13:11.605
output of that for your cart full environment.

718
01:13:12.785 –> 01:13:15.565
As the episode is increasing, your

719
01:13:17.345 –> 01:13:19.145
expected sum of discounted reward

720
01:13:20.235 –> 01:13:21.495
is not increasing too much.

721
01:13:21.495 –> 01:13:26.005
However, it is settling down to 10, the value

722
01:13:26.065 –> 01:13:27.685
of nine 10, okay?

723
01:13:28.995 –> 01:13:31.775
So it is basically providing a stable version.

724
01:13:32.875 –> 01:13:35.765
Your, uh, expected sum

725
01:13:35.765 –> 01:13:38.045
of discounted reward is not jumping too much.

726
01:13:38.065 –> 01:13:42.415
If you look at it, it provides a stability of your output.

727
01:13:44.315 –> 01:13:49.235
Even though the first episode, it has gone for reward of 27.

728
01:13:49.235 –> 01:13:51.595
After that, it has settled down quickly.

729
01:13:52.405 –> 01:13:55.195
That means your, uh, algorithm

730
01:13:56.505 –> 01:13:57.845
is a stable algorithm.

731
01:13:58.235 –> 01:14:00.005
Your reward is not jumping too much.

732
01:14:10.105 –> 01:14:14.225
This formula is not implemented. This is from TD learning.

733
01:14:16.575 –> 01:14:20.065
What is implemented here is this part alone.

734
01:14:27.355 –> 01:14:30.775
Okay? So do you have any question,

735
01:14:36.505 –> 01:14:36.725
sir?

736
01:14:36.725 –> 01:14:38.645
We have to figure out these. Yeah.

737
01:14:39.185 –> 01:14:43.125
Uh, these programs are not required for you to get expertise

738
01:14:43.125 –> 01:14:47.905
because this, I’m doing for you to get a brief,

739
01:14:48.045 –> 01:14:51.335
uh, understanding about this. Yeah.

740
01:14:51.755 –> 01:14:55.575
So sir, in in summary, these advantages are used, uh,

741
01:14:55.595 –> 01:14:58.775
to compute the loss and later for back propagation, right?

742
01:14:59.635 –> 01:15:01.645
Mm, yes. Okay.

743
01:15:03.265 –> 01:15:05.685
So for every neural network,

744
01:15:05.785 –> 01:15:07.125
you would require a loss function

745
01:15:07.915 –> 01:15:10.985
and the loss function, uh, you have

746
01:15:11.005 –> 01:15:14.105
to find out your gradient.

747
01:15:15.965 –> 01:15:20.295
So this is your gradient

748
01:15:20.295 –> 01:15:21.295
of your loss function.

749
01:15:21.445 –> 01:15:26.295
Loss function, okay? So this gradient,

750
01:15:26.675 –> 01:15:28.015
we are not doing it manually.

751
01:15:28.755 –> 01:15:32.725
What we’re doing is advantage multiplied by block

752
01:15:33.365 –> 01:15:37.375
A given S, and then we give it to,

753
01:15:37.935 –> 01:15:40.695
and they’ll do the, uh, back progression.

754
01:15:41.395 –> 01:15:44.325
This is for policy network for the value network.

755
01:15:46.035 –> 01:15:48.755
This is not the, this is not the loss function.

756
01:15:48.815 –> 01:15:53.415
The value network loss function is the reward

757
01:15:55.135 –> 01:15:59.645
minus of value function available at that particular time.

758
01:15:59.645 –> 01:16:03.545
Instead. So here is, there are two loss function here.

759
01:16:03.975 –> 01:16:05.985
That is why I have two network here.

760
01:16:08.395 –> 01:16:12.125
Your actor losses, the one that you always do

761
01:16:12.385 –> 01:16:14.005
for gradient lock probability

762
01:16:14.625 –> 01:16:17.685
and your critic losses advantage whole square.

763
01:16:19.245 –> 01:16:21.825
So it is basically means squared error for the advantage.

764
01:16:21.895 –> 01:16:24.265
Your advantage is return minus of your health.

765
01:16:29.095 –> 01:16:30.985
Okay? Sir, I’m not sharing this program

766
01:16:31.495 –> 01:16:34.585
because these are not going to be tested

767
01:16:34.885 –> 01:16:36.885
for your exam.

768
01:16:43.565 –> 01:16:46.625
So next class, I will try to finish up PPO

769
01:16:46.625 –> 01:16:49.285
after that, this coming week,

770
01:16:51.105 –> 01:16:53.115
this Saturday, I’ll finish up your PPO.

771
01:16:54.935 –> 01:16:57.225
This coming week is going to be the last class. Okay?

772
01:16:57.305 –> 01:16:59.145
I don’t need to extend it beyond that.

773
01:17:00.475 –> 01:17:03.175
So this coming week, that means Wednesday

774
01:17:03.175 –> 01:17:04.575
and 14th, uh, Saturday.

775
01:17:04.805 –> 01:17:05.815
I’ll finish up the course.

776
01:17:05.925 –> 01:17:10.415
Okay, well, I’ll only saw you some use case

777
01:17:10.415 –> 01:17:11.655
of this reinforcement learning

778
01:17:11.795 –> 01:17:14.175
and large language model training.

779
01:17:14.405 –> 01:17:17.295
Okay? This last week is going to be on application side.

780
01:17:23.715 –> 01:17:28.055
If you want anything more to be covered, let me know so

781
01:17:28.055 –> 01:17:29.055
that I can plan it.

782
01:17:29.195 –> 01:17:31.935
I’m thinking not to extend it beyond 14th. Okay?

783
01:17:37.325 –> 01:17:40.385
You can okay. Come up with your suggestion in, uh,

784
01:17:41.705 –> 01:17:43.755
seventh that day.

785
01:17:44.265 –> 01:17:48.655
Otherwise, we’ll finish up everything on 14th of this month,

786
01:17:53.065 –> 01:17:53.795
yeah, shows.

787
01:17:57.655 –> 01:18:01.725
Okay. Thank you, sir. Sir, you sir.

788
01:18:02.395 –> 01:18:06.965
Good night, sir. Okay, good. Thank you, sir.

[vimeo- [https://vimeo.com/1161872079?fl=pl&fe=cm](https://vimeo.com/1161872079?fl=pl&fe=cm)](https://www.notion.so/vimeo-https-vimeo-com-1161872079-fl-pl-fe-cm-30161edeb73980199392ca3b022179c6?pvs=21)

---

## 3. Mathematical Foundation

## 5. Training Loop Integration (1:05:00-1:13:00)

### Actor-Critic with GAE:

The professor demonstrated how GAE fits into the Actor-Critic training:

```python
# Training loop for one episode
for episode in range(num_episodes):
    states, actions, rewards, values, log_probs = [], [], [], [], []

    state = env.reset()

    # Collect trajectory
    while not done:
        # Actor: select action
        logits, value = model(state)
        dist = Categorical(logits)
        action = dist.sample()

        # Store information
        log_probs.append(dist.log_prob(action))
        values.append(value)

        # Environment step
        next_state, reward, done = env.step(action)
        rewards.append(reward)
        state = next_state

    # After episode completes:
    _, next_value = model(next_state)

    # Compute GAE advantages
    advantages = compute_gae(rewards, values, next_value)
    returns = advantages + values

    # Update Actor (policy network)
    actor_loss = -(log_probs * advantages.detach()).mean()

    # Update Critic (value network)
    critic_loss = (returns - values).pow(2).mean()
```

### Key Points from Professor (1:08:00):

1. **Two separate losses**:
    - Actor loss: Policy gradient with GAE advantage
    - Critic loss: MSE between returns and values
2. **Detach advantages**:
    - `.detach()` prevents gradients flowing to critic during actor update
    - Critical for stable training
3. **Episode-based updates**:
    - Collect full episode trajectory
    - Compute GAE after episode
    - Update both networks

## 6. Results & Observations (1:13:00-1:14:30)

### CartPole Performance:

The professor showed training results:

- **More stable** than REINFORCE
- **Less variance** than simple Actor-Critic
- Converges to reward ~910 quickly
- Maintains stability across episodes

Quote from lecture (1:13:45):

> "Look at the reward stability. It's not jumping too much. Even though first episode got 27, it settled down quickly. This shows the algorithm is stable."
> 

## 7. Exam Guidance (51:00-52:00)

### Professor's Statement:

> "In exam, it's not about writing exact equations. The exam questions will be conceptual. Some mistake in timestep notation is okay."
> 

### What to Focus On:

1. **Concept understanding**: Why GAE reduces variance
2. **Formula structure**: General form, not exact notation
3. **Relationship**: How δ_t relates to A_t
4. **Parameters**: Role of λ and γ
5. **Implementation flow**: Not exact code, but the logic

## 8. Key Takeaways

### Advantages of GAE:

1. ✅ **Variance Reduction**: Smoother than single-step TD error
2. ✅ **Bias-Variance Control**: λ parameter lets you tune the tradeoff
3. ✅ **Stability**: More reliable training than vanilla policy gradients
4. ✅ **Used in PPO**: Foundation for state-of-the-art algorithm

### Comparison Table:

| Method | Variance | Bias | Stability | Speed |
| --- | --- | --- | --- | --- |
| **REINFORCE** | Very High | Low | Poor | Slow |
| **Actor-Critic (1-step)** | Medium | Medium | Good | Fast |
| **GAE (λ=0.95)** | Low | Low-Medium | Excellent | Fast |
| **Monte Carlo (GAE λ=1)** | High | Very Low | Poor | Slow |

## 9. Connection to Next Topics

### PPO Preview (Sat urday's class):

- Uses GAE for advantage estimation
- Adds clipping mechanism for policy updates
- Current state-of-the-art for many RL tasks
- Used in ChatGPT's RLHF training

### LLM Application (Final class):

Professor mentioned (37:00-38:00):

- Will show reinforcement learning in language model training
- How GAE/PPO enables RLHF (Reinforcement Learning from Human Feedback)
- Real-world application of concepts learned

## 10. Resources

### Mentioned in Lecture:

1. **Lilian Weng's Blog**: Policy Gradient tutorial (same as before)
2. **GAE Paper**: "High-Dimensional Continuous Control Using Generalized Advantage Estimation" (Schulman et al., 2016)

### Professor's Note (57:00):

> "These programs are not required for expertise. I'm showing you for brief understanding. Not tested in exam."
> 

---

## 📌 Action Items:

1. Understand δ_t (TD error) concept thoroughly
2. Practice deriving A^(2)*t from δ_t and δ*{t+1}
3. Understand role of λ parameter (bias-variance tradeoff)
4. Review how GAE integrates with Actor-Critic
5. Prepare for PPO - builds directly on GAE

### TD Error Reminder (46:00-47:00):

**TD(0) Error** - One step ahead:

```
δ_t = r_t + γV(s_{t+1}) - V(s_t)
```

This is the basic building block!

### GAE Formula Progression (47:00-50:00):

**One-step advantage** (A^(1)):

```
A^(1)_t = δ_t
        = r_t + γV(s_{t+1}) - V(s_t)
```

**Two-step advantage** (A^(2)):

```
A^(2)_t = δ_t + γδ_{t+1}
        = r_t + γr_{t+1} + γ²V(s_{t+2}) - V(s_t)
```

**n-step advantage** (A^(n)):

```
A^(n)_t = δ_t + γδ_{t+1} + γ²δ_{t+2} + ... + γ^(n-1)δ_{t+n-1}
```

**Generalized Advantage (GAE(λ))**:

```
A^GAE(λ)_t = (1-λ) Σ_{n=1}^{∞} λ^{n-1} A^(n)_t

Simplified form:
A^GAE(λ)_t = Σ_{l=0}^{∞} (γλ)^l δ_{t+l}
```

### Key Parameters:

- **γ (gamma)**: Discount factor for future rewards
- **λ (lambda)**: Controls bias-variance tradeoff
    - λ = 0: Only immediate TD error (high bias, low variance)
    - λ = 1: Monte Carlo return (low bias, high variance)
    - λ = 0.95-0.99: Common in practice

## 4. Implementation Walkthrough (54:00-1:05:00)

### compute_gae() Function:

Professor showed the Python implementation with these components:

```python
def compute_gae(rewards, values, next_value, gamma=0.99, lam=0.95):
    """
    Compute Generalized Advantage Estimation

    Args:
        rewards: List of rewards [r_0, r_1, ..., r_T]
        values: List of value estimates [V(s_0), V(s_1), ..., V(s_T)]
        next_value: V(s_{T+1}) - value of next state
        gamma: Discount factor
        lam: Lambda parameter for GAE
    """
    gae = 0
    advantages = []

    # Work backwards from final timestep
    for t in reversed(range(len(rewards))):
        # TD error at time t
        delta = rewards[t] + gamma * next_value - values[t]

        # GAE accumulation
        gae = delta + gamma * lam * gae
        advantages.insert(0, gae)

        # Update next_value for previous timestep
        next_value = values[t]

    return advantages
```

### Critical Implementation Details (1:00:00-1:04:00):

1. **Mask Variable**:
    - Handles variable episode lengths
    - Set to 0 when episode terminates
    - Prevents using future values beyond episode boundary
2. **Value Arrays**:
    - `values[]`: Critic network output for each timestep
    - Stored during forward pass
    - Used in GAE computation after episode
3. **Return Calculation**:
    
    ```python
    returns = advantages + values
    ```
    
    This gives target values for critic training
    

# CLASS NOTES: RL03 - Generalized Advantage Estimation (GAE)

## Metadata

- **Video URL**: [https://vimeo.com/1161872079](https://vimeo.com/1161872079)
- **Duration**: ~78 minutes
- **Date**: February 2026
- **Topic**: Generalized Advantage Estimation (GAE)
- **Prerequisites**: Actor-Critic methods, TD learning
- **Next Topic**: Proximal Policy Optimization (PPO)

## 1. Course Context & Timeline

### Schedule Announced (37:00-38:00):

- **Last class**: February 21, 2026
- **End-semester exam**: First week of March (March 7)
- **This week**: GAE (Wednesday) + PPO (Saturday)
- **Next week**: Application to Large Language Models
- Professor plans to finish course by Feb 14

### Topics Remaining:

1. ✅ Generalized Advantage Estimation (today)
2. ⬜ Proximal Policy Optimization (PPO) - Saturday
3. ⬜ RL in LLM Training - final class

## 2. What is Generalized Advantage Estimation?

### Core Concept (41:00-43:00):

**Simple Advantage** (what we learned before):

```
A(s,a) = Q(s,a) - V(s)
```

**Generalized Advantage** extends this using TD(λ) learning:

- Instead of looking just 1 step ahead
- Look multiple steps into the future: n = 1, 2, 3, ...
- Weight each by λ (lambda factor)
- Combines bias-variance tradeoff

### The Philosophy (43:00-45:00):

Professor's explanation:

> "TD Lambda learning was applied to value functions. Now we want to apply the same concept to advantage functions."
> 

**Why GAE?**

- Reduces variance in policy gradients
- More stable training than simple advantage
- Balances immediate vs future rewards
- Core component of PPO algorithm