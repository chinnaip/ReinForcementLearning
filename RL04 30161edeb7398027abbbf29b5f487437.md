# RL04

1
00:05:44.425 –> 00:05:44.775
Hello.

2
00:05:47.865 –> 00:05:50.755
Good afternoon. Good afternoon. Good, good afternoon.

3
00:05:52.175 –> 00:05:53.435
So how are things going on?

4
00:05:58.805 –> 00:06:03.505
Going fine, sir. Fine. 19. Participant join till now.

5
00:06:05.495 –> 00:06:08.715
So this is, today’s the last topic of the course

6
00:06:08.785 –> 00:06:10.115
that I’m going to cover today.

7
00:06:11.385 –> 00:06:12.915
Next one week is going to be,

8
00:06:12.995 –> 00:06:16.755
I will be giving you some brief, uh, overview about how

9
00:06:17.395 –> 00:06:21.035
reinforcement learning is used in a large language model.

10
00:06:22.525 –> 00:06:26.135
Okay? And, uh, so your course is going

11
00:06:26.135 –> 00:06:27.335
to end the next Saturday,

12
00:06:28.865 –> 00:06:31.965
or if you want, uh, we can have it on everywhere next day

13
00:06:31.965 –> 00:06:33.685
because Saturday is a packed schedule for you, right?

14
00:06:34.225 –> 00:06:37.515
If you want,

15
00:06:37.515 –> 00:06:39.635
we can have it on 11th and then 18th.

16
00:06:40.855 –> 00:06:41.855
Yes sir. Yeah,

17
00:06:43.345 –> 00:06:44.915
Yeah, that’d be much helpful

18
00:06:44.915 –> 00:06:46.595
because a lot of our, uh,

19
00:06:46.595 –> 00:06:48.315
friends were actually working today,

20
00:06:48.335 –> 00:06:50.795
so they may not be attending Saturday’s class.

21
00:06:51.755 –> 00:06:54.165
Okay, fine. So we’ll have it on this Wednesday

22
00:06:54.165 –> 00:06:55.805
and then coming under 11

23
00:06:55.805 –> 00:06:59.435
and 18th, this is going to be application per se.

24
00:07:00.575 –> 00:07:02.985
Okay? So the last topic is,

25
00:07:04.885 –> 00:07:06.345
or one of the most, uh,

26
00:07:06.575 –> 00:07:09.025
important algorithm in policy gradient.

27
00:07:09.405 –> 00:07:10.585
Let me share you the screen.

28
00:07:21.075 –> 00:07:25.335
Actually, I do not give you much of the, uh,

29
00:07:25.885 –> 00:07:27.575
storyline of about all the things

30
00:07:27.575 –> 00:07:32.525
that is happening over here in, in,

31
00:07:32.585 –> 00:07:34.205
uh, LLM domain.

32
00:07:44.685 –> 00:07:49.335
Okay? So you go to this place, uh,

33
00:07:54.695 –> 00:07:56.005
white floating control.

34
00:07:59.585 –> 00:08:04.555
The way for the beginners to learn about this is PPO.

35
00:08:05.275 –> 00:08:06.035
I will go to them.

36
00:08:12.115 –> 00:08:15.075
There is a here, this is for the beginners level,

37
00:08:39.115 –> 00:08:39.405
okay?

38
00:08:39.485 –> 00:08:40.485
I will cover from here.

39
00:08:41.645 –> 00:08:45.535
In addition, I have A PPT, which will be covering about

40
00:08:50.265 –> 00:08:52.675
have A PPT, but it is only, um,

41
00:08:54.285 –> 00:08:55.515
how it is derived.

42
00:08:55.855 –> 00:08:57.875
Let me just check out whether I have it

43
00:08:57.905 –> 00:08:59.315
already downloaded or not.

44
00:09:09.375 –> 00:09:10.185
Plus not.

45
00:09:39.995 –> 00:09:41.655
So, yeah, so here I have it

46
00:09:45.715 –> 00:09:47.195
E PT for this also,

47
00:09:53.025 –> 00:09:54.845
let me go from my PPT itself.

48
00:10:02.415 –> 00:10:05.625
Okay? So we already know

49
00:10:07.265 –> 00:10:12.125
the objective of the policy gradient is to maximize,

50
00:10:13.525 –> 00:10:16.655
to maximize the probability of

51
00:10:18.195 –> 00:10:21.715
the action along the traject trajectory.

52
00:10:21.935 –> 00:10:24.515
So tau Tao is the trajectory, okay?

53
00:10:24.945 –> 00:10:26.275
This Tao is the trajectory.

54
00:10:27.835 –> 00:10:32.385
Our tau is the reward for the trajectory. Okay?

55
00:10:33.485 –> 00:10:35.585
So we are the objective.

56
00:10:35.585 –> 00:10:38.585
This is you theater, you, it is the objective function

57
00:10:38.805 –> 00:10:42.905
and gradient is the this delta ized by gradient

58
00:10:42.905 –> 00:10:44.465
of the objective function.

59
00:10:44.655 –> 00:10:47.625
Okay? So the objective function is only this much.

60
00:10:50.705 –> 00:10:55.595
That is some over this multiplication probability

61
00:10:55.735 –> 00:10:58.575
for tau multiplied by au.

62
00:10:58.605 –> 00:11:01.015
Okay? And when you take the gradient,

63
00:11:01.285 –> 00:11:04.375
then we have a log trick that we come up for this.

64
00:11:04.485 –> 00:11:08.985
That is we have to maximize this gradient

65
00:11:09.045 –> 00:11:12.705
of the log probability of probability of probability

66
00:11:12.705 –> 00:11:13.945
of tau, okay?

67
00:11:14.865 –> 00:11:17.465
Multiplied by art tau. So this is the objective

68
00:11:17.465 –> 00:11:20.145
for all the classes of policy gradient

69
00:11:20.725 –> 00:11:21.945
we have learned till now.

70
00:11:22.725 –> 00:11:24.835
First one is called reinforcement algorithm,

71
00:11:24.895 –> 00:11:27.515
or is called value vanilla policy gradient.

72
00:11:27.515 –> 00:11:32.405
Second one is called advantage estimation. Normal.

73
00:11:32.875 –> 00:11:35.725
Just advantage estimation in which we don’t

74
00:11:36.275 –> 00:11:37.445
calculate this art.

75
00:11:37.985 –> 00:11:42.085
We just take this reward for the whole reactive minus

76
00:11:42.185 –> 00:11:46.325
of your value function, which is basically advantage,

77
00:11:46.325 –> 00:11:48.965
basically difference between Q function and value function.

78
00:11:49.275 –> 00:11:52.355
Okay? And then we have a generalized advantage

79
00:11:52.415 –> 00:11:55.725
as these three things we have learned for last one

80
00:11:55.725 –> 00:11:56.725
or two weeks, okay?

81
00:11:58.165 –> 00:12:00.185
In which this term remains the same.

82
00:12:00.185 –> 00:12:01.985
That is a gradient of log probability.

83
00:12:02.535 –> 00:12:03.825
This term remains the same.

84
00:12:03.935 –> 00:12:06.505
Only this reward function is changing

85
00:12:06.655 –> 00:12:09.425
with this is the reward for the whole trajectory.

86
00:12:10.795 –> 00:12:13.445
This is called basic reinforcement algorithm.

87
00:12:14.295 –> 00:12:16.835
Second one was called actor critic,

88
00:12:16.845 –> 00:12:20.155
which was basically we are having a actor

89
00:12:20.375 –> 00:12:24.275
and a critic network by which we are actor is choosing the

90
00:12:24.615 –> 00:12:25.715
policy to be chosen.

91
00:12:26.225 –> 00:12:29.635
That is this one. And critic is finding out

92
00:12:29.635 –> 00:12:33.645
what is the value for this particular action

93
00:12:33.645 –> 00:12:36.045
that you are taking over the trajectory.

94
00:12:37.045 –> 00:12:38.975
Okay? So this is changing all the time.

95
00:12:39.005 –> 00:12:40.695
This reward how we are calculating,

96
00:12:40.755 –> 00:12:42.735
if we are considering act method,

97
00:12:42.805 –> 00:12:45.615
that is the normal act critique that is called

98
00:12:46.315 –> 00:12:48.655
you just modify this with advantage function.

99
00:12:48.685 –> 00:12:50.135
That is called difference

100
00:12:50.135 –> 00:12:51.855
between Q function and value function.

101
00:12:53.215 –> 00:12:57.935
In the case of, uh, in the case

102
00:12:57.995 –> 00:13:00.475
of today’s topic

103
00:13:00.855 –> 00:13:03.555
for it is called proximal policy optimization.

104
00:13:04.085 –> 00:13:07.035
There we are having a major deviation from the last two

105
00:13:07.035 –> 00:13:08.115
topics that we have done.

106
00:13:09.055 –> 00:13:12.405
We are going to modify here itself, okay?

107
00:13:13.085 –> 00:13:15.465
So that is one point for you to remember.

108
00:13:15.915 –> 00:13:18.545
We’re not only modifying this

109
00:13:20.515 –> 00:13:22.285
like in case of advantage estimation.

110
00:13:22.285 –> 00:13:24.965
Then last class witness that I have shown you

111
00:13:24.965 –> 00:13:27.525
what is called generalized advantage estimation.

112
00:13:28.065 –> 00:13:30.885
In case of generalized advantage estimation, we are applying

113
00:13:32.295 –> 00:13:35.915
it is basically TD in learning on this value function.

114
00:13:36.505 –> 00:13:41.155
Okay? Here today we are going

115
00:13:41.155 –> 00:13:42.515
to modify something in here.

116
00:13:42.695 –> 00:13:45.595
It is the main contribution of proximal policy optimization.

117
00:13:46.145 –> 00:13:49.525
Okay? Overall, what is the objective

118
00:13:49.525 –> 00:13:51.805
for every policy gradient is basically this

119
00:13:52.525 –> 00:13:54.165
increase the probability of path

120
00:13:55.105 –> 00:13:59.055
which has a positive reward, okay?

121
00:14:00.735 –> 00:14:02.595
And decrease the probability

122
00:14:02.975 –> 00:14:06.115
of the path which has a negative reward.

123
00:14:09.375 –> 00:14:11.675
So here is one more important line that is

124
00:14:13.285 –> 00:14:16.505
of all the policy gradient mechanism, we try

125
00:14:16.505 –> 00:14:19.665
to change the probability of the experience path

126
00:14:20.445 –> 00:14:24.105
and it does not change the path itself.

127
00:14:26.375 –> 00:14:30.805
It means that you are trying to increase

128
00:14:30.985 –> 00:14:32.245
or decrease the probability

129
00:14:33.025 –> 00:14:36.405
for the actions along the path, okay?

130
00:14:37.855 –> 00:14:39.955
It does not try to change the path itself.

131
00:14:42.475 –> 00:14:45.535
So if any action has resulted to positive reward,

132
00:14:45.595 –> 00:14:46.815
you will increase the probability.

133
00:14:46.995 –> 00:14:50.455
If any action is resulting to negative reward,

134
00:14:50.455 –> 00:14:54.255
it’ll decrease, decrease the priority of the path, priority

135
00:14:54.255 –> 00:14:55.895
of the action, not the path priority

136
00:14:55.895 –> 00:14:57.295
of the action along the path.

137
00:14:58.655 –> 00:15:00.665
Okay? So these are the decomposition method.

138
00:15:00.855 –> 00:15:03.705
Basically first class I had told you

139
00:15:03.705 –> 00:15:04.905
what is the meaning of this.

140
00:15:07.295 –> 00:15:10.875
So we have got into this phase that is gradient

141
00:15:10.935 –> 00:15:14.755
of log priority of the action given the state

142
00:15:16.465 –> 00:15:19.605
and T stands for all the time stepss along the particular

143
00:15:19.605 –> 00:15:21.325
trajectory, okay?

144
00:15:21.425 –> 00:15:25.005
So for every action that you’re taking along the trajectory,

145
00:15:25.225 –> 00:15:28.925
you are increasing the log probability for the action.

146
00:15:28.985 –> 00:15:31.285
If the action has resulted a positive reward,

147
00:15:31.285 –> 00:15:33.045
if the action has result a negative reward,

148
00:15:33.395 –> 00:15:35.845
then you decrease the probability for that.

149
00:15:36.235 –> 00:15:37.925
That is the main objective

150
00:15:37.925 –> 00:15:39.725
for every policy gradient type mechanism.

151
00:15:41.815 –> 00:15:45.865
Okay? Now what we are going to learn about these are

152
00:15:45.865 –> 00:15:46.985
for actor critic method.

153
00:15:47.325 –> 00:15:48.545
As I told you for team.

154
00:15:48.545 –> 00:15:51.985
Now, what I learned is modification on the reward part

155
00:15:52.995 –> 00:15:54.555
modification the reward part.

156
00:15:55.535 –> 00:15:57.875
So it is basically difference between your reward

157
00:15:57.875 –> 00:16:00.195
that you are obtaining for the whole trajectory, minus

158
00:16:00.335 –> 00:16:03.035
of your estimation for every state

159
00:16:03.105 –> 00:16:05.275
that you are transitioning into one by one.

160
00:16:05.815 –> 00:16:07.515
But every state you have some estimation.

161
00:16:07.515 –> 00:16:12.015
This value is coming from your critic model.

162
00:16:12.525 –> 00:16:14.935
This value is coming from critic model, okay?

163
00:16:15.355 –> 00:16:18.975
And this part that is action given particular state.

164
00:16:18.975 –> 00:16:21.535
This value is coming from your actor model.

165
00:16:21.675 –> 00:16:24.655
So last two classes we have seen actor critic model. Okay?

166
00:16:27.765 –> 00:16:30.745
Now coming up, what is policy gradient mechanism.

167
00:16:31.845 –> 00:16:33.305
So policy gradient mechanism,

168
00:16:33.645 –> 00:16:35.985
IT is taking a major deviation from all

169
00:16:35.985 –> 00:16:37.985
other previous algorithm.

170
00:16:38.455 –> 00:16:41.345
They’re going to make changes here in the probability

171
00:16:41.405 –> 00:16:44.875
of action given ace here.

172
00:16:45.055 –> 00:16:47.715
So for that, there are some little mathematical things

173
00:16:47.715 –> 00:16:48.835
that involved that is called,

174
00:16:49.575 –> 00:16:53.165
one is called import sampling, okay?

175
00:16:53.945 –> 00:16:56.805
So important sampling is a very, very important, uh,

176
00:16:58.145 –> 00:17:01.725
method in the statistical influence course.

177
00:17:02.645 –> 00:17:06.975
That is the major problem is see, forget about this

178
00:17:07.645 –> 00:17:08.655
denominator part.

179
00:17:08.945 –> 00:17:11.255
Don’t, don’t consider, don’t look at it right now.

180
00:17:11.645 –> 00:17:15.635
What is our objective basically is expected

181
00:17:16.735 –> 00:17:19.315
sum of discounted reward to be maximized.

182
00:17:19.315 –> 00:17:22.195
That is normal objective for any reinforcement learning

183
00:17:23.485 –> 00:17:25.095
poly in case of policy gradient,

184
00:17:25.095 –> 00:17:28.735
what we’re maximizing is expectation of this quantity.

185
00:17:28.765 –> 00:17:32.015
That is probability of tau multiplied by au.

186
00:17:32.045 –> 00:17:34.015
This denominator is not there, okay?

187
00:17:34.475 –> 00:17:37.255
So probably of tau given theta is basically the policy

188
00:17:37.365 –> 00:17:38.415
that you are learning.

189
00:17:40.365 –> 00:17:43.065
So the policy that you are learning, you have feed

190
00:17:43.065 –> 00:17:45.345
that policy into your neural network

191
00:17:46.485 –> 00:17:50.405
and then you are trying to learn what is the return

192
00:17:50.405 –> 00:17:51.725
that you’re getting for the policy.

193
00:17:52.625 –> 00:17:55.285
And if you change your one particular state action,

194
00:17:55.545 –> 00:17:58.415
you get a another kind of reward.

195
00:17:58.875 –> 00:18:00.895
Now you are going to do a comparison between them.

196
00:18:01.635 –> 00:18:03.415
Now, whenever we’re going to go comparison,

197
00:18:03.475 –> 00:18:06.975
you have already seen in case of uh, deep Q learning,

198
00:18:07.075 –> 00:18:09.615
if you remember, we are maintaining two neural network.

199
00:18:11.715 –> 00:18:15.545
One neural network is used first storing your

200
00:18:16.135 –> 00:18:17.785
real output y

201
00:18:18.245 –> 00:18:19.825
and another neural network

202
00:18:20.005 –> 00:18:21.425
by which you’re learning your policy.

203
00:18:22.245 –> 00:18:24.785
You can make changes of the weight of the neural network,

204
00:18:24.885 –> 00:18:27.905
of the policy network, not on the target network.

205
00:18:28.135 –> 00:18:31.545
Okay? So if you remember that DQ learning,

206
00:18:32.205 –> 00:18:35.465
we are using two Q network, the two network,

207
00:18:35.685 –> 00:18:38.905
two neural network for maintain, for assessing your

208
00:18:39.945 –> 00:18:43.865
Q function here in the case of policy gradient, so far

209
00:18:45.195 –> 00:18:48.215
in active critic network, we had two newer network

210
00:18:48.325 –> 00:18:51.135
that is fine, but in that case, actor critic,

211
00:18:51.355 –> 00:18:55.055
we are maintaining actor is modeling your policy.

212
00:18:55.245 –> 00:19:00.095
That is this part and critic is modeling your value

213
00:19:00.235 –> 00:19:01.335
for the particular action

214
00:19:01.395 –> 00:19:04.215
or value for living a particular state that is

215
00:19:05.445 –> 00:19:07.695
already learned by the critic method.

216
00:19:07.875 –> 00:19:09.735
Critic is the expert model, okay?

217
00:19:09.755 –> 00:19:12.095
So it knows it’s called teacher model,

218
00:19:12.355 –> 00:19:15.655
and the policy network is basically student student network.

219
00:19:15.885 –> 00:19:19.995
Okay? So can we utilize that kind of concept here?

220
00:19:21.945 –> 00:19:23.925
So that is why this denominator is coming.

221
00:19:23.985 –> 00:19:25.445
So denominator is basically,

222
00:19:25.445 –> 00:19:26.805
there is one more mathematical reason.

223
00:19:26.865 –> 00:19:29.925
The reason is we, if you don’t have this denominator,

224
00:19:30.385 –> 00:19:32.995
we are trying to maximize this new matter.

225
00:19:32.995 –> 00:19:37.155
What that is. Probative tau multiplied by tau given theta,

226
00:19:37.245 –> 00:19:38.715
theta is the weight of the neural

227
00:19:38.715 –> 00:19:41.075
network multiplied by art al.

228
00:19:41.775 –> 00:19:46.505
Now this probative tau given theta,

229
00:19:47.095 –> 00:19:48.945
this is a function of your theta,

230
00:19:49.525 –> 00:19:51.665
and if you’re changing that theta itself,

231
00:19:51.665 –> 00:19:53.745
that is you’re updating the neural network’s weight,

232
00:19:54.365 –> 00:19:57.305
you’ll not be able to find out the optimal weight

233
00:19:57.405 –> 00:19:59.585
for which these quantity is going to be maximized.

234
00:20:00.205 –> 00:20:04.105
So for that, what they do is they will do the

235
00:20:04.625 –> 00:20:08.985
optimization by considering that you are freezing the

236
00:20:09.885 –> 00:20:12.665
weight of your network to the previous policies value,

237
00:20:13.495 –> 00:20:16.385
that is why they’re considering a ratio

238
00:20:16.385 –> 00:20:18.545
between the current policy and the previous policy.

239
00:20:18.805 –> 00:20:21.305
So that is three. Is that just one step before?

240
00:20:21.305 –> 00:20:24.705
What was the policy? So you freeze that network

241
00:20:25.685 –> 00:20:30.345
and you just change one very little parameter in your

242
00:20:30.845 –> 00:20:34.345
ne network weight that will be corresponding

243
00:20:34.345 –> 00:20:36.185
to some particular state action.

244
00:20:36.515 –> 00:20:38.025
It’ll not change all the state

245
00:20:38.085 –> 00:20:41.145
and all the action only for few particular state,

246
00:20:41.165 –> 00:20:43.105
and it’ll make a little bit of change.

247
00:20:43.455 –> 00:20:44.985
That will become a new policy.

248
00:20:45.685 –> 00:20:48.085
So the, and the ratio between them is taken up

249
00:20:48.585 –> 00:20:52.245
and this ratio multiplied by the reward that is the quantity

250
00:20:52.355 –> 00:20:53.805
that they’re trying to maximize.

251
00:20:54.235 –> 00:20:56.325
This is all because there is a necessity.

252
00:20:57.405 –> 00:21:01.305
The necessity, you cannot do the simultaneous update

253
00:21:01.325 –> 00:21:04.385
and then ex uh, maximize your expectation.

254
00:21:04.925 –> 00:21:08.425
You cannot do that. That is why they’re decoupling it,

255
00:21:08.755 –> 00:21:12.305
decoupling by the help of taking of the one step

256
00:21:12.305 –> 00:21:13.425
before what was the policy.

257
00:21:14.525 –> 00:21:19.115
Okay? Now, in that doing that, if you take,

258
00:21:19.225 –> 00:21:20.835
this is your objective function,

259
00:21:21.265 –> 00:21:25.195
then if you apply a gradient for the current policy of the,

260
00:21:25.825 –> 00:21:27.115
then this gradient is going

261
00:21:27.115 –> 00:21:28.915
to be applicable on the new matter part,

262
00:21:29.415 –> 00:21:30.755
not on the denominator part.

263
00:21:31.665 –> 00:21:36.155
Okay? Then, uh, expectation of gradient

264
00:21:36.155 –> 00:21:39.875
of this thing, this is what they’re trying

265
00:21:39.875 –> 00:21:42.155
to maximize.

266
00:21:43.335 –> 00:21:47.735
Okay? So that is why the objective function

267
00:21:48.115 –> 00:21:51.135
for this is going to be written like this,

268
00:21:52.485 –> 00:21:54.415
that ratio of the current policy

269
00:21:54.475 –> 00:21:58.865
and the previous policy multiplied by the reward, okay?

270
00:21:59.945 –> 00:22:02.125
And this is called a surrogate loss function.

271
00:22:02.145 –> 00:22:05.365
Now, surrogate means basically this is the new loss

272
00:22:05.645 –> 00:22:10.485
function, which will ensure that if you do the optimization

273
00:22:10.485 –> 00:22:12.845
of the surrogate loss function, you will eventually

274
00:22:13.435 –> 00:22:17.085
modifying the original, original optimization.

275
00:22:17.085 –> 00:22:20.605
That is the new matter part. This part alone, okay?

276
00:22:20.635 –> 00:22:25.225
That is don’t, you don’t have any denominator part. Okay?

277
00:22:25.245 –> 00:22:29.145
So that is as a background of this, now I’ll, I’ll go back

278
00:22:29.165 –> 00:22:30.545
to your doc.

279
00:22:30.605 –> 00:22:33.515
Uh, you can go over here this

280
00:22:34.215 –> 00:22:37.185
hugging face, okay?

281
00:22:38.835 –> 00:22:40.975
So here I’ll go to your,

282
00:22:42.075 –> 00:22:45.115
the ratio function.

283
00:22:45.185 –> 00:22:49.915
Here, I’ll go this ratio function is what, uh, we are trying

284
00:22:49.935 –> 00:22:54.825
to, okay, so in case of policy gradient,

285
00:22:54.855 –> 00:22:56.425
this was an objective all the time.

286
00:22:56.895 –> 00:22:58.465
Lock probability of action given

287
00:22:58.865 –> 00:23:01.205
s multiplied by the advantage.

288
00:23:02.305 –> 00:23:06.995
Now this expectation means expectation is happening over

289
00:23:07.185 –> 00:23:08.675
what happening over the year.

290
00:23:09.535 –> 00:23:13.525
Individual time step for a particular projectile.

291
00:23:13.865 –> 00:23:17.295
For a particular projectile, you are going to have small T

292
00:23:18.425 –> 00:23:20.065
starting from zero to capital T.

293
00:23:20.295 –> 00:23:21.545
That means capital T number

294
00:23:21.545 –> 00:23:24.865
of transition you can make within a particular projectile.

295
00:23:25.055 –> 00:23:28.025
Okay? So this expectation is happening over

296
00:23:29.845 –> 00:23:32.915
one projectile, okay?

297
00:23:33.885 –> 00:23:34.105
Now

298
00:23:39.515 –> 00:23:41.715
directly they’re showing this, uh,

299
00:23:41.985 –> 00:23:44.515
clipped ob clipped objective function,

300
00:23:44.955 –> 00:23:46.365
surrogate objective function.

301
00:23:46.435 –> 00:23:47.885
They’re showing it directly.

302
00:23:48.825 –> 00:23:50.125
So before that, I’m just trying

303
00:23:50.125 –> 00:23:51.925
to look at whether they have written down

304
00:23:51.925 –> 00:23:53.645
what is called this ratio or not.

305
00:23:54.075 –> 00:23:58.205
Yeah. So this ratio, this ratio is basically

306
00:23:58.955 –> 00:24:00.165
probability of action.

307
00:24:00.165 –> 00:24:05.145
Given ace divided by prob of action given is

308
00:24:05.295 –> 00:24:10.085
with respect to the old policy, the old means one step

309
00:24:10.085 –> 00:24:11.445
before, what was your theater?

310
00:24:11.715 –> 00:24:13.765
Ethnicity was the old policy, okay?

311
00:24:14.065 –> 00:24:16.285
And this is what is the same thing over here.

312
00:24:17.965 –> 00:24:19.495
This one, okay?

313
00:24:20.155 –> 00:24:24.135
Now, as I told you, these are all approximation method.

314
00:24:24.635 –> 00:24:29.455
We are not going to be able to do this optimization.

315
00:24:29.455 –> 00:24:33.215
That prob of tau given theta multiplied by our Tao,

316
00:24:33.755 –> 00:24:35.735
an expectation over theta

317
00:24:35.995 –> 00:24:38.175
and that expectation is going to be maximized.

318
00:24:39.255 –> 00:24:40.865
Okay? That is not going to possible.

319
00:24:40.925 –> 00:24:42.305
So we go for the surrogate function.

320
00:24:42.305 –> 00:24:46.025
That is we are taking ratio between current policy

321
00:24:46.045 –> 00:24:50.555
and the old policy, but that is your objective function.

322
00:24:50.615 –> 00:24:52.995
Now, whenever we need a gradient, we need

323
00:24:52.995 –> 00:24:55.275
to take apply gradient over here, right?

324
00:24:55.275 –> 00:24:57.155
That is gradient for this.

325
00:24:58.365 –> 00:25:00.785
Now, for this gradient is also going to be

326
00:25:01.085 –> 00:25:05.455
for the approximate that here they’re saying you are

327
00:25:05.995 –> 00:25:07.975
not going to find out the gradient easily.

328
00:25:08.275 –> 00:25:09.935
So forget about gradient.

329
00:25:10.085 –> 00:25:12.375
This is all our approximation matter, okay?

330
00:25:13.115 –> 00:25:15.335
So when you’re forgetting about gradient, that means

331
00:25:16.655 –> 00:25:19.675
you can take the help of the, uh, auto grad function

332
00:25:19.675 –> 00:25:20.715
of your PyTorch

333
00:25:20.855 –> 00:25:25.195
or all kind of, uh, all kind of deep learning framework.

334
00:25:25.195 –> 00:25:27.275
Like tens floor also has their own

335
00:25:27.945 –> 00:25:29.235
auto differentiation method.

336
00:25:29.295 –> 00:25:30.355
You can take help of them.

337
00:25:31.565 –> 00:25:34.515
So you take this value on this value,

338
00:25:34.695 –> 00:25:38.405
if you have multiple sample collection, then you can

339
00:25:39.195 –> 00:25:42.925
some over all of those samples divide by total number

340
00:25:42.925 –> 00:25:46.325
of sample, you get a average value on that average value,

341
00:25:46.425 –> 00:25:48.685
you can apply just auto differentiation

342
00:25:49.205 –> 00:25:51.125
function from chain flow, anything,

343
00:25:51.545 –> 00:25:54.005
you’ll get a differential quantity for this.

344
00:25:54.115 –> 00:25:55.965
That is gradient of this, okay?

345
00:25:57.105 –> 00:26:00.405
Now, they have observed that that gradient is going to be

346
00:26:01.215 –> 00:26:02.245
noisy gradient.

347
00:26:02.245 –> 00:26:03.525
Noisy gradient. Why?

348
00:26:03.755 –> 00:26:08.165
Because, uh, this function is not a expl.

349
00:26:08.165 –> 00:26:09.925
As I told you, this function is not a

350
00:26:10.125 –> 00:26:11.245
explicit function of theater.

351
00:26:12.065 –> 00:26:13.925
Exp explicit function means

352
00:26:14.555 –> 00:26:17.285
like if I tell you PI theater is basically,

353
00:26:18.695 –> 00:26:22.845
let’s say it is action given a so action

354
00:26:22.875 –> 00:26:25.845
that is 80 multiplied by theta, one square,

355
00:26:26.195 –> 00:26:28.725
then plus 80 multiplied by theta two square

356
00:26:28.865 –> 00:26:31.645
and all theta one, theta two is basically your neural

357
00:26:31.645 –> 00:26:33.725
network weight according to the layers.

358
00:26:33.875 –> 00:26:37.525
Okay? If I can write a explicit formula

359
00:26:37.595 –> 00:26:39.285
that becomes explicit function,

360
00:26:39.825 –> 00:26:41.765
but if I cannot write any explicit formula,

361
00:26:41.875 –> 00:26:43.610
like if you can think of scientific,

362
00:26:43.855 –> 00:26:46.325
scientific is explicit formula, okay?

363
00:26:47.365 –> 00:26:51.225
But if you take a neural network, you have a output value,

364
00:26:51.325 –> 00:26:54.225
you have a input sample that you’re providing,

365
00:26:54.765 –> 00:26:57.385
so your output value is some combination of your weight

366
00:26:57.385 –> 00:27:01.795
and your input sample, that is not a explicit formula

367
00:27:01.875 –> 00:27:05.235
because that is going to be taking a lot of algebra

368
00:27:06.175 –> 00:27:08.235
dot product, algebra product,

369
00:27:08.235 –> 00:27:09.515
something like that is going to happen.

370
00:27:10.095 –> 00:27:12.435
If you have a dense network, it is not going to be easy

371
00:27:12.455 –> 00:27:15.795
to find out explicit relation between output and input.

372
00:27:16.375 –> 00:27:19.475
So in those cases, if you apply the gradient operator,

373
00:27:19.535 –> 00:27:21.195
you are going to get a noisy value.

374
00:27:22.135 –> 00:27:24.995
Now, to get rid of that noisy value, you are going

375
00:27:24.995 –> 00:27:26.395
to do a clipping operation.

376
00:27:27.385 –> 00:27:30.645
The clipping means if the gradient you set a threshold,

377
00:27:31.245 –> 00:27:34.445
threshold is silent, if silent value will be between zero

378
00:27:34.505 –> 00:27:36.885
and one, it should not be zero, it should not be one

379
00:27:36.885 –> 00:27:38.085
between between zero and one.

380
00:27:38.085 –> 00:27:42.005
Okay? Now, if this, uh, this value,

381
00:27:42.665 –> 00:27:46.805
if this valid ratio value is going to be greater than one,

382
00:27:47.515 –> 00:27:51.405
then you slice it on the top side

383
00:27:51.995 –> 00:27:56.605
that you should not go beyond one plus of asylum.

384
00:27:58.005 –> 00:28:01.705
As I told Fal value is less than one between you and one.

385
00:28:02.415 –> 00:28:06.325
So your top slicing upper cap value is

386
00:28:07.525 –> 00:28:08.545
one plus fal.

387
00:28:09.415 –> 00:28:13.065
Okay? And if your gradient value is going down

388
00:28:14.565 –> 00:28:19.485
by uh, below, uh, it, if it is going below

389
00:28:19.545 –> 00:28:23.905
of below, I think, uh, zero in that case,

390
00:28:24.295 –> 00:28:27.665
what you do is you reduce it

391
00:28:29.185 –> 00:28:30.485
by one minus silent.

392
00:28:31.645 –> 00:28:33.735
Okay? So that is what clipping operation

393
00:28:33.735 –> 00:28:35.095
and this clipping is given over here.

394
00:28:35.155 –> 00:28:36.775
We just take a look over here.

395
00:28:36.935 –> 00:28:39.975
I will explain it again, phone is coming to me.

396
00:28:39.975 –> 00:28:41.575
Just attend the call one minute. Okay.

397
00:28:44.855 –> 00:28:47.265
Hello? Hello, hello

398
00:29:10.155 –> 00:29:11.735
Sir, can you please recap?

399
00:29:12.195 –> 00:29:13.895
Uh, can you please recap?

400
00:29:13.995 –> 00:29:15.495
Uh, the, yeah, I’ll tell, I’ll tell.

401
00:29:15.995 –> 00:29:17.535
So here is the clip function, okay?

402
00:29:17.655 –> 00:29:19.015
I will tell you the meaning of this.

403
00:29:19.845 –> 00:29:22.975
Look at if this ratio, this ratio is basically

404
00:29:23.125 –> 00:29:25.735
between your current policy and the old policy.

405
00:29:25.885 –> 00:29:29.085
That is, this is the ratio, current policy and old policy.

406
00:29:30.115 –> 00:29:33.845
Your, your current policy might, uh, the update

407
00:29:33.845 –> 00:29:36.205
that you’re making is on the current policy, okay?

408
00:29:37.325 –> 00:29:39.785
And you one step before you had a old policy,

409
00:29:40.605 –> 00:29:42.485
so the current policy, uh,

410
00:29:42.625 –> 00:29:46.965
for which you are updating if those values are going beyond

411
00:29:47.645 –> 00:29:51.545
a particular limit, beyond your old policy,

412
00:29:52.245 –> 00:29:54.385
so the ratio is going to be quite higher,

413
00:29:54.525 –> 00:29:56.705
it is will be bigger than two three.

414
00:29:56.815 –> 00:29:59.785
Also in that case we are going to do a

415
00:30:00.425 –> 00:30:01.625
slicing from the top side.

416
00:30:02.465 –> 00:30:06.075
Okay? That means your objection is jumping too much

417
00:30:06.425 –> 00:30:09.595
that you have gone beyond your well behaved region.

418
00:30:10.375 –> 00:30:11.675
So you do a top slicing.

419
00:30:11.675 –> 00:30:14.155
So you should not go beyond one plus of upsider.

420
00:30:14.755 –> 00:30:18.925
That is this value. This value, okay? That is top slicing.

421
00:30:19.625 –> 00:30:21.525
It may happen that your new policy

422
00:30:23.145 –> 00:30:27.685
is quite lesser, quite lesser than your old policy.

423
00:30:29.525 –> 00:30:33.955
In that case, your value function at which this ratio

424
00:30:34.235 –> 00:30:38.905
function will go down farther than uh,

425
00:30:40.225 –> 00:30:44.225
I mean it’ll be between zero fraction.

426
00:30:44.765 –> 00:30:47.065
It is between zero and one. Okay?

427
00:30:47.335 –> 00:30:50.545
It’s a fraction ’cause the old policy value was quite higher

428
00:30:51.005 –> 00:30:53.225
and this current policy value is quite less

429
00:30:53.325 –> 00:30:54.505
in between zero and one.

430
00:30:54.965 –> 00:30:59.725
So in that case, what they’re telling is you should not go

431
00:30:59.855 –> 00:31:03.045
below a particular value that is one minus of fal,

432
00:31:04.115 –> 00:31:06.615
as I told Fal value is between Jira and one.

433
00:31:06.875 –> 00:31:09.175
So one minus fal will be, let’s say if you take

434
00:31:09.795 –> 00:31:12.715
Fal value two 0.2, okay?

435
00:31:12.815 –> 00:31:15.755
In that case, this clipping function tells you

436
00:31:15.755 –> 00:31:18.635
that you should not go down below 0.8.

437
00:31:19.205 –> 00:31:23.995
That ratio ratio should not go down between below that 0.8

438
00:31:24.215 –> 00:31:28.165
and the ratio should not go above 1.2 heavy.

439
00:31:28.305 –> 00:31:31.955
So it’ll be lying between 1.2 and 0.8. Okay?

440
00:31:32.745 –> 00:31:36.765
That is the objective of cliff function. Okay?

441
00:31:37.485 –> 00:31:38.785
So that is the clip function.

442
00:31:38.935 –> 00:31:41.665
Then you are applying a minimum

443
00:31:42.305 –> 00:31:44.225
operator on two values.

444
00:31:44.685 –> 00:31:46.585
One is the clipped objective value.

445
00:31:46.585 –> 00:31:48.065
This is the clipped objective value,

446
00:31:48.725 –> 00:31:50.225
and this is the original value.

447
00:31:51.385 –> 00:31:53.425
Original value is if the ratio between

448
00:31:54.325 –> 00:31:55.665
old pol current policy

449
00:31:55.685 –> 00:31:59.765
and old policy is lying between 0.8

450
00:31:59.765 –> 00:32:02.725
and 1.2, if the ratio is between 0.8

451
00:32:02.725 –> 00:32:06.485
and 1.2, in that case you take this particular value itself,

452
00:32:06.835 –> 00:32:10.885
that is ratio multiplied by advantage value, okay?

453
00:32:12.075 –> 00:32:16.495
If that is not the case, if it is below then 0.8,

454
00:32:17.005 –> 00:32:20.215
then you are going to slice it to only 0.8 in that list.

455
00:32:20.245 –> 00:32:23.795
This clip will be activated. This clip will be activated.

456
00:32:23.945 –> 00:32:27.025
Okay? Since you’re doing a minimization

457
00:32:27.025 –> 00:32:29.465
between two quantity one is this original quantity,

458
00:32:29.715 –> 00:32:31.585
other one is the clip of objective.

459
00:32:32.285 –> 00:32:34.985
In between them, whoever is minimum you are going

460
00:32:34.985 –> 00:32:36.795
to take them, okay?

461
00:32:37.065 –> 00:32:38.395
That is a minimum operator

462
00:32:38.945 –> 00:32:41.725
and on that value, you’re applying this expectation.

463
00:32:41.725 –> 00:32:44.125
Expectation means this value is going

464
00:32:44.125 –> 00:32:47.485
to getting com computed for every time state

465
00:32:49.825 –> 00:32:51.245
for a particular trajectory.

466
00:32:51.675 –> 00:32:54.885
Okay? Then for a particular trajectory you average them out.

467
00:32:55.505 –> 00:32:59.965
So again, I’m telling this proximal policy optimization.

468
00:33:00.105 –> 00:33:04.195
The name itself suggests that proximity

469
00:33:04.855 –> 00:33:07.395
to a particular policy, you know,

470
00:33:07.395 –> 00:33:10.915
what is the policy proximity proximal policy.

471
00:33:10.945 –> 00:33:13.395
This is the, this algorithm name is proximal

472
00:33:13.395 –> 00:33:14.635
policy optimization.

473
00:33:14.745 –> 00:33:17.995
Okay? Proximal means you are going near to something

474
00:33:18.575 –> 00:33:22.045
to whom you are going to near you are going to

475
00:33:23.435 –> 00:33:25.815
not too much far away from your old policy.

476
00:33:27.745 –> 00:33:29.805
Old policy is something that you have freezed

477
00:33:29.805 –> 00:33:32.245
because the policy that you have got it one step before.

478
00:33:32.905 –> 00:33:35.365
Now the next step you are updating the policy

479
00:33:35.465 –> 00:33:36.805
of the old policy itself.

480
00:33:37.105 –> 00:33:40.865
You get a new policy that is called PI theater condition

481
00:33:40.865 –> 00:33:44.425
to the fact that that a pi theta should not be deviating too

482
00:33:44.425 –> 00:33:46.705
large amount from the old policy.

483
00:33:48.195 –> 00:33:51.735
So that is the main reason behind the clip objective. Okay?

484
00:33:53.265 –> 00:33:55.115
Now there was a, uh,

485
00:33:56.455 –> 00:33:59.795
before proximal policy optimization was proposed,

486
00:34:00.165 –> 00:34:03.865
there was another algorithm which was named as

487
00:34:04.825 –> 00:34:08.745
T-R-P-O-T-R-P-O means trust region prox, uh,

488
00:34:08.745 –> 00:34:09.825
policy optimization.

489
00:34:09.895 –> 00:34:14.665
Okay? So TRPO was a um, precursor of this algorithm

490
00:34:15.605 –> 00:34:19.265
for TRPO, there was a necessity that

491
00:34:20.835 –> 00:34:23.525
this clipped objective function was not proposed.

492
00:34:23.555 –> 00:34:25.845
What there, this is going

493
00:34:25.845 –> 00:34:28.525
to be a computationally very efficient

494
00:34:28.525 –> 00:34:32.885
because the ratio is going above too much high of one.

495
00:34:32.945 –> 00:34:35.085
You clip it by one plus excelon.

496
00:34:35.085 –> 00:34:39.805
If it is going down, um, near to zero,

497
00:34:39.905 –> 00:34:43.045
in that case you from the lower side, you clip it up

498
00:34:43.595 –> 00:34:46.085
that it should not fall below one man.

499
00:34:46.355 –> 00:34:48.045
This is very L algebra cooperation.

500
00:34:48.045 –> 00:34:50.205
There is no mathematics involved in it. Okay?

501
00:34:51.165 –> 00:34:52.675
Prior to this PP algorithm,

502
00:34:52.675 –> 00:34:55.155
there was another algorithm called TPO that was full

503
00:34:55.155 –> 00:34:58.195
of mathematics that they are telling that you have

504
00:34:58.195 –> 00:35:00.795
to measure the divergence between these two quantity

505
00:35:01.545 –> 00:35:04.715
with respect to a scale divergence.

506
00:35:04.805 –> 00:35:08.035
Scale divergence means you might have already learned this

507
00:35:08.035 –> 00:35:10.835
term called, you might have heard this term scale divergence

508
00:35:10.945 –> 00:35:14.165
last semester we used, we read ah,

509
00:35:14.265 –> 00:35:17.925
so kil divergence stands for deviation

510
00:35:17.925 –> 00:35:20.205
between two probability distribution function.

511
00:35:20.865 –> 00:35:23.205
In that case you need to, the formula

512
00:35:23.385 –> 00:35:25.525
for kle divergence is quite complicated.

513
00:35:26.295 –> 00:35:28.635
If I have two functions like PFX

514
00:35:28.655 –> 00:35:31.315
and QFX, these are the two distribution function.

515
00:35:32.145 –> 00:35:35.995
Then the kale divergencies basically integration of QX

516
00:35:39.505 –> 00:35:42.505
multiplied by log of log

517
00:35:42.525 –> 00:35:44.705
of PX divided by qx.

518
00:35:45.855 –> 00:35:48.195
And this is going to be taken expectation

519
00:35:49.115 –> 00:35:50.775
for all possible values of X

520
00:35:51.395 –> 00:35:53.175
and that is a very exhaustive mechanism.

521
00:35:54.635 –> 00:35:59.575
Killens, the property of kill divergencies is the output

522
00:35:59.575 –> 00:36:01.535
of K is always positive.

523
00:36:02.935 –> 00:36:07.735
It’ll never become negative if the PX value is less

524
00:36:07.765 –> 00:36:11.945
than QX or the PX value is greater than QX for all

525
00:36:11.945 –> 00:36:14.385
of the cases because you are going to do averaging out

526
00:36:14.385 –> 00:36:17.185
for all the, you are going to expectation are going

527
00:36:17.185 –> 00:36:21.025
to do averaging out killed evidence will never go

528
00:36:21.025 –> 00:36:22.345
to less than zero,

529
00:36:22.445 –> 00:36:25.105
but it’ll be quite positive quantity in

530
00:36:25.105 –> 00:36:27.425
that case there is the jump that they’re making

531
00:36:27.485 –> 00:36:29.905
for the update is going to be quite high.

532
00:36:30.605 –> 00:36:33.825
So this, uh, people behind this TRPO, they came up

533
00:36:33.825 –> 00:36:34.825
with PP algorithm.

534
00:36:34.975 –> 00:36:37.465
They said that we are not going to do a tedious job

535
00:36:37.685 –> 00:36:39.065
for finding kle divergence.

536
00:36:39.485 –> 00:36:41.985
Let us have a algebra method of straightforward.

537
00:36:42.735 –> 00:36:46.795
Just take a scissor, clip it from the upside,

538
00:36:46.795 –> 00:36:48.435
clip it from the lower side, that’s it.

539
00:36:48.435 –> 00:36:50.955
We’re not going to do too much of mathematical operation.

540
00:36:51.945 –> 00:36:55.205
So it reduce the computational core, uh, com burden

541
00:36:56.065 –> 00:36:57.205
via lot amount.

542
00:36:57.475 –> 00:37:01.915
Okay? So that was the clipped objective, okay?

543
00:37:02.535 –> 00:37:05.875
Now this 80, it is your advantage that is going

544
00:37:05.875 –> 00:37:08.715
to be remaining same for all the previous algorithm.

545
00:37:08.825 –> 00:37:10.955
Also, like you have learned generalized advantage

546
00:37:10.955 –> 00:37:13.595
estimation, which is a general version

547
00:37:13.695 –> 00:37:15.475
of TD in learning itself.

548
00:37:16.015 –> 00:37:19.075
Before that we have learned just a normal return.

549
00:37:19.075 –> 00:37:21.235
We’re not going to do generalized t learning,

550
00:37:21.265 –> 00:37:23.875
just normal return by a one step transition,

551
00:37:24.175 –> 00:37:25.755
one step difference between the current

552
00:37:25.755 –> 00:37:26.955
state and the previous state.

553
00:37:28.495 –> 00:37:31.035
You can apply that, you will not get too much of difference.

554
00:37:31.695 –> 00:37:34.925
So the, it is not changing only what change happen is,

555
00:37:34.945 –> 00:37:39.755
is there, as I told you here, uh,

556
00:37:39.935 –> 00:37:42.955
for your generalized advantage, we have changed here.

557
00:37:43.015 –> 00:37:47.755
We have not changed this part probably of tau given the

558
00:37:48.465 –> 00:37:52.115
only in the case of proximal policy optimization, TRPO.

559
00:37:52.815 –> 00:37:54.635
So PPO and TRP,

560
00:37:54.635 –> 00:37:57.915
they have been proposed from open AI company, okay?

561
00:37:59.285 –> 00:38:03.765
The people who have invented chat GPT in 2022,

562
00:38:03.825 –> 00:38:05.965
it was proposed this PPU algorithm

563
00:38:07.145 –> 00:38:11.525
and uh, in 2024, January, first January, China came up

564
00:38:11.525 –> 00:38:13.615
with uh, deep seek.

565
00:38:14.575 –> 00:38:17.305
Deep seek is much more intelligent mechanism.

566
00:38:17.305 –> 00:38:21.015
They’re using G-R-P-O-G-R-P-O.

567
00:38:21.015 –> 00:38:24.295
Full form is group relative policy optimization.

568
00:38:26.065 –> 00:38:27.705
I will not cover GRPO here,

569
00:38:28.525 –> 00:38:31.865
but I’ll just give you a brief difference between in terms

570
00:38:31.865 –> 00:38:35.065
of English language, not in terms of any mathematics

571
00:38:35.125 –> 00:38:37.725
because GRP mathematics is much difficult than open

572
00:38:37.985 –> 00:38:39.285
AI is P algorithm.

573
00:38:40.225 –> 00:38:44.755
So look at for all of the cases what we do, we give a,

574
00:38:45.615 –> 00:38:47.995
uh, state action pair, which is called trajectory,

575
00:38:49.275 –> 00:38:51.495
and at the end of finishing trajectory,

576
00:38:51.595 –> 00:38:54.695
we provide the reward to the whole trajectory.

577
00:38:56.005 –> 00:39:00.495
Okay? Now we do while doing that

578
00:39:02.805 –> 00:39:06.745
one application of like this for large language model is

579
00:39:07.805 –> 00:39:10.695
whenever we are generating the sentence.

580
00:39:11.755 –> 00:39:15.895
So you write something in chat, GPT or uh,

581
00:39:16.435 –> 00:39:20.975
or Germany, you write a question, it’ll give you answer

582
00:39:21.005 –> 00:39:24.495
with a paragraph of the information

583
00:39:24.605 –> 00:39:26.815
that he knows about this question.

584
00:39:27.085 –> 00:39:30.125
Okay? So for this to happen,

585
00:39:31.455 –> 00:39:32.885
there are two things.

586
00:39:33.005 –> 00:39:35.325
Running in backbone in the LLM.

587
00:39:35.585 –> 00:39:37.965
One is called next word prediction.

588
00:39:38.865 –> 00:39:41.965
So whenever you are writing a word,

589
00:39:42.765 –> 00:39:46.025
LLM can predict the next word that you’re going

590
00:39:46.025 –> 00:39:47.305
to type in, okay?

591
00:39:48.305 –> 00:39:49.915
That is called next word prediction.

592
00:39:51.695 –> 00:39:53.515
So next word prediction is happening

593
00:39:53.695 –> 00:39:55.755
by using just a transformer model.

594
00:39:56.235 –> 00:39:58.875
A very, very deep architecture transformer model.

595
00:39:59.655 –> 00:40:02.155
For that you would not require any reinforcement learning,

596
00:40:03.365 –> 00:40:06.025
but whenever you are predicting a sentence,

597
00:40:06.025 –> 00:40:09.545
suppose you write a question, your LLM is going

598
00:40:09.545 –> 00:40:11.625
to answer your whole paragraph.

599
00:40:11.805 –> 00:40:15.225
So basically completing a whole sentence or set of sentence.

600
00:40:16.125 –> 00:40:20.785
So in that case to produce a sentence, this LLM,

601
00:40:20.815 –> 00:40:24.555
what they do is suppose they give you the answer,

602
00:40:24.555 –> 00:40:25.955
they will ask you whether you

603
00:40:25.955 –> 00:40:27.315
are satisfied with the answer or not.

604
00:40:27.315 –> 00:40:29.715
That is a reward actually that it is asking from.

605
00:40:30.625 –> 00:40:34.115
They will use it for further for training of their model.

606
00:40:34.625 –> 00:40:39.185
Suppose you are not satisfied with the answer, okay?

607
00:40:39.775 –> 00:40:43.535
Then what will happen then you are going to give a very,

608
00:40:43.535 –> 00:40:45.175
very less point to them.

609
00:40:45.485 –> 00:40:49.665
They will take it for learning their data set, I mean

610
00:40:49.765 –> 00:40:51.345
for preparing the data set, not learning

611
00:40:51.645 –> 00:40:53.425
for preparing the dataset that

612
00:40:54.015 –> 00:40:57.185
they will mark the whole sentence who will less reward

613
00:40:57.525 –> 00:41:00.775
to probably a negative reward also, so that

614
00:41:01.695 –> 00:41:04.655
whenever the next time a similar question asked to the LLM,

615
00:41:05.005 –> 00:41:08.895
what they’ll do is they will update their reward scheme.

616
00:41:10.395 –> 00:41:15.195
That if they are offering you that particular answer, again,

617
00:41:16.645 –> 00:41:18.855
that may not be getting a high reward,

618
00:41:20.495 –> 00:41:24.525
but they will provide it to you so that they are making a

619
00:41:25.035 –> 00:41:26.805
less rewarded output to you.

620
00:41:26.805 –> 00:41:30.205
They’re offering a less rewarded output to you, not

621
00:41:30.205 –> 00:41:32.325
that they want to satisfy you, but

622
00:41:32.325 –> 00:41:34.725
because they want to learn their model, they want

623
00:41:34.725 –> 00:41:38.165
to increase their learning of a new answer.

624
00:41:38.515 –> 00:41:42.645
Okay? So this scoring you are providing at the end

625
00:41:42.645 –> 00:41:44.965
of completion of a sentence, okay?

626
00:41:45.885 –> 00:41:48.065
So that, that is how chat GP was working

627
00:41:49.605 –> 00:41:52.905
and it got popularized from 2022 to 2024.

628
00:41:53.525 –> 00:41:57.735
But this, uh, deep seek model, what they did is they have

629
00:41:59.775 –> 00:42:03.175
grouped up the response that they’re generating

630
00:42:04.185 –> 00:42:07.155
that is called this group relative policy optimization.

631
00:42:07.155 –> 00:42:09.915
They’re not going to give you the score based on your

632
00:42:11.275 –> 00:42:13.415
one answer generated by the LLM.

633
00:42:13.765 –> 00:42:17.955
They will give you a group of

634
00:42:18.655 –> 00:42:20.355
uh, responses, okay?

635
00:42:21.155 –> 00:42:24.695
For each, uh, of the response, which is inside a group.

636
00:42:24.765 –> 00:42:28.135
They have separate, separate reward assigned to them

637
00:42:29.705 –> 00:42:33.515
and they will have a robust optimization,

638
00:42:33.745 –> 00:42:37.675
much robust than PPO that they will combine.

639
00:42:38.235 –> 00:42:40.915
I mean the formation of this structure, okay?

640
00:42:41.295 –> 00:42:44.435
You are going over inside a particular episode

641
00:42:44.535 –> 00:42:46.915
for all the word that you have predicted.

642
00:42:46.915 –> 00:42:50.115
Instead of that, they will go over all the response,

643
00:42:51.275 –> 00:42:55.005
each response having a different weight of the reward.

644
00:42:56.065 –> 00:42:59.475
Okay? So that is the main use case level,

645
00:43:02.475 –> 00:43:07.055
and that brought a huge change paradigm shift in AI domain.

646
00:43:07.475 –> 00:43:11.215
The deep model later it has given rise

647
00:43:11.275 –> 00:43:12.575
to reasoning model.

648
00:43:14.425 –> 00:43:17.405
Uh, reasoning model means, see, you are writing a question.

649
00:43:17.595 –> 00:43:22.585
Your chat gpt is giving you the answer for that, uh, chat.

650
00:43:22.745 –> 00:43:26.195
GPT is not forced to think about.

651
00:43:27.695 –> 00:43:31.795
Uh, think about beyond the relation between

652
00:43:32.625 –> 00:43:34.915
various words typed by you

653
00:43:35.255 –> 00:43:38.155
and the various words produced by the Chad g Bt not

654
00:43:38.155 –> 00:43:39.435
beyond that, okay?

655
00:43:39.815 –> 00:43:43.935
The reasoning means suppose it is

656
00:43:43.935 –> 00:43:47.925
beyond the long-term dependency reasoning means suppose

657
00:43:49.895 –> 00:43:52.995
you have given a mathematical question to be solved

658
00:43:54.025 –> 00:43:55.365
before 2024.

659
00:43:55.795 –> 00:43:57.805
This, uh, deep seek came up.

660
00:43:58.935 –> 00:44:02.785
Chad g PT and all, they were not made, they’re not designed

661
00:44:02.805 –> 00:44:04.705
to solve any mathematical question,

662
00:44:05.125 –> 00:44:07.305
but deep seek is the model which has given rise

663
00:44:07.305 –> 00:44:10.305
to solving mathematical question itself.

664
00:44:10.305 –> 00:44:11.305
Mathematical question.

665
00:44:11.305 –> 00:44:13.705
It was not like, like give it two sums

666
00:44:13.705 –> 00:44:17.025
or multiple sums, not that we can give a question of

667
00:44:17.945 –> 00:44:22.285
you give a question of finding a integration

668
00:44:22.505 –> 00:44:24.125
of a function.

669
00:44:24.905 –> 00:44:28.725
You give a integration of synex divided by coex.

670
00:44:29.935 –> 00:44:31.835
You provide a range of of hell of x

671
00:44:32.525 –> 00:44:33.635
deeps seek has done that.

672
00:44:34.215 –> 00:44:37.835
So that deeps seek model based on G two that has given rise

673
00:44:37.855 –> 00:44:39.035
to reasoning model,

674
00:44:40.605 –> 00:44:42.945
not the model came up from open ai.

675
00:44:43.415 –> 00:44:46.385
Okay? Now, if you look at, uh,

676
00:44:46.385 –> 00:44:48.505
mathematical problem can be solved by

677
00:44:50.025 –> 00:44:52.875
reinforcement learning based LLM model.

678
00:44:54.675 –> 00:44:59.095
Now your chat GT version four, it does

679
00:44:59.095 –> 00:45:01.295
that reasoning model for mathematical solving.

680
00:45:01.955 –> 00:45:06.415
Gemini. Recently Google’s this one, which he announced that,

681
00:45:07.315 –> 00:45:12.015
uh, I TJ’s mock testing can be done in Google, Gemini

682
00:45:12.995 –> 00:45:16.695
all happened because deep seek had solved it much earlier

683
00:45:16.845 –> 00:45:18.135
than Google

684
00:45:18.275 –> 00:45:22.445
and OpenAI another company from China, his name is

685
00:45:23.085 –> 00:45:27.535
Q-N-Q-W-E-N-Q-N has

686
00:45:27.535 –> 00:45:28.655
been bought by Alibaba.

687
00:45:30.555 –> 00:45:33.525
Alibaba, yeah.

688
00:45:33.665 –> 00:45:38.355
So yeah, Jack ma, Jack Ma is the owner of AWA company.

689
00:45:39.455 –> 00:45:42.875
They have bought this QN model. Okay?

690
00:45:45.175 –> 00:45:47.635
So that is a brief, uh, thing I told you about this.

691
00:45:47.655 –> 00:45:50.685
So that is what is your

692
00:45:52.635 –> 00:45:54.235
proximal policy, Optim action.

693
00:45:57.515 –> 00:45:59.295
Now time is square 45.

694
00:46:01.755 –> 00:46:05.195
I have a program for this, which I have made it

695
00:46:05.325 –> 00:46:07.595
after spending quite a lot amount of time.

696
00:46:08.745 –> 00:46:10.315
I’ll show you that thing also.

697
00:46:11.165 –> 00:46:12.295
That one one note.

698
00:46:12.635 –> 00:46:15.215
Um, so when you say that, uh, for LLM

699
00:46:15.315 –> 00:46:18.255
and the, uh, policy approximation

700
00:46:18.595 –> 00:46:22.055
or proximate, uh, how it actually does,

701
00:46:22.365 –> 00:46:26.455
because it, the LM have a huge parameters

702
00:46:26.555 –> 00:46:28.455
or huge knowledge that it has.

703
00:46:29.035 –> 00:46:32.095
Um, so how would the prediction actually happening?

704
00:46:32.295 –> 00:46:34.415
I mean, what sort of calculation it is done in

705
00:46:34.735 –> 00:46:36.095
GPU to give that prediction?

706
00:46:37.745 –> 00:46:40.115
Okay, so there are two levels of, uh,

707
00:46:40.285 –> 00:46:41.395
prediction they make.

708
00:46:41.925 –> 00:46:45.795
First is by using normal transformer model,

709
00:46:45.865 –> 00:46:49.435
they will predict your next word itself by

710
00:46:49.955 –> 00:46:51.875
learning from a huge like LAMA

711
00:46:51.875 –> 00:46:55.235
and all, if you run lamis open source model, it’ll help you

712
00:46:55.235 –> 00:46:56.875
to find out the next, uh,

713
00:46:58.135 –> 00:47:00.195
it is called predicting the next word itself.

714
00:47:00.345 –> 00:47:03.565
Okay? So that is a normal transformer training.

715
00:47:03.705 –> 00:47:07.805
Second phase is called training with reinforcement learning.

716
00:47:08.865 –> 00:47:12.565
So in that case, they’re going to produce a whole sentence,

717
00:47:12.785 –> 00:47:14.085
not the next word itself.

718
00:47:14.865 –> 00:47:19.485
So, uh, repeatedly, if you call for generating next word,

719
00:47:19.545 –> 00:47:21.885
you will get a whole sentence whether the

720
00:47:22.285 –> 00:47:23.445
sentence is good or bad.

721
00:47:24.025 –> 00:47:25.965
For that, they need a human intervention.

722
00:47:26.625 –> 00:47:29.485
So human intervention is modeled

723
00:47:29.505 –> 00:47:31.165
by something called reward modeling.

724
00:47:31.635 –> 00:47:34.205
Okay? So once this generat a sentence,

725
00:47:34.755 –> 00:47:39.505
then they will feed their sentence to a, this kind

726
00:47:39.505 –> 00:47:41.425
of reinforcement learning algorithm called

727
00:47:41.425 –> 00:47:42.545
actor critic algorithm.

728
00:47:43.645 –> 00:47:46.225
In that they have already stored a critic model.

729
00:47:46.735 –> 00:47:49.385
This critic model is already pre learned model.

730
00:47:49.845 –> 00:47:53.785
Who knows that? What is the reward should I provide

731
00:47:54.245 –> 00:47:56.745
to this particular sentence that has been generated

732
00:47:56.925 –> 00:47:59.245
by ulama open lama?

733
00:47:59.245 –> 00:48:01.965
Okay, so they will provide a reward.

734
00:48:02.235 –> 00:48:03.445
Once they get the reward.

735
00:48:03.545 –> 00:48:06.685
Now they will send that to the actor model.

736
00:48:06.745 –> 00:48:09.725
It is called actor critic. Then actor model.

737
00:48:09.755 –> 00:48:13.925
What it’ll learn, it’ll learn about probative

738
00:48:13.925 –> 00:48:18.245
of action given, is what is the state of a, state of A is

739
00:48:18.805 –> 00:48:21.605
whatever words has been seen by the actor model.

740
00:48:22.305 –> 00:48:24.095
So it is generating word

741
00:48:24.155 –> 00:48:27.095
by word generating a whole sentence word by word.

742
00:48:27.555 –> 00:48:30.695
So let’s say you are having 15 words in a sentence just

743
00:48:30.695 –> 00:48:33.055
to consider that there’s 15 words in a sentence.

744
00:48:34.065 –> 00:48:37.175
First word it has generated, okay?

745
00:48:37.995 –> 00:48:42.065
The state is set two is one second word.

746
00:48:42.065 –> 00:48:44.825
It is generated by looking at the first word.

747
00:48:45.445 –> 00:48:48.105
Now it is moving into state of S two,

748
00:48:48.845 –> 00:48:53.305
and the action is the action is basically, what is the word

749
00:48:53.455 –> 00:48:56.225
that it is predicting on the state of S one.

750
00:48:56.765 –> 00:49:00.505
It is choosing one word out of all possible word

751
00:49:00.505 –> 00:49:01.745
that it has a prediction.

752
00:49:02.255 –> 00:49:04.985
Okay? Then it has gone to next step called S two.

753
00:49:05.895 –> 00:49:09.115
How good is the S two given that it has seen only s one,

754
00:49:09.705 –> 00:49:12.435
that evaluation is going to happen by the critic model.

755
00:49:14.025 –> 00:49:16.045
So it is basically this actor critic model,

756
00:49:16.695 –> 00:49:18.285
which is a reinforcement learning model,

757
00:49:18.655 –> 00:49:20.165
which is taking into account

758
00:49:20.345 –> 00:49:24.405
or state statements, whatever word you have seen

759
00:49:24.505 –> 00:49:26.005
so far, okay?

760
00:49:26.065 –> 00:49:29.885
Action is basically what is the possible words

761
00:49:29.915 –> 00:49:33.645
that you can predict at a particular state out

762
00:49:33.645 –> 00:49:35.885
of all those possible word, you can pick anyone.

763
00:49:37.145 –> 00:49:38.475
Once you have picked up anyone,

764
00:49:39.015 –> 00:49:40.755
you are now moving into a new state

765
00:49:41.935 –> 00:49:46.255
that is st plus one state at that state, you go

766
00:49:46.255 –> 00:49:49.615
and ask your actor critic model, how good is my

767
00:49:50.395 –> 00:49:53.375
living into this particular state called st plus one?

768
00:49:53.825 –> 00:49:55.415
It’ll give you a reward for that.

769
00:49:56.115 –> 00:49:58.455
You take that reward, that will become your rt.

770
00:49:59.755 –> 00:50:01.895
At time t you have got this reward. Okay?

771
00:50:02.925 –> 00:50:07.495
Then you, once you have moved into st plus one,

772
00:50:07.875 –> 00:50:10.255
you also have a prediction at that particular state.

773
00:50:10.255 –> 00:50:13.735
Prediction means, let’s say you have your vocabulary length

774
00:50:13.755 –> 00:50:15.175
is 50,000 words in your

775
00:50:15.175 –> 00:50:16.575
vocabulary of your English dictionary.

776
00:50:17.355 –> 00:50:20.775
Out of 50,000 word, you have a probability assigned

777
00:50:20.775 –> 00:50:22.535
to each one of the 50,000 word.

778
00:50:22.955 –> 00:50:26.575
You are choosing the highest probability assigned

779
00:50:26.795 –> 00:50:28.245
by the soft max operator.

780
00:50:29.025 –> 00:50:32.475
You pick that word, which is becoming your action

781
00:50:32.975 –> 00:50:33.995
as a result of action.

782
00:50:33.995 –> 00:50:37.555
You go to next word, at every action you are evaluating

783
00:50:38.095 –> 00:50:40.355
how good is your state and action.

784
00:50:40.415 –> 00:50:44.715
So this will like that you are going to generate one right?

785
00:50:44.745 –> 00:50:49.555
Time means you have generated one sentence at the end of,

786
00:50:49.975 –> 00:50:52.315
uh, normally this reward model

787
00:50:52.785 –> 00:50:55.355
that has verification from the critic is not going

788
00:50:55.355 –> 00:50:56.595
to happen for every word.

789
00:50:56.865 –> 00:51:00.115
It’s going to happen after completion of your sentence.

790
00:51:00.115 –> 00:51:02.995
Because at every word, if you do, you are going to

791
00:51:03.825 –> 00:51:05.515
make your model to be inefficient.

792
00:51:06.255 –> 00:51:07.555
On top of that, you are going

793
00:51:07.555 –> 00:51:09.955
to make a high variance of your estimation.

794
00:51:09.985 –> 00:51:12.235
High variance, because every time,

795
00:51:12.285 –> 00:51:15.075
every time every word you generate, you get a reward.

796
00:51:16.305 –> 00:51:18.845
By that reward. If you’re updating your actor model again

797
00:51:18.845 –> 00:51:21.085
and again, you are going to make your actor model

798
00:51:21.085 –> 00:51:22.365
to be much more noisy model.

799
00:51:22.875 –> 00:51:25.485
Instead of that, you have a delayed reward.

800
00:51:26.105 –> 00:51:29.085
You hold accumulation, you don’t wait for reward. Right?

801
00:51:29.085 –> 00:51:30.765
Now, sir, generate the whole sentence.

802
00:51:31.315 –> 00:51:35.265
Then you go for a critic, say, how good is my sentence?

803
00:51:36.295 –> 00:51:38.665
Okay? You, you give you, uh, reward model.

804
00:51:38.735 –> 00:51:41.145
Then using that reward, you

805
00:51:42.485 –> 00:51:45.225
update your weight parameters, this update is going

806
00:51:45.225 –> 00:51:46.785
to happen by your back prop.

807
00:51:48.815 –> 00:51:51.625
Okay? So when you are doing the back prop,

808
00:51:51.925 –> 00:51:55.305
you are only updating your reinforcement learning model.

809
00:51:55.405 –> 00:51:58.425
You are not going to update your LAMA models,

810
00:51:59.505 –> 00:52:02.855
which has generated your next token or next one, okay?

811
00:52:02.875 –> 00:52:06.215
You don’t go and update them. So they, they are freed.

812
00:52:06.285 –> 00:52:08.695
Once the next token prediction is done

813
00:52:08.695 –> 00:52:10.015
by the lama, they’re free.

814
00:52:10.015 –> 00:52:11.335
They’re not going to be changing.

815
00:52:15.305 –> 00:52:19.195
So you understood now two phase of training for this LL

816
00:52:20.635 –> 00:52:25.525
Yeah, and also sir, uh, how can we see this actor critic?

817
00:52:25.685 –> 00:52:27.885
I understand that, um, it’s been there,

818
00:52:28.025 –> 00:52:30.445
but I’ve been trying multiple lms like

819
00:52:30.555 –> 00:52:31.885
from Microsoft five three.

820
00:52:31.965 –> 00:52:34.965
I have tried the lm I have tried,

821
00:52:35.045 –> 00:52:36.605
I have tried from Google lava models

822
00:52:36.665 –> 00:52:38.885
and all, everything gives a different results,

823
00:52:39.065 –> 00:52:41.645
and the quality of the results also varies.

824
00:52:42.225 –> 00:52:45.685
So how do I, uh, uh, know

825
00:52:45.685 –> 00:52:50.165
that which one could be needed for my project or,

826
00:52:50.265 –> 00:52:51.845
or my, my experiment?

827
00:52:52.425 –> 00:52:53.845
Uh, because everyone, every,

828
00:52:53.845 –> 00:52:56.405
every language model have its own, uh, limitations,

829
00:52:56.425 –> 00:52:59.085
its own biases, yeah, by, yes.

830
00:53:00.945 –> 00:53:02.845
So that is, uh, that is why I’m telling

831
00:53:04.715 –> 00:53:09.305
they are building the LLM model, not only to satisfy you,

832
00:53:09.645 –> 00:53:13.745
not only to satisfy you if you are not satisfied every time

833
00:53:13.745 –> 00:53:16.505
they’ll ask whether you are satisfied, yes or no.

834
00:53:17.135 –> 00:53:20.935
That value they will take into account to say

835
00:53:21.035 –> 00:53:25.645
to their reward model that whatever, uh,

836
00:53:25.825 –> 00:53:29.445
output generated by my LLM, my means I am Microsoft.

837
00:53:29.445 –> 00:53:32.445
Just considered I am LLM, you are asking me a question.

838
00:53:33.105 –> 00:53:36.085
If you’re not satisfied, I will take into account

839
00:53:36.085 –> 00:53:38.245
to my reward model that the sentence generated

840
00:53:38.265 –> 00:53:40.325
by my model is not a good enough sentence.

841
00:53:40.745 –> 00:53:43.645
So I’ll give you some kind of reward adjustment.

842
00:53:44.185 –> 00:53:46.565
So next time, similar question asked by any user,

843
00:53:47.075 –> 00:53:51.005
what I will do is I will, uh, not produce that output

844
00:53:52.115 –> 00:53:56.175
as a first two or three response given by the LLM.

845
00:53:56.845 –> 00:54:00.615
Okay? So this, this is called, there are two models

846
00:54:00.645 –> 00:54:03.455
that they’re learning on the backbone of LLM.

847
00:54:03.595 –> 00:54:05.215
One model is called reward model.

848
00:54:07.405 –> 00:54:09.545
By reward model training is completed.

849
00:54:10.545 –> 00:54:14.175
They will update the critics value function,

850
00:54:14.175 –> 00:54:17.015
critic value function means if this is the sentence,

851
00:54:17.715 –> 00:54:19.765
you give this much reward.

852
00:54:20.575 –> 00:54:23.745
Okay? So this critic is now updating

853
00:54:23.755 –> 00:54:25.745
after reward model training is done.

854
00:54:26.965 –> 00:54:28.385
Now, when a user going

855
00:54:28.645 –> 00:54:31.145
and asking him a question, he’s generating a sentence.

856
00:54:31.175 –> 00:54:33.025
This LLM model generating a sentence,

857
00:54:33.525 –> 00:54:36.265
the sentence while they are generating, they’ll generate

858
00:54:36.325 –> 00:54:37.865
by using the actor model.

859
00:54:39.335 –> 00:54:41.025
Then the actor model will send

860
00:54:41.055 –> 00:54:43.385
that whole sentence generation to the critic.

861
00:54:43.435 –> 00:54:46.825
Model critic will give a a reward for

862
00:54:46.935 –> 00:54:48.265
that particular generation.

863
00:54:49.185 –> 00:54:52.085
And if the reward is not above some threshold,

864
00:54:52.615 –> 00:54:55.605
which threshold is set by the LMS orchestrator model?

865
00:54:56.605 –> 00:54:58.765
Orchestrator model is basically the final authority.

866
00:54:59.305 –> 00:55:01.335
Who takes that?

867
00:55:01.525 –> 00:55:04.895
Whether I should be, I should be allowing this sentence

868
00:55:04.895 –> 00:55:06.455
to be displayed to the user or not.

869
00:55:06.485 –> 00:55:10.295
Okay? The orchestrator might decide that, okay, if it is

870
00:55:10.585 –> 00:55:13.135
below threshold, he will send it back to again

871
00:55:14.355 –> 00:55:17.435
critique model that okay, this has not passed the test.

872
00:55:18.945 –> 00:55:20.195
This is not positive test.

873
00:55:20.295 –> 00:55:22.315
You force the actor to generate again.

874
00:55:23.415 –> 00:55:28.135
So critic will now have an understanding that the value

875
00:55:28.205 –> 00:55:31.655
that has been predicted by me was not good enough.

876
00:55:32.685 –> 00:55:35.245
I will keep it in my memory. Then the actor is forced

877
00:55:35.245 –> 00:55:36.685
to generate one more sentence.

878
00:55:37.315 –> 00:55:39.325
That sentence again, it’ll go to critic.

879
00:55:39.325 –> 00:55:41.525
Model critic will evaluate, give you value,

880
00:55:42.275 –> 00:55:44.485
that value again, verified by orchestrator.

881
00:55:44.865 –> 00:55:47.765
If it is passing the threshold, it’ll be display to you.

882
00:55:48.585 –> 00:55:50.835
Overall, this is a continuous learning.

883
00:55:52.125 –> 00:55:54.785
It is depending on the user who is posing the question

884
00:55:55.365 –> 00:55:57.505
and what kind of knowledge the user is having,

885
00:55:58.085 –> 00:56:00.785
and the moment he is giving some response, like yes

886
00:56:00.785 –> 00:56:04.105
or no, that knowledge is helping the LMS

887
00:56:04.905 –> 00:56:06.545
critique model reward model

888
00:56:06.725 –> 00:56:08.865
to further update their knowledge base.

889
00:56:09.945 –> 00:56:10.945
I will give you one example.

890
00:56:10.945 –> 00:56:15.025
Yesterday I was writing a blog about something, uh,

891
00:56:15.025 –> 00:56:17.265
something on the, his historical background.

892
00:56:17.965 –> 00:56:19.785
Now our own country’s independent struggle.

893
00:56:21.425 –> 00:56:24.485
So there was a, there was a commission.

894
00:56:26.285 –> 00:56:29.785
You have heard about this problem of this, uh,

895
00:56:29.965 –> 00:56:31.705
recent UGC regulation, right?

896
00:56:31.915 –> 00:56:35.685
Nationwide protest, uh, launched

897
00:56:35.825 –> 00:56:38.005
by the general cast people and all.

898
00:56:39.235 –> 00:56:42.615
So what happened was this regard me

899
00:56:43.595 –> 00:56:46.095
to find out the historical background of all these

900
00:56:46.825 –> 00:56:51.085
cost-based discrimination by constitution, by government.

901
00:56:51.085 –> 00:56:54.245
Government is an instrument of the Constitution, okay?

902
00:56:54.245 –> 00:56:56.685
They have to maintain everything as per the constitution

903
00:56:57.965 –> 00:57:00.185
by just, just reading about all those things from

904
00:57:00.185 –> 00:57:01.225
historical background.

905
00:57:02.385 –> 00:57:06.945
So then I found out that these are all practice made

906
00:57:08.645 –> 00:57:10.755
legitimate in our constitution itself.

907
00:57:12.745 –> 00:57:17.005
So then I further went down to history that since when

908
00:57:17.665 –> 00:57:20.365
all our, all of our matters are written in constitution is

909
00:57:20.605 –> 00:57:24.685
actually copy pasted from British government of India Act.

910
00:57:24.995 –> 00:57:27.685
Okay? 1935, that act was written.

911
00:57:28.765 –> 00:57:29.945
All of our constitutions,

912
00:57:29.945 –> 00:57:32.065
everything is a plagiarized document.

913
00:57:32.615 –> 00:57:34.465
Just change up some word here and there.

914
00:57:35.735 –> 00:57:37.305
Overall, it is not a document written

915
00:57:37.325 –> 00:57:38.425
by Indian people at all.

916
00:57:38.895 –> 00:57:42.005
Okay? So there I was searching out,

917
00:57:42.575 –> 00:57:46.865
there was a commission called Simon Commission,

918
00:57:47.645 –> 00:57:49.035
which came into India.

919
00:57:49.945 –> 00:57:53.245
It was not applied, just proposed by British government.

920
00:57:54.905 –> 00:57:56.725
In 1928, they proposed it.

921
00:57:58.395 –> 00:57:59.655
So it was, uh,

922
00:57:59.655 –> 00:58:03.775
because of which la la large, he

923
00:58:04.285 –> 00:58:05.535
protested against it

924
00:58:06.335 –> 00:58:09.115
and in Laur he was beaten up by the police

925
00:58:10.525 –> 00:58:13.025
and within few days, seven days he died

926
00:58:13.375 –> 00:58:17.305
because of which actually Hain came up for.

927
00:58:17.405 –> 00:58:21.145
Um, that is the reason why Hain became a revolutionary.

928
00:58:22.265 –> 00:58:25.085
But then I went into the thing of Simon commission.

929
00:58:26.065 –> 00:58:29.675
Then I asked the question to Germany, JGBT,

930
00:58:30.975 –> 00:58:34.335
I think two or three L lms top notch LMI asked the question

931
00:58:34.365 –> 00:58:38.125
that which commission came to India

932
00:58:39.685 –> 00:58:40.995
after Simon commission

933
00:58:40.995 –> 00:58:43.835
because Simon commission had been withheld

934
00:58:43.835 –> 00:58:45.915
because La la Lara kind of leader

935
00:58:45.935 –> 00:58:48.075
who was the top most leader in Congress at that time,

936
00:58:48.855 –> 00:58:50.915
he died because of police beating up.

937
00:58:51.675 –> 00:58:54.135
So after that which commission came up, I asked a question,

938
00:58:54.605 –> 00:58:55.855
they could not give any answer.

939
00:58:57.425 –> 00:58:59.325
So that was a basically direct question.

940
00:58:59.465 –> 00:59:00.885
So they may not have the knowledge base.

941
00:59:01.235 –> 00:59:05.325
Then I have given some kind of, uh, hint about that matter

942
00:59:06.095 –> 00:59:08.825
that in 1928 Simon commission came,

943
00:59:09.745 –> 00:59:14.275
what was its main intention and why it was withheld.

944
00:59:15.935 –> 00:59:18.355
So actually this, all this commission that had came up

945
00:59:18.355 –> 00:59:23.125
to India, all of them had a underlying design that how to

946
00:59:24.215 –> 00:59:28.705
further divide on the basis of cast, on the basis

947
00:59:28.725 –> 00:59:30.425
of religion like that.

948
00:59:31.915 –> 00:59:33.895
So that was my intention to know

949
00:59:33.955 –> 00:59:37.295
how much knowledge they have all the elemental.

950
00:59:37.435 –> 00:59:39.215
So they don’t have any knowledge about it.

951
00:59:39.475 –> 00:59:42.175
But the moment I ask that question, I given some hint,

952
00:59:43.215 –> 00:59:45.335
I am sure that today if I ask some question,

953
00:59:45.335 –> 00:59:47.985
they’ll give me some tentative answer so

954
00:59:47.985 –> 00:59:49.865
that every day they are keep on updating

955
00:59:49.995 –> 00:59:51.385
every knowledge base.

956
00:59:54.875 –> 00:59:58.455
But if you go to Google, if you ask some commission some

957
00:59:58.485 –> 01:00:02.055
information, you will get some commissions, PDF what is

958
01:00:02.055 –> 01:00:04.815
where written in that document and you’ll get some p.

959
01:00:04.875 –> 01:00:08.375
But if you ask a question that what came after this?

960
01:00:08.445 –> 01:00:13.205
What was the solution proposed by the BTS to uh,

961
01:00:13.235 –> 01:00:16.045
alleviate the problems generated by Simon commission

962
01:00:16.065 –> 01:00:18.985
and all that, they will not be able to answer.

963
01:00:21.175 –> 01:00:25.245
So all of this that you are seeing in India proposed

964
01:00:25.245 –> 01:00:26.405
by government of India

965
01:00:26.665 –> 01:00:30.125
or some kind of legislation coming up to in terms

966
01:00:30.145 –> 01:00:32.965
of social bill, I’m not talking about any kind of

967
01:00:33.515 –> 01:00:34.885
bill related to GST

968
01:00:34.885 –> 01:00:37.925
and all in terms of controlling the society.

969
01:00:38.835 –> 01:00:40.175
All of these document,

970
01:00:41.125 –> 01:00:44.535
they have a origin into British documents.

971
01:00:48.495 –> 01:00:52.895
But anyhow, this is very, very good domain that

972
01:00:53.435 –> 01:00:54.895
how to train a LLM.

973
01:00:55.775 –> 01:00:58.935
Actually I am trying to make one LLM based on all this thing

974
01:00:59.945 –> 01:01:04.235
like this kind of historical matters,

975
01:01:04.685 –> 01:01:09.185
which, uh, which I know, which may be known

976
01:01:09.245 –> 01:01:12.425
by some more persons who are into that domain.

977
01:01:13.465 –> 01:01:16.785
Hmm. All those debates that you hear in the national TV

978
01:01:17.525 –> 01:01:20.185
and all, they’re very noisy debate, too much

979
01:01:20.185 –> 01:01:23.185
of sound they create, you cannot hear any debate

980
01:01:23.185 –> 01:01:26.465
for more than two, three minutes, especially if you go

981
01:01:26.465 –> 01:01:28.825
to any kind of prime media channels, right?

982
01:01:29.045 –> 01:01:30.785
Versus the knowledge is getting lost.

983
01:01:30.925 –> 01:01:34.705
So how can I make some kind of sequence of events

984
01:01:35.445 –> 01:01:37.465
and you can query, you will get something.

985
01:01:37.535 –> 01:01:40.265
What happened before that, what it led to like that,

986
01:01:40.775 –> 01:01:42.145
that is my motivation.

987
01:01:42.285 –> 01:01:44.265
So I have given you this example for that.

988
01:01:45.575 –> 01:01:47.595
Now it is one o’clock I thought of showing it

989
01:01:47.595 –> 01:01:51.125
to you about this PPO implementation.

990
01:01:52.715 –> 01:01:55.645
I’ll show it to on uh, your witness day class. Okay?

991
01:02:00.105 –> 01:02:02.635
Okay. So next witness day class will be there after that.

992
01:02:02.635 –> 01:02:05.075
Again, another witness day, right?

993
01:02:07.315 –> 01:02:11.455
So two class I required, I will show you one, uh,

994
01:02:11.455 –> 01:02:15.455
practical example of your LLM, uh, how

995
01:02:16.315 –> 01:02:19.365
you can get a feel of using.

996
01:02:19.465 –> 01:02:21.365
So there is a open RLHF.

997
01:02:22.165 –> 01:02:24.235
There is a, I will show you here itself.

998
01:02:25.225 –> 01:02:30.005
You go to Google, those of you working to LLM domain,

999
01:02:31.115 –> 01:02:33.135
you don’t want to do training by yourself.

1000
01:02:33.675 –> 01:02:36.935
You want to just modify the you models code

1001
01:02:36.935 –> 01:02:41.425
and all there is open RLHF.

1002
01:02:44.975 –> 01:02:46.585
Okay? Read the docs.

1003
01:02:47.535 –> 01:02:49.265
This is the second link you will get in.

1004
01:02:49.265 –> 01:02:52.105
You’ll get into this. This is the open RLHF.

1005
01:02:54.015 –> 01:02:57.845
Here. You can go

1006
01:02:57.845 –> 01:02:59.645
to this training guide.

1007
01:03:00.895 –> 01:03:03.765
Those of you on training is going to be difficult for you.

1008
01:03:03.785 –> 01:03:06.285
You can readily available model, you can run them.

1009
01:03:07.595 –> 01:03:09.095
So this is going to help you

1010
01:03:09.155 –> 01:03:11.255
to suppose you are using LAMA model

1011
01:03:11.395 –> 01:03:12.735
to generate your next token.

1012
01:03:13.685 –> 01:03:16.095
Then if you want to build one of your LLM

1013
01:03:16.095 –> 01:03:19.535
to generate the whole sentence, the sentence will be near

1014
01:03:19.595 –> 01:03:23.215
to the human level expectation sentence generation.

1015
01:03:23.235 –> 01:03:25.775
So for that you can use this o open RLH.

1016
01:03:27.455 –> 01:03:30.555
You can go to agent-based paradigm here. Execution.

1017
01:03:31.415 –> 01:03:35.655
See here, whatever I have told you, like two phases

1018
01:03:35.655 –> 01:03:38.375
of execution is documented over here.

1019
01:03:43.755 –> 01:03:45.665
After you do this two phase,

1020
01:03:45.665 –> 01:03:48.945
like the last phase R AL algorithm, that is where you have

1021
01:03:48.945 –> 01:03:53.705
to use either PPO either reinforce G-R-P-O-G-R PS made

1022
01:03:53.705 –> 01:03:56.615
by DIPE and all, okay?

1023
01:03:58.425 –> 01:04:00.475
This is basically called consistency

1024
01:04:00.475 –> 01:04:01.635
in your token trajectory.

1025
01:04:05.675 –> 01:04:07.675
I have not used open RLHF so far.

1026
01:04:07.675 –> 01:04:09.435
What I am trying to do is I’m trying

1027
01:04:09.675 –> 01:04:14.615
to make the Python programs by myself so that I don’t need

1028
01:04:14.635 –> 01:04:18.015
to wait for, because there are a lot of neur networks.

1029
01:04:18.085 –> 01:04:22.215
Wait, too much of weight in my laptop. I cannot run it.

1030
01:04:22.415 –> 01:04:25.255
I require at least 20 GB GPU.

1031
01:04:27.245 –> 01:04:29.985
So I’m running very small model to have a very

1032
01:04:30.585 –> 01:04:33.025
specific knowledge base, information like this,

1033
01:04:33.025 –> 01:04:34.345
historical backgrounds and all.

1034
01:04:38.815 –> 01:04:40.875
You can go into this and you can explore,

1035
01:04:43.555 –> 01:04:47.175
you can download the pre pretend model of RLHF, attach it

1036
01:04:47.175 –> 01:04:48.935
to your LAMA model.

1037
01:04:50.075 –> 01:04:52.525
Then you can generate tokens from lama, send it

1038
01:04:52.525 –> 01:04:55.455
to the RL open RLHF, okay?

1039
01:04:55.985 –> 01:04:57.935
It’ll generate the sentence.

1040
01:04:58.155 –> 01:05:00.655
You can give a score from your side.

1041
01:05:01.705 –> 01:05:03.145
I have not done that experiment.

1042
01:05:03.485 –> 01:05:05.265
I’m trying to build up my own.

1043
01:05:06.135 –> 01:05:08.665
This, uh, very, very small model of this, uh,

1044
01:05:09.135 –> 01:05:12.875
lean force algorithm alone to make a sentence.

1045
01:05:16.535 –> 01:05:18.765
Those were who are working in industrial scale

1046
01:05:18.765 –> 01:05:19.965
operation, okay?

1047
01:05:20.465 –> 01:05:24.805
Uh, you don’t have the freedom to add these open RLHF.

1048
01:05:24.915 –> 01:05:27.445
Also, you write a question, they will give you the answer.

1049
01:05:29.055 –> 01:05:32.035
But those answers, suppose you give those answers some

1050
01:05:32.275 –> 01:05:34.755
question pair to your open RLHF.

1051
01:05:34.815 –> 01:05:37.675
You’ll be able to get some kind of model

1052
01:05:38.095 –> 01:05:39.915
of your reward model of actor

1053
01:05:39.975 –> 01:05:41.635
and critic model weight parameters.

1054
01:05:41.655 –> 01:05:43.915
But those weight parameters, you cannot attach it

1055
01:05:43.915 –> 01:05:46.595
to your models that you’re using, like Gemini

1056
01:05:46.595 –> 01:05:49.155
or Open Eyes things and all.

1057
01:05:49.155 –> 01:05:50.875
Because they are not open model.

1058
01:05:50.935 –> 01:05:52.515
You cannot interact with them.

1059
01:05:53.525 –> 01:05:55.785
You can interact with only the end product of them,

1060
01:05:55.925 –> 01:05:59.195
but you, you’ll not be able to feed back to their model.

1061
01:06:07.305 –> 01:06:10.615
So we can quit today’s class then.

1062
01:06:12.475 –> 01:06:15.335
Yes, sir. Thank you. Yes, sir. Okay. Yeah.

1063
01:06:15.425 –> 01:06:17.295
Thank you, sir. Thank you, sir.

1064
01:06:17.905 –> 01:06:22.755
Thank you, sir. Okay, thank you, sir. Yeah, you can quit.

1065
01:06:24.645 –> 01:06:26.865
Yeah. Thank you, sir. Mm, thank you.

[vimeo - [https://vimeo.com/1162772382?fl=pl&fe=vl](https://vimeo.com/1162772382?fl=pl&fe=vl)](https://www.notion.so/vimeo-https-vimeo-com-1162772382-fl-pl-fe-vl-30161edeb7398076a43ec62ae6903e7f?pvs=21)

## 📚 Lecture Summary: Proximal Policy Optimization (PPO)

**Topic:** Policy Gradient Methods - Final class topic of the Reinforcement Learning course
**Date Context:** This is the last core RL topic before covering RL applications in Large Language Models

---

### 🎯 Key Concepts Covered

---

**### 📝 Policy Gradient Foundations**

**ximal Policy Optimization (PPO)** - Main algorithm of the lecture---

**Objective Function:**
The goal is to maximize the expected reward along a trajectory τ:

1. **REINFORCE (Vanilla Policy Gradient)** - Uses total trajectory reward R(τ)

---

### 🎯 Proximal Policy Optimization (PPO) - Core Algorithm

**Key Innovation:** PPO modifies the probability ratio itself (not just the reward term)

**✨ The Clipped Objective Function (PPO's Key Contribution):**

---

### 🔄 PPO vs TRPO (Trust Region Policy Optimization)

**TRPO (Predecessor to PPO):**

- Used KL-divergence to measure deviation between policies
- KL-divergence formula: D_KL(P||Q) = ∑ Q(x) log(P(x)/Q(x))
- Mathematically rigorous but computationally expensive
- Required complex constraint optimization

**PPO Improvements:**

- Simple clipping operation (algebraic, not mathematical)
- No need to compute KL-divergence
- Much more computationally efficient
- Easier to implement and tune
- Same effectiveness as TRPO with better performance

**Why "Proximal"?**

- Proximal = staying close/near to something
- PPO keeps new policy close to old policy
- Prevents destructive large updates
- Name reflects the core principle of the algorithm

---

### 🤖 Applications in Large Language Models (LLMs)

**Two-Phase Training Process:**

**Real-World LLM Implementations:**

**OpenAI (ChatGPT) - Using PPO:**

- ChatGPT-3.5 and GPT-4 use PPO for RLHF
- Proposed in 2022, popularized AI globally
- PPO chosen for computational efficiency

**DeepSeek (China) - Using GRPO:**

- Released January 2024
- GRPO = Group Relative Policy Optimization
- Key innovation: Groups multiple responses together
- Each response in group has different reward weight
- More robust optimization than PPO
- Enabled reasoning capabilities (mathematical problem solving)

**QWen (Alibaba/Jack Ma):**

- Alternative Chinese LLM using advanced RL
- Competing with DeepSeek

**Google Gemini:**

- Recently announced reasoning model capabilities
- Can solve IIT-JEE mock tests
- Following DeepSeek's lead in reasoning models

**Key Takeaway:**
DeepSeek's GRPO demonstrated that RL techniques beyond PPO can unlock new capabilities like mathematical reasoning, pushing other companies (OpenAI, Google) to develop similar features.

---

### 💡 Key Takeaways

1. **PPO is the cornerstone of modern LLMs** - Used by ChatGPT and revolutionized AI interaction
2. **Simple yet powerful** - Replaces complex KL-divergence with simple clipping
3. **Stability through proximity** - Keeps policy updates controlled and safe
4. **Actor-Critic architecture essential** - Separates action selection from value estimation
5. **RLHF is the secret sauce** - Human feedback makes LLMs helpful and aligned
6. **Evolution continues** - GRPO and beyond are pushing RL capabilities further
7. **Delayed rewards work** - Sentence-level rewards more effective than word-level

---

### 📚 Resources & Tools

**Mentioned in Lecture:**

- HuggingFace PPO documentation
- OpenRLHF - Open-source RL framework for LLM training
- Spinning Up in Deep RL (OpenAI educational resource)

**Key Papers:**

- Original PPO paper (OpenAI, 2017)
- TRPO paper (predecessor)
- RLHF papers from OpenAI and DeepMind

---

### 🔗 Video Link

[RL LS 23 - Full Lecture Recording](https://vimeo.com/1162772382)

---

## 📝 Full Transcript (WEBVTT Format)

**Phase 1: Next-Word Prediction (Transformer Model)**

- Base model: LLAMA, GPT, etc.
- Trained on massive text corpora
- Learns patterns and relationships between words
- Predicts the most likely next word given context
- No reinforcement learning at this stage

**Phase 2: Sentence Generation (RLHF - RL from Human Feedback)**

- Uses PPO or similar RL algorithms
- Components:
    - **Actor**: Generates complete sentences/responses
    - **Critic (Reward Model)**: Evaluates quality of generated text
    - **Human Feedback**: Used to train the reward model

**How RL Maps to Text Generation:**

- **State (Sₜ)**: Words seen so far in the sentence
- **Action (Aₜ)**: Next word to generate (chosen from vocabulary)
- **Policy (π)**: Neural network that selects words
- **Reward (R)**: Human satisfaction score for complete response
- **Episode**: Complete sentence or paragraph generation

**Key Challenge - Delayed Reward:**

- Reward only given after complete sentence
- Not after each word (would create high variance)
- Actor-critic helps estimate intermediate values

The gradient of the surrogate function can be noisy because the policy isn't an explicit function of θ.

**Solution: Clip the ratio to constrain policy updates**

```
Lᴖᴸᴵᴾ(θ) = E[min(
  ratio × Aₜ,
  clip(ratio, 1-ε, 1+ε) × Aₜ
)]
```

**Clipping Rules:**

- ε (epsilon) is typically 0.2
- Upper bound: ratio cannot exceed 1 + ε = 1.2
- Lower bound: ratio cannot go below 1 - ε = 0.8
- Prevents large policy updates that could destabilize training

**How It Works:**

1. If ratio is between [0.8, 1.2] → Use original value
2. If ratio > 1.2 → Clip to 1.2 (policy diverging too much upward)
3. If ratio < 0.8 → Clip to 0.8 (policy diverging too much downward)
4. Take minimum between clipped and unclipped objectives

**The Problem PPO Solves:**
When optimizing P(τ|θ) × R(τ) directly:

- Cannot update network weights θ and optimize simultaneously
- Need to decouple the optimization process

**PPO Solution - Importance Sampling:**
Use ratio between current and previous policy:

```
ratio = πθ(ᵃₜ|𝑠ₜ) / πθ_ₒₗ₄(ᵃₜ|𝑠ₜ)
```

- πθ = Current policy (being updated)
- πθ_ₒₗ₄ = Old policy (frozen from previous step)

**Surrogate Objective Function:**

```
L(θ) = E[ratio × Aₜ]
       = E[(πθ(aₜ|sₜ) / πθ_old(aₜ|sₜ)) × Aₜ]
```

1. **Actor-Critic** - Uses advantage Aₜ = Rₜ - V(sₜ) with separate actor and critic networks
2. **Generalized Advantage Estimation (GAE)** - Applies TD(λ) learning to advantage estimation

```
J(θ) = E[Σ P(τ|θ) × R(τ)]
```

**Policy Gradient Theorem:**

```
∇J(θ) = E[Σₜ ∇log π(aₜ|sₜ, θ) × Aₜ]
```

Where:

- π(aₜ|sₜ, θ) = Probability of action given state
- Aₜ = Advantage function
- θ = Neural network parameters

**Three Approaches Learned:**

1. **Policy Gradient Review** - Recap of vanilla policy gradient, actor-critic, and advantage estimation
2. **Importance Sampling** - Statistical method for comparing policies
3. **Surrogate Loss Function** - Using ratio between current and old policies
4. **Clipped Objective Function** - PPO's key innovation for stable training
5. **Comparison with TRPO** - Trust Region Policy Optimization as PPO's predecessor
6. **Applications in LLMs** - How PPO is used in ChatGPT and other language models