# RL01

WEBVTT

1
00:09:40.675 –> 00:09:43.485
Hello. Good evening.

2
00:09:43.675 –> 00:09:46.165
Good evening, sir. Good evening.

3
00:09:46.595 –> 00:09:49.245
Yeah, good evening sir. Good evening.

4
00:10:08.935 –> 00:10:10.155
So how are the things going on?

5
00:10:16.055 –> 00:10:20.225
All good, sir. Okay,

6
00:10:35.285 –> 00:10:37.905
sir, had you checked that, uh, quiz question, sir?

7
00:10:39.715 –> 00:10:41.665
There was a mistake from our side.

8
00:10:42.765 –> 00:10:45.145
So they’ll take care of giving the marks.

9
00:10:45.485 –> 00:10:46.865
Uh, our will take care.

10
00:11:01.155 –> 00:11:02.565
Okay, so how many has joined?

11
00:11:08.105 –> 00:11:09.325
23 sir,

12
00:11:15.225 –> 00:11:16.715
This score says around how many people?

13
00:11:18.145 –> 00:11:19.145
60. Nearly.

14
00:11:20.155 –> 00:11:21.895
Uh, above 80, sir.

15
00:11:22.745 –> 00:11:27.625
Above 80. The time

16
00:11:27.685 –> 00:11:29.665
of 6 35.

17
00:11:30.285 –> 00:11:34.685
Let me start. So today I’m having something called

18
00:11:35.785 –> 00:11:37.725
the first algorithm of reinforcement learning

19
00:11:37.725 –> 00:11:38.765
in the policy gradient.

20
00:11:39.085 –> 00:11:42.565
I will show you by the help of a Python notebook.

21
00:11:43.285 –> 00:11:45.495
This algorithm is called reinforce algorithm.

22
00:11:46.665 –> 00:11:48.315
This is the first algorithm of

23
00:11:50.675 –> 00:11:51.775
policy gradient.

24
00:11:52.435 –> 00:11:55.315
So, so let me share the screen.

25
00:12:12.575 –> 00:12:15.975
So this is the tutorial that I’m going

26
00:12:15.975 –> 00:12:18.855
to follow today from hugging face.

27
00:12:20.095 –> 00:12:24.255
I will share you this link in, uh, so

28
00:12:24.805 –> 00:12:27.775
just type hugging face tutorial for reinforcement learning.

29
00:12:27.835 –> 00:12:28.975
You will get this page.

30
00:12:29.725 –> 00:12:31.975
This is the main page, introduction

31
00:12:31.975 –> 00:12:33.175
to Deep reinforcement learning.

32
00:12:33.485 –> 00:12:37.235
Okay, so like, uh,

33
00:12:37.475 –> 00:12:38.875
Q learning, deep Q learning.

34
00:12:38.875 –> 00:12:40.635
Everything is available here.

35
00:12:46.315 –> 00:12:49.135
So I’ll go over policy gradient using PyTorch.

36
00:12:54.245 –> 00:12:58.515
This is a document that I’m going to refer to them.

37
00:12:59.335 –> 00:13:02.115
All of this has already been explained to you

38
00:13:02.175 –> 00:13:03.875
for last two to three classes.

39
00:13:09.835 –> 00:13:11.855
So this one is called Reinforce learning,

40
00:13:12.495 –> 00:13:13.695
reinforce Algorithm.

41
00:13:13.715 –> 00:13:15.535
The algorithm name is Reinforce algorithm.

42
00:13:16.085 –> 00:13:19.215
This is the fast, uh, policy gradient based algorithm.

43
00:13:20.445 –> 00:13:23.255
This is also called monteon Inforce Algorithm.

44
00:13:26.425 –> 00:13:29.565
Okay, so let go

45
00:13:31.695 –> 00:13:36.355
just to have some kind of brief touch up

46
00:13:36.355 –> 00:13:37.395
with the previous topics.

47
00:13:40.025 –> 00:13:43.565
So in case of, um, policy gradient based method,

48
00:13:44.355 –> 00:13:48.325
this is a very important thing for all of you to remember.

49
00:13:49.295 –> 00:13:53.235
What is the difference between, uh, like Q learning

50
00:13:53.735 –> 00:13:55.635
and policy gradient based method?

51
00:13:57.025 –> 00:14:00.045
The difference is basically Q learning comes under value

52
00:14:00.045 –> 00:14:01.405
based approximation.

53
00:14:02.865 –> 00:14:04.365
Uh, why? It is called value based.

54
00:14:04.445 –> 00:14:08.605
Because we are using a target

55
00:14:09.775 –> 00:14:12.205
which is already coming from another neural network

56
00:14:12.505 –> 00:14:14.525
to train New York policy network.

57
00:14:15.225 –> 00:14:17.165
We have seen it is using two neural network.

58
00:14:18.345 –> 00:14:19.845
So the target network is

59
00:14:20.505 –> 00:14:23.395
getting the value from a value based algorithm.

60
00:14:23.615 –> 00:14:25.395
Either it can be TD learning

61
00:14:25.815 –> 00:14:28.955
or it can be TD Lambda learning like that.

62
00:14:30.865 –> 00:14:32.155
Okay? And

63
00:14:32.155 –> 00:14:34.755
after that you have a policy network that you are training

64
00:14:36.365 –> 00:14:38.505
by the help of this target value coming from

65
00:14:38.505 –> 00:14:39.545
the value based learning.

66
00:14:40.245 –> 00:14:44.295
On the other hand, policy-based, the based

67
00:14:44.435 –> 00:14:47.095
or can say policy gradient mechanism.

68
00:14:47.315 –> 00:14:50.545
In this case, we directly try to

69
00:14:51.165 –> 00:14:52.525
optimize the policy itself.

70
00:14:52.625 –> 00:14:55.045
We don’t try to take help of the value function.

71
00:14:56.595 –> 00:14:59.655
So here you can see that this is the difference.

72
00:15:04.985 –> 00:15:08.525
Our objective here is to maximize the expected, some

73
00:15:09.365 –> 00:15:13.325
expected sum of discounted reward, okay

74
00:15:13.545 –> 00:15:14.845
for IT policy.

75
00:15:15.025 –> 00:15:19.545
And then we update the policy every time Parameterized,

76
00:15:20.005 –> 00:15:24.745
uh, this is also called parameterized stochastic policy

77
00:15:24.745 –> 00:15:27.825
because we are going to formulate the policy

78
00:15:27.965 –> 00:15:29.305
by the help of a neural network.

79
00:15:29.525 –> 00:15:32.945
So we have a neural network with parameters by which we can,

80
00:15:34.135 –> 00:15:35.515
uh, formulate the policy.

81
00:15:37.115 –> 00:15:41.475
Okay? From next class onward,

82
00:15:41.555 –> 00:15:43.755
I will get into something called actor critic method.

83
00:15:44.025 –> 00:15:47.355
Okay? Actor critic method is basically, it is a mix

84
00:15:47.375 –> 00:15:52.155
of this policy gradient and value-based technique.

85
00:15:52.695 –> 00:15:56.815
So in case of actor critic network, we are going to use,

86
00:15:59.245 –> 00:16:02.865
we are going to use a policy network trained

87
00:16:02.885 –> 00:16:03.905
by policy gradient,

88
00:16:04.085 –> 00:16:07.545
and we have a value based network which is used

89
00:16:07.605 –> 00:16:10.545
for a critic, a person who is a critic

90
00:16:10.645 –> 00:16:12.905
of evaluating the action of the actor.

91
00:16:14.265 –> 00:16:18.755
Okay? So here is clearly deference between policy based

92
00:16:18.815 –> 00:16:21.695
and policy gradient.

93
00:16:26.225 –> 00:16:28.885
Before that there is difference between value-based

94
00:16:28.885 –> 00:16:30.525
and policy based, which is, you know, here.

95
00:16:32.935 –> 00:16:37.075
So you look at in case of value-based network,

96
00:16:37.175 –> 00:16:39.995
we have a objective that is minimizing the loss

97
00:16:39.995 –> 00:16:41.995
between predicted and target value.

98
00:16:43.805 –> 00:16:48.095
Okay? In case of policy gradient network, we don’t do that.

99
00:16:56.505 –> 00:17:00.825
So policy gradient method, it

100
00:17:01.445 –> 00:17:02.665
has objective function.

101
00:17:04.625 –> 00:17:08.005
That is, yeah.

102
00:17:10.685 –> 00:17:13.865
So for a particular example, I’ll get into it later.

103
00:17:14.205 –> 00:17:16.385
So what is a basically policy?

104
00:17:16.485 –> 00:17:18.905
Policy means probability of taking any action.

105
00:17:19.095 –> 00:17:23.145
What a particular state is, which are we are going

106
00:17:23.145 –> 00:17:24.545
to take the help of neural network.

107
00:17:24.805 –> 00:17:28.505
So this action that is chosen at a state

108
00:17:28.505 –> 00:17:31.745
of ace is also depending on a network

109
00:17:31.775 –> 00:17:33.105
with parameter called theta.

110
00:17:34.325 –> 00:17:37.615
Okay? So that is what is called pi theta is.

111
00:17:39.415 –> 00:17:41.075
So what is the overall objective

112
00:17:41.075 –> 00:17:43.855
of policy in gradient policy In gradient?

113
00:17:43.955 –> 00:17:48.445
The objective is you have to find out

114
00:17:49.915 –> 00:17:54.565
the action which action is helping to increase your

115
00:17:55.365 –> 00:17:58.145
expected sum of discounted reward, okay?

116
00:17:58.165 –> 00:18:01.265
In other way, whatever return you are getting from the last

117
00:18:01.355 –> 00:18:04.985
state, that is the final state, the actions

118
00:18:05.015 –> 00:18:06.505
that you have taken sequential

119
00:18:06.885 –> 00:18:09.465
for all the previous states, okay?

120
00:18:09.675 –> 00:18:14.305
Which action has increased your expected reward, that is

121
00:18:14.305 –> 00:18:15.825
what we are going to increase the

122
00:18:16.095 –> 00:18:17.465
probability for that action.

123
00:18:17.465 –> 00:18:20.145
That is the main objective, which is retain award here.

124
00:18:21.185 –> 00:18:22.725
So the training loop is basically

125
00:18:23.585 –> 00:18:25.365
it is you collect an episode.

126
00:18:26.505 –> 00:18:29.315
For a policy episode means

127
00:18:30.015 –> 00:18:34.335
you have a state action reward, state action reward.

128
00:18:34.345 –> 00:18:38.135
It’ll go on until you complete the last, uh, I mean

129
00:18:38.135 –> 00:18:39.575
until you reach the final state.

130
00:18:39.875 –> 00:18:41.975
So for every state, you may have a reward,

131
00:18:41.975 –> 00:18:43.855
you may not have a reward, but the final state is

132
00:18:43.855 –> 00:18:45.015
going to have some reward.

133
00:18:45.685 –> 00:18:49.415
That reward may be a positive, it can be negative, okay?

134
00:18:50.085 –> 00:18:54.095
So you collect the reward for a given policy, okay?

135
00:18:54.205 –> 00:18:57.455
Then what you do, whatever action you have taken

136
00:18:57.675 –> 00:19:00.015
for reaching the final state, okay?

137
00:19:00.435 –> 00:19:02.655
If the reward is positive, you increase the

138
00:19:03.245 –> 00:19:04.375
actions probability.

139
00:19:05.245 –> 00:19:07.305
If the reward is a negative reward,

140
00:19:07.695 –> 00:19:10.545
then you decrease the probability of the action

141
00:19:10.545 –> 00:19:11.865
that you have taken for every state.

142
00:19:11.885 –> 00:19:14.905
So that is the main, I mean brief in brief,

143
00:19:14.905 –> 00:19:16.385
this is the job they’re going to do

144
00:19:16.405 –> 00:19:20.035
for every policy gradient mechanism, okay?

145
00:19:20.695 –> 00:19:22.795
In our uh, course, we are going

146
00:19:22.795 –> 00:19:24.395
to have only two remaining thing.

147
00:19:24.435 –> 00:19:27.515
One is called actor gradient, uh, actor critic method.

148
00:19:28.205 –> 00:19:31.115
Today I’m teaching this reinforce is a very basic algorithm.

149
00:19:31.185 –> 00:19:33.835
Then I will be teaching actor critic method.

150
00:19:33.905 –> 00:19:37.035
Then the final one is called proximal policy optimization.

151
00:19:37.505 –> 00:19:38.595
Then the course will end.

152
00:19:43.715 –> 00:19:46.765
Okay? So all of this are al already been given

153
00:19:47.025 –> 00:19:48.245
to you for many times.

154
00:19:49.825 –> 00:19:51.475
This is the objective function.

155
00:19:51.585 –> 00:19:53.115
Objective function is basically

156
00:19:53.765 –> 00:19:55.995
every time the objective function, do not change.

157
00:19:56.175 –> 00:19:59.995
It is you have to increase the expected sum

158
00:19:59.995 –> 00:20:01.115
of discounted reward

159
00:20:01.135 –> 00:20:02.675
for all the episode that you are having.

160
00:20:04.485 –> 00:20:08.985
As I told you, have a one episode like this, draw that,

161
00:20:10.405 –> 00:20:12.545
uh, let me show you whatever

162
00:20:14.365 –> 00:20:16.425
policy gradient plus note.

163
00:20:25.145 –> 00:20:26.835
Yeah. So this is what is an episode.

164
00:20:31.765 –> 00:20:33.015
Episode looks like this.

165
00:20:37.975 –> 00:20:42.635
So this episode stands for from the timestamp zero

166
00:20:42.695 –> 00:20:43.995
to timestamp, capital T,

167
00:20:44.315 –> 00:20:45.595
whatever states you have gone through.

168
00:20:45.945 –> 00:20:49.245
Okay? So it’s not is a starting state a not

169
00:20:49.245 –> 00:20:50.365
easier starting action.

170
00:20:51.325 –> 00:20:55.045
S one is the state that you’re reaching at times.

171
00:20:55.045 –> 00:20:57.605
Step one, A one is at times step one,

172
00:20:57.675 –> 00:20:58.965
what action you have taken.

173
00:20:59.705 –> 00:21:03.525
Now this a no, A one A to all of this can be one

174
00:21:03.525 –> 00:21:06.365
of these four element left, right, up and down.

175
00:21:06.835 –> 00:21:11.045
Okay? So if this is the policy, let’s say this is my policy.

176
00:21:11.225 –> 00:21:13.045
Now under this policy,

177
00:21:14.735 –> 00:21:18.675
if I roll out one episode, one episode can be like this.

178
00:21:18.835 –> 00:21:21.595
I am currently starting the game from s not I have taken

179
00:21:23.155 –> 00:21:25.725
a left action, okay?

180
00:21:27.255 –> 00:21:30.665
Then I may be reaching, if I take a left action,

181
00:21:30.865 –> 00:21:33.145
I may be reaching either same state

182
00:21:34.005 –> 00:21:38.465
or the downstate, okay?

183
00:21:38.835 –> 00:21:39.945
Under the left action.

184
00:21:41.005 –> 00:21:42.745
So if the ANOT is a left action,

185
00:21:42.885 –> 00:21:45.825
my next state can be either zero or three.

186
00:21:47.385 –> 00:21:49.125
On the other hand, if I consider another thing,

187
00:21:49.315 –> 00:21:52.685
like I have taken the right hand side action on the state

188
00:21:52.685 –> 00:21:57.405
of zero, I may be reaching the states called one here,

189
00:21:57.505 –> 00:22:00.685
or I may be coming back to zero, or I may be going to three.

190
00:22:01.065 –> 00:22:03.765
So next it can be zero, one, or three. Okay?

191
00:22:04.845 –> 00:22:07.185
So I’m not changing the policy, that means

192
00:22:08.485 –> 00:22:10.225
I’m not changing the direction of action here.

193
00:22:10.775 –> 00:22:12.385
This direction of action, I’m not changing.

194
00:22:13.805 –> 00:22:16.025
So as a result of, uh, I mean

195
00:22:16.095 –> 00:22:19.025
because the slippery environment, you may be landing up

196
00:22:19.025 –> 00:22:23.465
to any other next state at a one itself, okay?

197
00:22:24.125 –> 00:22:27.905
So if you choose your next date is zero, then your

198
00:22:28.455 –> 00:22:32.185
tral is going to be zero, left again, zero.

199
00:22:33.275 –> 00:22:35.615
So this is going to be one episode like

200
00:22:35.615 –> 00:22:37.095
that you can roll out one episode.

201
00:22:37.475 –> 00:22:39.695
You don’t have to change the direction of your policy,

202
00:22:39.695 –> 00:22:40.855
direction of your action.

203
00:22:41.395 –> 00:22:44.415
You can get multiple values of the next state itself.

204
00:22:45.125 –> 00:22:48.695
That is going to give you a new episode all the time.

205
00:22:48.695 –> 00:22:53.305
That is how I like this tau one can be like this tau two

206
00:22:53.975 –> 00:22:58.105
here instead of zero, you are taking one state tau three,

207
00:22:58.805 –> 00:23:01.985
you can take the state number three as

208
00:23:02.765 –> 00:23:04.145
the replacement of S one.

209
00:23:04.145 –> 00:23:05.545
So here had come three coming up.

210
00:23:05.545 –> 00:23:08.745
So every time you are getting new states for all

211
00:23:08.745 –> 00:23:11.805
of those episode under the particular policy,

212
00:23:13.055 –> 00:23:15.675
you find out the reward, okay?

213
00:23:15.745 –> 00:23:18.635
Then sum up the reward by averaging for every episode.

214
00:23:19.215 –> 00:23:23.125
So that is your expected sum of discounted reward.

215
00:23:24.835 –> 00:23:27.245
Now, once you were expected sum

216
00:23:27.245 –> 00:23:30.985
of discounted reward is finalized, then you can,

217
00:23:31.045 –> 00:23:33.385
you are allowed to change the direction of this action

218
00:23:33.405 –> 00:23:34.425
to get a new policy

219
00:23:36.315 –> 00:23:40.135
after that which action,

220
00:23:41.115 –> 00:23:43.895
uh, I mean which action a particular state is

221
00:23:44.555 –> 00:23:45.565
to be increased.

222
00:23:45.635 –> 00:23:50.305
That is what is the objective here before doing that.

223
00:23:52.045 –> 00:23:56.445
Hmm, sir, in that diagram you had a, a zero, A,

224
00:23:56.705 –> 00:23:59.685
you had a zero, a, a two, A one,

225
00:23:59.765 –> 00:24:01.725
A two in the style one if you see.

226
00:24:02.225 –> 00:24:06.805
Mm-hmm. So which means that we have changed the action also.

227
00:24:07.625 –> 00:24:10.125
No, no, no. This a zero remains the

228
00:24:10.125 –> 00:24:11.405
same at the state of zero.

229
00:24:11.545 –> 00:24:13.245
It is right side. Mm-hmm.

230
00:24:14.285 –> 00:24:17.105
That’s why I have not, I’ve just kept it a zero here,

231
00:24:17.495 –> 00:24:18.505
here also a zero.

232
00:24:18.925 –> 00:24:21.465
Oh, this, A zero is already defined under this

233
00:24:21.465 –> 00:24:22.625
policy. It is right hand side,

234
00:24:23.355 –> 00:24:24.355
Right?

235
00:24:24.835 –> 00:24:28.215
And okay, so basically the what when you said

236
00:24:28.215 –> 00:24:30.415
that we are not changing the actions for this policy,

237
00:24:30.885 –> 00:24:32.735
then we are taking those, those arrows

238
00:24:32.735 –> 00:24:33.935
marked in that diagram.

239
00:24:34.025 –> 00:24:36.335
Those are the actions and we are sticking to that action.

240
00:24:37.275 –> 00:24:39.365
Yeah, you are sticking to, according

241
00:24:39.365 –> 00:24:40.445
to this error policy, that

242
00:24:40.625 –> 00:24:41.625
Policy, okay?

243
00:24:41.715 –> 00:24:43.765
However, your next step can be anything

244
00:24:43.765 –> 00:24:45.165
because of slippery environment,

245
00:24:45.615 –> 00:24:46.615
Right? Okay.

246
00:24:46.615 –> 00:24:49.205
So when you roll out one episode,

247
00:24:49.865 –> 00:24:51.525
you may be getting this episode,

248
00:24:52.735 –> 00:24:54.195
you roll out another episode,

249
00:24:54.215 –> 00:24:55.795
you’ll be getting this episode.

250
00:24:55.935 –> 00:24:59.515
Mm-hmm. Okay? Every episode is having equal probability.

251
00:24:59.895 –> 00:25:03.325
No one is having any partiality, okay?

252
00:25:03.585 –> 00:25:05.285
So for every episode you’re going

253
00:25:05.285 –> 00:25:06.685
to get some kind of return.

254
00:25:07.785 –> 00:25:11.485
So that return, suppose you have rolled out,

255
00:25:11.485 –> 00:25:14.395
rolled out this episode at the end,

256
00:25:15.255 –> 00:25:17.595
you may not be going to the goal state.

257
00:25:18.485 –> 00:25:20.945
In that case, your return is zero under this

258
00:25:21.105 –> 00:25:22.305
particular environment, okay?

259
00:25:23.005 –> 00:25:25.665
So in that case, all of these, uh, action

260
00:25:25.665 –> 00:25:28.465
that you have taken, you are not going to

261
00:25:29.625 –> 00:25:32.305
increase the probability, not decrease the probability

262
00:25:32.405 –> 00:25:33.985
remain as it is, okay?

263
00:25:34.535 –> 00:25:36.705
Suppose you have reached the destination.

264
00:25:37.165 –> 00:25:39.265
In that case you are getting a positive reward.

265
00:25:39.445 –> 00:25:43.065
So on that particular time, you are going to do a

266
00:25:43.745 –> 00:25:45.545
gradient update of your actions.

267
00:25:45.615 –> 00:25:47.305
That is, you have to increase the probability

268
00:25:47.365 –> 00:25:49.825
of those actions that has led you

269
00:25:49.885 –> 00:25:51.345
to reach in the final state.

270
00:25:52.185 –> 00:25:55.015
Okay? Suppose the state

271
00:25:55.015 –> 00:25:57.175
before this that you are ending up, supposed

272
00:25:57.365 –> 00:26:02.255
because of some, uh, some kind of episode, you are reaching

273
00:26:02.275 –> 00:26:05.935
to this state and I’m giving a reward of minus one.

274
00:26:05.935 –> 00:26:07.295
That means I’m penalizing you.

275
00:26:07.395 –> 00:26:11.455
In that case, you must reduce the probability associated

276
00:26:12.035 –> 00:26:17.025
to this aot A one, A two in proportion

277
00:26:17.925 –> 00:26:20.185
to the reward that you are losing.

278
00:26:22.385 –> 00:26:23.485
Mm-hmm. Mm-hmm.

279
00:26:23.595 –> 00:26:28.275
That is what is called gradient update in this mechanism,

280
00:26:28.275 –> 00:26:32.195
because we don’t have a explicit formula

281
00:26:32.535 –> 00:26:35.755
to find out your relation

282
00:26:35.755 –> 00:26:37.715
of your action and your reward.

283
00:26:38.695 –> 00:26:41.155
So we don’t have any formula, so we are not going

284
00:26:41.155 –> 00:26:45.005
to find out any gradient like, uh, faster or derivative.

285
00:26:45.745 –> 00:26:49.165
So what we’re doing in proportion to your reward, uh,

286
00:26:49.475 –> 00:26:53.405
increment or decrement in proportion to that, you are going

287
00:26:53.405 –> 00:26:56.065
to either increase the probability

288
00:26:56.085 –> 00:26:57.585
or decrease the priority of action.

289
00:26:59.955 –> 00:27:02.015
Sir, an episode means trajectory.

290
00:27:03.995 –> 00:27:07.295
Yes. Right? Right? Correct. Okay.

291
00:27:13.335 –> 00:27:15.715
Um, like in most of the cases, trajectory means,

292
00:27:15.745 –> 00:27:20.625
suppose we are having some kind of, um, missile that we, uh,

293
00:27:21.075 –> 00:27:23.665
trajectory talks about that that missile

294
00:27:23.665 –> 00:27:24.745
or any kind of satellite,

295
00:27:24.985 –> 00:27:28.305
whenever it is taking off, it follows a,

296
00:27:28.965 –> 00:27:30.305
uh, guided path.

297
00:27:30.565 –> 00:27:35.115
Yes, that guided path that is called traject,

298
00:27:36.865 –> 00:27:40.205
and if there is a deviation from the guided path, then we

299
00:27:40.745 –> 00:27:44.005
get to know that our mission is going to failing.

300
00:27:44.875 –> 00:27:49.065
Okay? Like same terminology used

301
00:27:49.065 –> 00:27:52.495
here, okay?

302
00:27:52.555 –> 00:27:55.655
So this is what is called your return, return

303
00:27:55.715 –> 00:27:57.255
for the episode of tau.

304
00:27:57.605 –> 00:27:58.855
This is for one episode.

305
00:27:59.645 –> 00:28:02.775
Like that you have multiple episode for every episode,

306
00:28:03.455 –> 00:28:04.715
you have to get this return.

307
00:28:04.975 –> 00:28:08.575
Do the averaging operation that aging is called,

308
00:28:08.635 –> 00:28:11.735
is expectation taken over all possible

309
00:28:12.955 –> 00:28:15.025
episode, okay?

310
00:28:15.335 –> 00:28:19.005
This is called trajectory also, which is written as Tao.

311
00:28:21.555 –> 00:28:24.325
Okay? So this has already been told to you many times.

312
00:28:24.665 –> 00:28:25.925
You can find out this one.

313
00:28:25.985 –> 00:28:28.965
Let me share this one with you,

314
00:28:30.615 –> 00:28:35.405
with you chat window.

315
00:28:40.315 –> 00:28:43.755
Uh, someone has already shared it. Yeah.

316
00:28:45.025 –> 00:28:47.085
So go to unit four. It is policy gradient.

317
00:28:48.445 –> 00:28:50.405
Somebody has already shared it. Okay, fine. Good.

318
00:29:03.085 –> 00:29:06.095
Okay. So this, this means that you are going

319
00:29:06.095 –> 00:29:10.195
to find out there expected sum

320
00:29:10.195 –> 00:29:11.835
of discounted reward for that.

321
00:29:11.835 –> 00:29:16.505
Basically, this is the, this is the reward

322
00:29:16.565 –> 00:29:18.975
for the episode called Tao,

323
00:29:19.355 –> 00:29:22.095
and what is the probability of choosing that episode?

324
00:29:24.745 –> 00:29:27.475
That probability is called probability of Tao given theater.

325
00:29:28.805 –> 00:29:32.755
Okay? So this is a requirement. Why?

326
00:29:32.755 –> 00:29:36.515
Because you need to compute this or computing this.

327
00:29:36.735 –> 00:29:39.115
You need to, this is a mathematical relation

328
00:29:39.115 –> 00:29:41.365
between the first expression, the second expression, okay?

329
00:29:42.825 –> 00:29:45.605
Now from this, they’re forming that you have

330
00:29:45.605 –> 00:29:48.885
to find out the gradient of gradient of jta.

331
00:29:48.885 –> 00:29:51.885
For that you need gradient of this quantity.

332
00:29:53.825 –> 00:29:56.205
So gradient of this quantity is written again,

333
00:29:56.205 –> 00:29:59.925
with this multiplied by gradient of log of this term.

334
00:30:00.195 –> 00:30:02.045
That is what they have replaced it here.

335
00:30:02.735 –> 00:30:06.505
This one, basically the gradient is going to be like this.

336
00:30:08.385 –> 00:30:11.035
This will become gradient of log

337
00:30:11.055 –> 00:30:13.835
of this term, okay?

338
00:30:15.045 –> 00:30:19.485
Now this, this gradient computation is very, very difficult.

339
00:30:20.115 –> 00:30:21.525
It’s what? That there are many

340
00:30:22.035 –> 00:30:23.725
approximation algorithm given.

341
00:30:23.905 –> 00:30:26.165
So this is the first, uh,

342
00:30:26.165 –> 00:30:28.325
approximation algorithm is called reinforce.

343
00:30:29.635 –> 00:30:32.935
So reinforce what they have given is basically,

344
00:30:34.495 –> 00:30:35.515
let me show you that.

345
00:30:37.055 –> 00:30:40.455
So in very, very crude manner,

346
00:30:42.415 –> 00:30:44.835
you compute the reward for a

347
00:30:45.385 –> 00:30:48.345
episode, okay?

348
00:30:48.965 –> 00:30:53.165
And this reward is computed for every state manner, okay?

349
00:30:54.045 –> 00:30:56.035
Every state you are taking action.

350
00:30:57.015 –> 00:30:58.875
You may have reward, you may not have reward,

351
00:30:58.875 –> 00:31:01.235
but the final state definitely has some kind of reward.

352
00:31:01.695 –> 00:31:04.155
So for most of the cases, we don’t have reward.

353
00:31:04.165 –> 00:31:05.635
It’ll be multiply by jro only.

354
00:31:05.705 –> 00:31:09.995
This town au is computed as a sum

355
00:31:10.015 –> 00:31:11.915
of every step reward, right?

356
00:31:12.145 –> 00:31:15.895
This every step, step by step,

357
00:31:15.895 –> 00:31:17.055
you are going to get a reward.

358
00:31:17.635 –> 00:31:19.615
For most this reward remains jro,

359
00:31:19.615 –> 00:31:20.975
only the last state has some reward.

360
00:31:22.035 –> 00:31:25.805
However, this gradient of this log probability of

361
00:31:26.565 –> 00:31:28.845
A given s and the time t.

362
00:31:29.975 –> 00:31:32.115
So this part is going to very difficult for us

363
00:31:32.115 –> 00:31:33.275
to know, okay?

364
00:31:34.055 –> 00:31:36.635
And the objective of policy gradient is basically

365
00:31:37.795 –> 00:31:39.615
if the final reward that you’re getting,

366
00:31:40.315 –> 00:31:43.975
the reward is positive, then you have to change the value

367
00:31:43.995 –> 00:31:45.655
of this with a higher value.

368
00:31:46.315 –> 00:31:47.695
The final reward is negative.

369
00:31:47.765 –> 00:31:49.895
Then you have to reduce the probability of this

370
00:31:49.895 –> 00:31:51.375
with lower value.

371
00:31:52.045 –> 00:31:53.775
This higher value and lower value by

372
00:31:53.775 –> 00:31:55.775
how much adjustment you’re going to do that is going

373
00:31:55.775 –> 00:31:59.165
to be proportion of your, uh, this sum

374
00:31:59.165 –> 00:32:01.165
of discounted reward, okay?

375
00:32:01.705 –> 00:32:05.025
And you have to update your neur network weight

376
00:32:05.055 –> 00:32:06.545
with the gradient ascent,

377
00:32:06.995 –> 00:32:10.665
which last class I have told you we have to do plus

378
00:32:10.685 –> 00:32:13.745
of alpha multiplied by this gradient operator.

379
00:32:15.495 –> 00:32:18.135
This is the gradient operator, okay?

380
00:32:18.945 –> 00:32:23.845
And this t stands for the timestamp within eight episode.

381
00:32:24.035 –> 00:32:27.085
Your episode might be having capital T number of timestamp.

382
00:32:27.815 –> 00:32:29.075
So you’re starting your game from

383
00:32:29.075 –> 00:32:31.315
0 1, 2, 3, up to capital T.

384
00:32:31.335 –> 00:32:34.955
So T is flowing from zero all the way to up to capital T.

385
00:32:37.535 –> 00:32:38.935
Okay? And,

386
00:32:38.945 –> 00:32:40.805
and here, this is called Montecarlo

387
00:32:41.615 –> 00:32:43.045
Montecarlo policy gradient

388
00:32:43.075 –> 00:32:47.105
because at every episode

389
00:32:47.135 –> 00:32:51.505
that you have considered is R tau, IR tai, basically

390
00:32:51.505 –> 00:32:52.785
what I episode

391
00:32:53.245 –> 00:32:57.875
and that I episode whatever reward you have obtained, okay?

392
00:32:59.185 –> 00:33:03.235
That you do the submission for every IH episode.

393
00:33:04.045 –> 00:33:07.785
So I basically stands for the index of your episode.

394
00:33:07.805 –> 00:33:10.745
So this is how many episode you have, one, two, M,

395
00:33:11.245 –> 00:33:13.865
you have aim, number of episode, every episode,

396
00:33:15.085 –> 00:33:16.345
reward whatever you are getting.

397
00:33:16.765 –> 00:33:20.785
You do the submission divide by total number of episodes,

398
00:33:20.785 –> 00:33:22.625
and this is called Monte, called estimation.

399
00:33:27.095 –> 00:33:31.695
Okay? So here is a policy gradient, theodor, which

400
00:33:32.455 –> 00:33:34.855
I have already done it, plain paper

401
00:33:34.995 –> 00:33:37.655
and think everything is written on here, okay?

402
00:33:38.685 –> 00:33:40.015
Same thing, whatever I have done.

403
00:33:48.235 –> 00:33:52.475
So Now I will show you the reinforce algorithm

404
00:33:57.895 –> 00:33:58.895
Inforce.

405
00:34:22.795 –> 00:34:27.785
So Lillian, when this website, one of the most,

406
00:34:28.125 –> 00:34:31.065
uh, detailed, uh, blogging,

407
00:34:31.525 –> 00:34:34.225
she makes basic Chinese.

408
00:34:42.115 –> 00:34:44.695
So this is again, proof of policy, gradient theorem,

409
00:34:44.695 –> 00:34:48.615
which you can, whatever I have derived it is given here also

410
00:34:48.725 –> 00:34:50.495
with the proper explanation is given.

411
00:34:50.495 –> 00:34:51.895
Here, you can read on your own.

412
00:34:53.175 –> 00:34:55.475
I’m going to reinforce algorithm.

413
00:34:55.475 –> 00:34:57.035
This is called inforce algorithm.

414
00:34:57.625 –> 00:35:00.635
This algorithm was proposed by the father figure

415
00:35:00.655 –> 00:35:01.795
of reinforcement learning.

416
00:35:02.645 –> 00:35:04.715
There are two authors, bra, and

417
00:35:09.925 –> 00:35:13.845
then I have to teach you actor critic, that’s it.

418
00:35:13.845 –> 00:35:18.365
Then policy grad and okay, and then, uh, PP algorithm.

419
00:35:19.755 –> 00:35:21.335
So what is this reinforced algorithm?

420
00:35:22.775 –> 00:35:25.475
So this is also known as Monte Caldo policy gradient.

421
00:35:27.955 –> 00:35:30.495
So it is very, very simple algorithm that

422
00:35:31.285 –> 00:35:33.895
this is the normal operation that we all have to do,

423
00:35:34.665 –> 00:35:35.875
okay? That is basically

424
00:35:36.505 –> 00:35:38.275
This, uh,

425
00:35:44.925 –> 00:35:46.745
You can correlate this and this easily,

426
00:35:49.965 –> 00:35:50.965
What is it?

427
00:35:52.475 –> 00:35:56.245
This grad, the gradient of this JTA is

428
00:35:58.135 –> 00:36:00.915
the sum over gradient

429
00:36:00.935 –> 00:36:03.555
of this term multiplied by rau.

430
00:36:03.665 –> 00:36:04.665
Okay?

431
00:36:05.055 –> 00:36:09.845
This one Au multi au basically your gain,

432
00:36:10.085 –> 00:36:11.165
I mean reward, okay?

433
00:36:11.425 –> 00:36:15.405
So that is what is given au multiplied by gradient of this

434
00:36:15.995 –> 00:36:18.245
lock probability of action given state.

435
00:36:20.715 –> 00:36:21.925
Okay? Now to do that,

436
00:36:23.425 –> 00:36:25.045
so you can initialize your policy

437
00:36:25.045 –> 00:36:26.645
parameter randomly first steps.

438
00:36:26.805 –> 00:36:29.885
Second step is you generate one projectile,

439
00:36:33.075 –> 00:36:34.415
one random projectile you generate.

440
00:36:35.205 –> 00:36:37.875
Okay? Now, after that,

441
00:36:39.235 –> 00:36:43.685
after that, for this projectile, it is the first episode

442
00:36:43.685 –> 00:36:46.365
that you have generated like that you’re going to do

443
00:36:47.605 –> 00:36:49.445
multiple episodes, okay?

444
00:36:49.905 –> 00:36:53.805
For every episode, you have to do this, this small T stands

445
00:36:53.865 –> 00:36:58.595
for the small, T stands for it is stepping

446
00:36:58.595 –> 00:37:02.165
through the total number of states

447
00:37:02.825 –> 00:37:05.005
inside your environment, okay?

448
00:37:05.115 –> 00:37:09.565
Like you have S one, S two, all the way up

449
00:37:09.565 –> 00:37:10.605
to is capital T.

450
00:37:10.605 –> 00:37:11.845
So you have capital T number

451
00:37:11.845 –> 00:37:13.165
of states within the environment.

452
00:37:13.945 –> 00:37:15.685
So you’re stepping through every states.

453
00:37:16.525 –> 00:37:17.815
That is what they want to say,

454
00:37:18.725 –> 00:37:19.725
Okay?

455
00:37:20.185 –> 00:37:24.985
So estimate the return, the return that is available

456
00:37:25.515 –> 00:37:27.345
after every action that you take.

457
00:37:27.465 –> 00:37:28.625
That is called gt.

458
00:37:29.305 –> 00:37:31.435
It is gt for most of the case,

459
00:37:31.575 –> 00:37:34.675
GT will be zero only when you go to the last state,

460
00:37:34.675 –> 00:37:36.315
then only you get some positive

461
00:37:36.375 –> 00:37:37.915
or negative value for the gt.

462
00:37:37.985 –> 00:37:41.965
Okay? Then the next step is the policy update.

463
00:37:42.815 –> 00:37:46.395
So policy update is basically what it is saying that

464
00:37:47.815 –> 00:37:49.315
at every action that you do,

465
00:37:50.595 –> 00:37:54.275
whatever reward you are getting based on that, so

466
00:37:54.935 –> 00:37:57.955
you do this gumma power of T, that is your discount.

467
00:37:57.955 –> 00:38:01.715
You’re applying at every times step T multiplied

468
00:38:01.715 –> 00:38:03.275
by your reward.

469
00:38:03.275 –> 00:38:06.195
That is available to you at every time. Step action.

470
00:38:07.275 –> 00:38:09.415
If this reward is zero for most of the cases,

471
00:38:09.685 –> 00:38:11.895
then this part will not contribute anything at all.

472
00:38:12.045 –> 00:38:15.245
Okay? Only for the last date it is going to be happening.

473
00:38:17.045 –> 00:38:20.065
So after that, the next time is this gradient

474
00:38:20.085 –> 00:38:21.825
of log of this thing,

475
00:38:23.175 –> 00:38:24.175
Okay?

476
00:38:24.365 –> 00:38:27.705
Now, this gradient term is impossible to compute.

477
00:38:27.845 –> 00:38:29.465
So what they say, they say

478
00:38:29.465 –> 00:38:31.465
that just you forget about this gradient term.

479
00:38:31.765 –> 00:38:36.470
You just take this part alone, Just take locked probability

480
00:38:36.565 –> 00:38:40.195
of a givenness and it become ready.

481
00:38:41.365 –> 00:38:42.835
It’ll take long amount of time

482
00:38:42.835 –> 00:38:45.395
because you are, instead of taking gradient, you are.

483
00:38:45.775 –> 00:38:48.275
So whenever you take gradient, your value will reduce.

484
00:38:48.615 –> 00:38:50.795
If you don’t take gradient, your value will be higher.

485
00:38:51.255 –> 00:38:54.795
So naturally you have a large error, you can reduce a error.

486
00:38:55.015 –> 00:38:59.235
If you do this operation for, uh, thousands

487
00:38:59.235 –> 00:39:01.835
and thousands of iteration, then only this

488
00:39:02.405 –> 00:39:05.575
will be getting f uh, refined value.

489
00:39:06.365 –> 00:39:08.455
Okay? So that is what is not,

490
00:39:09.075 –> 00:39:11.895
is not much critical, okay?

491
00:39:13.455 –> 00:39:17.175
You understand we are not going to, we don’t have

492
00:39:17.175 –> 00:39:20.015
that much capacity to do this gradient of this thing.

493
00:39:21.565 –> 00:39:23.465
So just take lock prob of this thing.

494
00:39:24.715 –> 00:39:28.575
So what does it literally translate to in physical way?

495
00:39:29.085 –> 00:39:31.655
That is at every time you take action,

496
00:39:31.715 –> 00:39:36.695
if the reward is available to you, then based on the reward,

497
00:39:36.795 –> 00:39:41.415
if the GT is zero, then this term be ing the GT is negative.

498
00:39:42.435 –> 00:39:44.415
So your theta parameter will be

499
00:39:45.335 –> 00:39:47.985
reducing the probability reducing.

500
00:39:48.225 –> 00:39:50.065
I mean, your theta update is going

501
00:39:50.065 –> 00:39:51.505
to be on the reduction side

502
00:39:51.815 –> 00:39:55.145
because this action has resulted to a negative result.

503
00:39:55.775 –> 00:40:00.505
Okay? If the action has resolved, that action is a given,

504
00:40:00.565 –> 00:40:04.185
is if the action has resulted to a positive result, then in

505
00:40:04.185 –> 00:40:06.225
that case you have

506
00:40:06.225 –> 00:40:08.065
to increase the probability of your action.

507
00:40:09.215 –> 00:40:11.155
So your probability of action is represented

508
00:40:11.155 –> 00:40:12.235
by your theater parameters.

509
00:40:12.235 –> 00:40:16.275
So they will do this plus operation in that case. Okay?

510
00:40:19.265 –> 00:40:22.085
Now I’ll show you the inforce algorithm here

511
00:40:22.485 –> 00:40:23.565
I have written down.

512
00:40:24.845 –> 00:40:27.915
So this is the collab.

513
00:40:29.435 –> 00:40:32.575
So here I will show you where is that happening?

514
00:40:34.985 –> 00:40:36.845
So this is for Cardpool environment, okay?

515
00:40:37.235 –> 00:40:39.125
This Cardpool game, you already know

516
00:40:41.555 –> 00:40:44.355
Cardpool game is basically there is a, uh,

517
00:40:45.045 –> 00:40:48.875
there is a pole you can think of like there is a pain here.

518
00:40:49.845 –> 00:40:52.825
Like I have a pain here. You already know this, right?

519
00:40:52.825 –> 00:40:53.825
This is a pain here.

520
00:40:55.405 –> 00:40:58.975
This pain, if I give up my another hand,

521
00:40:59.195 –> 00:41:02.095
so this will not stand like this, okay?

522
00:41:02.515 –> 00:41:04.575
So there is a pole which is on a cart.

523
00:41:05.275 –> 00:41:07.455
The pole is basically oscillating.

524
00:41:08.645 –> 00:41:10.945
If the pole is oscillating less than 30

525
00:41:10.965 –> 00:41:12.105
degree, then it is fine.

526
00:41:12.105 –> 00:41:14.185
If it is more 30 degree, you are lost.

527
00:41:14.195 –> 00:41:17.625
Again, your cart is moving in the x axis direction,

528
00:41:19.205 –> 00:41:21.285
positive meaning it is moving.

529
00:41:22.065 –> 00:41:24.965
As the movement is going on, the pole is oscillating.

530
00:41:25.025 –> 00:41:26.525
So that is what the game is.

531
00:41:29.005 –> 00:41:31.985
So for this game, I have already shown you how to do this

532
00:41:31.985 –> 00:41:33.965
by own deep queue learning, okay?

533
00:41:34.755 –> 00:41:36.325
This game, we are going to do it

534
00:41:36.325 –> 00:41:40.235
by using this inforce algorithm, the inforce

535
00:41:41.735 –> 00:41:43.275
in every reinforcement learning,

536
00:41:43.275 –> 00:41:45.115
what is our objective is to learn the policy.

537
00:41:45.655 –> 00:41:47.035
Policy means at every state.

538
00:41:47.035 –> 00:41:49.835
That means at every position of your card,

539
00:41:51.285 –> 00:41:52.935
what is the action he should take?

540
00:41:53.935 –> 00:41:56.835
He has, uh, what kind of actions either

541
00:41:58.535 –> 00:42:01.695
move this side or move this side based.

542
00:42:01.995 –> 00:42:05.345
So if you move this side naturally your pole will be

543
00:42:06.465 –> 00:42:08.355
falling back that side, okay?

544
00:42:10.135 –> 00:42:12.315
If you move farther, it’ll fall back like that.

545
00:42:13.055 –> 00:42:16.235
So you should not move too much on the same reaction,

546
00:42:16.235 –> 00:42:18.715
meaning the moment it has fallen back,

547
00:42:18.895 –> 00:42:23.625
if you move this side, then it’ll be by the opposite side,

548
00:42:23.765 –> 00:42:25.465
uh, action reaction mode.

549
00:42:25.475 –> 00:42:26.865
It’ll be going towards, it’ll be

550
00:42:26.865 –> 00:42:27.905
tilted towards that, like that.

551
00:42:27.905 –> 00:42:28.945
It’ll be balancing out.

552
00:42:29.525 –> 00:42:31.525
So at every state, it has to find out

553
00:42:31.525 –> 00:42:32.805
what action he’s going to take.

554
00:42:32.805 –> 00:42:34.165
That is the job, okay?

555
00:42:36.425 –> 00:42:38.685
So along with that, along with the movement detection,

556
00:42:38.685 –> 00:42:40.605
and there’s something called velocity of movement

557
00:42:40.745 –> 00:42:42.565
and the angular velocity of your pole.

558
00:42:42.695 –> 00:42:46.385
These are the four actions, uh, four uh, states possible.

559
00:42:48.015 –> 00:42:51.515
Okay? So, so we have a neural network

560
00:42:51.515 –> 00:42:52.795
that we are configuring here.

561
00:42:53.895 –> 00:42:57.395
We have two layers. It is accepting four, four

562
00:42:58.885 –> 00:43:01.455
dimension is four in the neural uh, network.

563
00:43:01.865 –> 00:43:05.425
First layer, it is taking that into 64 number

564
00:43:05.425 –> 00:43:07.305
of layers here, okay?

565
00:43:08.225 –> 00:43:09.795
They applying a activation.

566
00:43:09.795 –> 00:43:14.165
Then from 64 number of dimension

567
00:43:14.205 –> 00:43:17.485
of neuron, it is coming down to action space.

568
00:43:18.435 –> 00:43:19.865
Okay? So

569
00:43:19.865 –> 00:43:22.065
that means it is basically mapping between your state.

570
00:43:22.135 –> 00:43:24.025
Your state is having four dimension

571
00:43:24.615 –> 00:43:26.105
from state to action space.

572
00:43:26.215 –> 00:43:31.085
This is a neural net. So we are going to now do the stepping

573
00:43:31.795 –> 00:43:34.245
that is basically we are going to train our neural network.

574
00:43:35.365 –> 00:43:38.145
So for every neural network training, you need to have a

575
00:43:38.655 –> 00:43:41.505
cost function that you’re going to maximize

576
00:43:41.505 –> 00:43:42.705
or minimize, okay?

577
00:43:43.125 –> 00:43:45.145
The cost function here is nothing

578
00:43:45.245 –> 00:43:48.775
but geo theta.

579
00:43:49.485 –> 00:43:53.295
So geo theta is basically what this expected value

580
00:43:53.315 –> 00:43:55.015
of this term that is a J.

581
00:43:55.015 –> 00:43:59.575
The this is a J, the expected value

582
00:43:59.575 –> 00:44:00.615
of without the gradient

583
00:44:00.615 –> 00:44:02.175
operation expected value of this term.

584
00:44:02.245 –> 00:44:05.125
Okay? So we have to run now, right?

585
00:44:06.375 –> 00:44:07.995
So when you are doing the stepping through,

586
00:44:08.165 –> 00:44:10.395
first we reset it to initial state

587
00:44:10.395 –> 00:44:12.555
that when we are starting from the initial state.

588
00:44:13.175 –> 00:44:17.985
So then we are making this to be torch, uh,

589
00:44:18.875 –> 00:44:21.225
torch variable, let is called tiner, okay?

590
00:44:22.515 –> 00:44:24.935
So then after that we are going

591
00:44:24.935 –> 00:44:28.135
to store all the action state reward.

592
00:44:32.875 –> 00:44:36.885
So now we are starting this, that this tape stands for

593
00:44:38.795 –> 00:44:42.275
how many episode you want

594
00:44:42.275 –> 00:44:43.955
to run, okay?

595
00:44:44.645 –> 00:44:48.505
This is how many episode within an episode you are

596
00:44:48.505 –> 00:44:49.545
taping through all the states.

597
00:44:49.805 –> 00:44:52.105
So that is this, this while loop, okay?

598
00:44:54.325 –> 00:44:56.185
So within an episode, what you are doing,

599
00:44:56.565 –> 00:44:59.305
you are asking the neural network to take a action.

600
00:45:00.685 –> 00:45:02.425
So that is why you’re calling the neural network

601
00:45:03.325 –> 00:45:05.105
to take a action for a given state.

602
00:45:05.125 –> 00:45:06.705
The state is obvious, whatever.

603
00:45:07.835 –> 00:45:10.455
So you are going to get the probability

604
00:45:10.475 –> 00:45:13.525
of your action, okay?

605
00:45:16.515 –> 00:45:19.135
So then you are using this categorical.

606
00:45:19.285 –> 00:45:22.735
This is basically to find out the distribution.

607
00:45:22.735 –> 00:45:24.215
Suppose you have the probabilities,

608
00:45:24.325 –> 00:45:26.375
then you can use this categorical function

609
00:45:26.475 –> 00:45:30.105
to turn them into a probability distribution.

610
00:45:31.535 –> 00:45:34.435
And then this distribution function is coming from this

611
00:45:34.435 –> 00:45:35.515
basically categorical.

612
00:45:35.865 –> 00:45:38.995
This categorical we are already having, right? Yeah.

613
00:45:38.995 –> 00:45:41.515
So this is a tot within the tot you have

614
00:45:41.515 –> 00:45:42.595
this distribution class.

615
00:45:42.595 –> 00:45:45.205
Within that you have categorical, okay?

616
00:45:45.385 –> 00:45:49.005
If you call this categorical, the input for the categorical,

617
00:45:49.005 –> 00:45:52.645
if you provide a, that is a probability, it’ll turn them

618
00:45:52.665 –> 00:45:53.805
to a distribution function.

619
00:45:54.825 –> 00:45:57.715
From this distribution function. You can call this function.

620
00:45:57.865 –> 00:46:00.075
This function is readily available within the torch.

621
00:46:00.545 –> 00:46:03.675
This is not our user defined function.

622
00:46:03.855 –> 00:46:04.995
It is readily available.

623
00:46:05.055 –> 00:46:08.315
So you can call sample function available within

624
00:46:08.415 –> 00:46:10.475
the distribution.

625
00:46:12.285 –> 00:46:14.835
It’ll give you the sample. What is sample?

626
00:46:14.895 –> 00:46:19.045
Sample is basically what action you should take. Okay?

627
00:46:21.875 –> 00:46:26.785
So you have got your action now under this state,

628
00:46:26.925 –> 00:46:31.275
it is giving you what action you must take, okay?

629
00:46:31.375 –> 00:46:33.555
Now, if you give this action to the stepping function

630
00:46:33.555 –> 00:46:36.435
of your environment, it’ll give you the next

631
00:46:36.435 –> 00:46:37.515
state under that action.

632
00:46:37.815 –> 00:46:39.515
So the next state is stored over here,

633
00:46:39.515 –> 00:46:43.285
observation under spot, this reward terminated,

634
00:46:44.025 –> 00:46:45.405
all the things are there, okay?

635
00:46:47.675 –> 00:46:51.025
So then after you are basically appending all

636
00:46:51.025 –> 00:46:53.025
of your actions that you have already

637
00:46:53.925 –> 00:46:55.345
got from your neural network.

638
00:46:57.775 –> 00:47:00.875
You are appending all the states that you are observing

639
00:47:01.995 –> 00:47:04.805
from your neural network and all, okay?

640
00:47:05.505 –> 00:47:10.145
After that, this is what is your computation of your reward?

641
00:47:10.465 –> 00:47:12.265
Discounted, discounted return.

642
00:47:12.735 –> 00:47:14.745
What is the formula for discounted return?

643
00:47:16.125 –> 00:47:20.965
That formula is not shown over here. Not shown over here.

644
00:47:21.365 –> 00:47:26.175
Actually, they also did not

645
00:47:26.175 –> 00:47:27.615
write the formula for that.

646
00:47:28.055 –> 00:47:32.475
I, so the formula is basically

647
00:47:34.395 –> 00:47:35.705
gamma, the power of

648
00:47:36.105 –> 00:47:39.645
k, K is basically what?

649
00:47:40.535 –> 00:47:45.425
K is basically your every timestamp within that

650
00:47:45.985 –> 00:47:48.275
capital T number of total number

651
00:47:48.295 –> 00:47:49.915
of states possible inside a episode.

652
00:47:50.065 –> 00:47:52.915
Okay? So you are taking this

653
00:47:55.645 –> 00:48:00.045
discount, multi uh, discount to the power of your states

654
00:48:00.045 –> 00:48:02.405
that you have gone into the future multiplied,

655
00:48:02.405 –> 00:48:05.985
but the reward available for that particular, for

656
00:48:05.985 –> 00:48:08.835
that particular state do the summation.

657
00:48:08.835 –> 00:48:11.995
That means the sum of our come up, RK multiplied by R. Okay?

658
00:48:13.055 –> 00:48:17.025
So that is your uh, reward for

659
00:48:17.025 –> 00:48:18.345
that particular timestamp

660
00:48:19.045 –> 00:48:21.065
and for every timestamp you are basically

661
00:48:21.215 –> 00:48:22.465
appending your reward.

662
00:48:23.055 –> 00:48:25.305
This one, this is basically nothing,

663
00:48:25.365 –> 00:48:29.325
but this is one A that will have T number of elements.

664
00:48:29.595 –> 00:48:32.165
Each of the elements says what is the reward for

665
00:48:32.285 –> 00:48:33.725
that particular time?

666
00:48:34.035 –> 00:48:37.205
What is the cumulative reward for that particular time state

667
00:48:40.945 –> 00:48:42.975
After that, now we are going to do this.

668
00:48:43.765 –> 00:48:45.855
This is where we’re doing the gradient update.

669
00:48:46.915 –> 00:48:49.095
So we have formulated, uh,

670
00:48:49.095 –> 00:48:53.015
we have filled up our state action and discounted rewards.

671
00:48:54.825 –> 00:48:57.405
The states are coming out of what this

672
00:48:58.395 –> 00:49:01.185
look at this while loop is going

673
00:49:01.185 –> 00:49:04.145
to run till you have not reached your terminal state.

674
00:49:05.685 –> 00:49:07.705
So that means every time you are hitting a new state,

675
00:49:07.725 –> 00:49:09.865
you are storing the, and you are storing the reward.

676
00:49:09.935 –> 00:49:12.025
Once you have completed one episode,

677
00:49:12.025 –> 00:49:13.265
then you have come out of this.

678
00:49:14.015 –> 00:49:17.955
Then you are calculating what is the total amount

679
00:49:17.955 –> 00:49:21.755
of reward you have accumulated in every timestamp manner.

680
00:49:22.835 –> 00:49:26.205
This is the area that will track at every timestamp manner

681
00:49:26.315 –> 00:49:27.525
what is the total reward.

682
00:49:27.625 –> 00:49:28.885
Uh, got it. Okay?

683
00:49:29.345 –> 00:49:31.695
Now you have basically a area

684
00:49:31.755 –> 00:49:34.255
for the state area four action.

685
00:49:35.305 –> 00:49:38.835
And then, so this action is not going to be

686
00:49:40.095 –> 00:49:41.975
a two dimensional matters, going to a singular

687
00:49:41.975 –> 00:49:45.055
because what every state you have taken one action which has

688
00:49:45.055 –> 00:49:48.365
come up from this experience, okay?

689
00:49:48.365 –> 00:49:51.735
This action basically getting append, that is basically your

690
00:49:52.685 –> 00:49:54.955
experienced episode.

691
00:49:56.675 –> 00:50:00.775
So under that experienced episode, what is your total amount

692
00:50:00.875 –> 00:50:04.095
of discounted reward you have got that is stored here.

693
00:50:05.065 –> 00:50:07.845
Okay? So we are getting from here

694
00:50:08.145 –> 00:50:10.815
and then we are now

695
00:50:12.365 –> 00:50:15.575
sending this state information to the neural network.

696
00:50:15.955 –> 00:50:17.695
Neural network is giving some probability

697
00:50:19.165 –> 00:50:21.375
because it’ll give you the probability for the action

698
00:50:21.375 –> 00:50:22.695
that you must take it, okay?

699
00:50:23.005 –> 00:50:25.695
Then you use the categorical distribution

700
00:50:25.695 –> 00:50:27.365
to turn them into prob distribution.

701
00:50:28.665 –> 00:50:30.915
This will give you the log probability of that action

702
00:50:32.155 –> 00:50:36.505
and this log probability need to multiply with the return

703
00:50:37.495 –> 00:50:41.725
that is for each and every time state, okay?

704
00:50:42.025 –> 00:50:45.615
For each and every, we are not doing any submission here.

705
00:50:45.645 –> 00:50:48.055
Look at you. Uh, yeah.

706
00:50:48.055 –> 00:50:50.815
So this, this loss function that we’re con computing,

707
00:50:51.445 –> 00:50:54.295
this loss function is basically so loss minus

708
00:50:54.395 –> 00:50:56.735
of this second, so is by c*m cumulative loss.

709
00:50:58.515 –> 00:51:01.735
So this loss is computed for every timestamp, whatever

710
00:51:02.315 –> 00:51:05.695
the contribution coming from the calculation of G,

711
00:51:06.275 –> 00:51:09.105
which is multiplied by its own log probability,

712
00:51:09.375 –> 00:51:12.535
like this formula itself is the lock probability.

713
00:51:13.875 –> 00:51:15.645
Look at, we’re not taking gradient here.

714
00:51:16.375 –> 00:51:18.745
This is lock probability we’re taking multiplied by

715
00:51:20.175 –> 00:51:22.555
at every timestamp, what is the reward I have got.

716
00:51:22.695 –> 00:51:26.565
So this is what we are doing this, okay?

717
00:51:26.945 –> 00:51:31.615
And do that in a cumulative manner, okay?

718
00:51:32.595 –> 00:51:37.535
After that, you are doing this normal gradient

719
00:51:38.125 –> 00:51:40.695
step of your toch by toch mechanism.

720
00:51:41.195 –> 00:51:43.535
You set the zero gradient for the optimizer,

721
00:51:43.965 –> 00:51:45.415
then you do lost dot backward.

722
00:51:46.515 –> 00:51:50.725
Then you do this, okay?

723
00:51:52.765 –> 00:51:55.625
So this is the training part on training part is over.

724
00:51:55.815 –> 00:51:58.135
Then after what you do, then

725
00:51:58.135 –> 00:52:01.295
after you have already made your new network trained.

726
00:52:01.355 –> 00:52:04.635
So it is a frozen network now, so in the frozen network,

727
00:52:04.775 –> 00:52:07.875
if you give the input, what is your present state?

728
00:52:07.875 –> 00:52:09.635
It’ll give you what is the action you must take.

729
00:52:10.565 –> 00:52:13.185
You put that action into the environment again,

730
00:52:13.185 –> 00:52:17.265
you get a new state for every new state you are going to put

731
00:52:17.265 –> 00:52:21.055
that into again, back to your neural network.

732
00:52:21.385 –> 00:52:25.965
It’ll roll out all the based,

733
00:52:26.785 –> 00:52:28.505
I mean at every address,

734
00:52:28.505 –> 00:52:32.945
and you’ll be going to get a improved policy, okay?

735
00:52:33.765 –> 00:52:35.865
And this is what it is. Show you like this.

736
00:52:36.125 –> 00:52:37.185
So let me run this.

737
00:52:46.735 –> 00:52:49.785
Some next steps missed 5,000 episode is going to run.

738
00:53:27.715 –> 00:53:31.535
So approximately it has not taking that much amount of time.

739
00:53:32.645 –> 00:53:36.575
So look at this part is only training part, okay?

740
00:53:37.245 –> 00:53:38.575
Here is a training happening.

741
00:53:45.315 –> 00:53:47.575
So what is the output? You are getting the reward.

742
00:53:51.105 –> 00:53:55.115
It is sometimes increasing,

743
00:53:55.735 –> 00:53:59.235
it is decreasing and this reward has been shown

744
00:53:59.295 –> 00:54:03.095
for only five times.

745
00:54:04.765 –> 00:54:09.085
We can increase this to let’s

746
00:54:09.085 –> 00:54:10.125
say 20 times.

747
00:54:28.495 –> 00:54:31.625
Okay? So for every time, this reward is not going

748
00:54:31.625 –> 00:54:32.865
to be increasing all the time

749
00:54:33.095 –> 00:54:35.375
because it is

750
00:54:39.455 –> 00:54:40.875
the state that you are drawing.

751
00:54:41.325 –> 00:54:46.235
Every time You give a

752
00:54:46.265 –> 00:54:49.275
initial state here, then it’ll produce the probability

753
00:54:49.335 –> 00:54:50.875
for the actions under that state.

754
00:54:51.425 –> 00:54:53.755
Then when you give this probability, that means the actions

755
00:54:53.755 –> 00:54:57.265
that you are giving to the step function

756
00:54:57.265 –> 00:55:01.305
of your environment, it’ll return the new, new state.

757
00:55:02.385 –> 00:55:04.595
This new state is generated from your

758
00:55:05.695 –> 00:55:08.805
environment, okay?

759
00:55:10.505 –> 00:55:12.195
This is not generated by your neural network,

760
00:55:12.345 –> 00:55:14.715
your neur data only given any particular state.

761
00:55:14.715 –> 00:55:18.675
What will be the probability of outta four possible actions?

762
00:55:19.305 –> 00:55:20.555
What action you must choose.

763
00:55:26.745 –> 00:55:30.645
Okay? So this

764
00:55:30.645 –> 00:55:34.815
is reinforce algorithm.

765
00:55:37.105 –> 00:55:41.125
On the next day I will teach you this part.

766
00:55:41.195 –> 00:55:44.855
It’s called act critic, and

767
00:55:44.865 –> 00:55:49.665
after that we’ll be getting into the, this part,

768
00:55:49.985 –> 00:55:51.145
I may not be able to touch it

769
00:55:51.145 –> 00:55:52.905
because a lack of time is called

770
00:55:52.965 –> 00:55:54.865
as synchronous advantage activity.

771
00:56:00.965 –> 00:56:04.495
Then I will get into basically proximal policy, optimizing

772
00:56:05.815 –> 00:56:09.165
not all of them, p, PO, I’ll get into p, p

773
00:56:09.165 –> 00:56:11.045
and TR, PO, TRP and PP.

774
00:56:11.045 –> 00:56:12.805
This PPO is basically implemented in

775
00:56:15.255 –> 00:56:17.195
charge GPT version two on

776
00:56:27.485 –> 00:56:30.505
if possible, I will show you one example of how to generate,

777
00:56:31.565 –> 00:56:35.695
um, how to use PPO

778
00:56:36.435 –> 00:56:40.375
to generate a LLM, okay?

779
00:56:40.555 –> 00:56:43.415
The sentence generation

780
00:56:45.965 –> 00:56:49.685
sentence completion by using PPO for

781
00:56:50.765 –> 00:56:53.065
making a LLM if time is there.

782
00:56:53.995 –> 00:56:56.695
So do you guys have any courts called LLM in this semester?

783
00:56:58.695 –> 00:57:00.235
No sir. No sir.

784
00:57:01.305 –> 00:57:03.165
So what are the courts you’re having this semester?

785
00:57:05.465 –> 00:57:09.645
Deep learning. Deep learning

786
00:57:09.705 –> 00:57:10.925
and this one deep learning.

787
00:57:11.125 –> 00:57:12.125
I

788
00:57:12.505 –> 00:57:14.485
And one data science lab

789
00:57:16.185 –> 00:57:19.205
And deep learning, reinforcement learning, ai,

790
00:57:19.305 –> 00:57:21.565
lab and research.

791
00:57:21.745 –> 00:57:22.745
So what,

792
00:57:23.675 –> 00:57:25.855
Oh, you have AI systems lab something, right?

793
00:57:26.525 –> 00:57:28.895
Yeah. AI labs On Saturday you have four,

794
00:57:28.895 –> 00:57:29.935
five hours of lab.

795
00:57:31.475 –> 00:57:33.895
Yes, sir. Including practical, sir.

796
00:57:34.755 –> 00:57:35.885
Okay. AI lab.

797
00:57:35.905 –> 00:57:38.845
You have then deep learning for deep learning.

798
00:57:38.845 –> 00:57:39.845
There’s no lab,

799
00:57:40.995 –> 00:57:44.455
Uh, no, uh, we don’t have similar, uh, similar to you.

800
00:57:44.475 –> 00:57:47.895
Uh, they are sir is also sharing notebook,

801
00:57:47.895 –> 00:57:49.455
which we practice, sir.

802
00:57:49.995 –> 00:57:51.615
Oh, actually your AI lab takes

803
00:57:51.615 –> 00:57:52.775
care of deep learning, I think.

804
00:57:53.495 –> 00:57:57.345
Yeah. AI lab. It’s ml. ML algorithms.

805
00:57:57.455 –> 00:57:59.345
Yeah, only ml. This same

806
00:58:01.275 –> 00:58:03.815
And few initial classes were related

807
00:58:03.875 –> 00:58:06.175
to binary research algorithms and those

808
00:58:08.355 –> 00:58:09.855
Or AI system lab.

809
00:58:10.675 –> 00:58:12.335
Yes, banner research

810
00:58:12.515 –> 00:58:17.015
and it was first search optimization algorithms,

811
00:58:17.795 –> 00:58:19.255
two three AI algorithms.

812
00:58:19.285 –> 00:58:20.615
Initial class was there.

813
00:58:20.625 –> 00:58:23.215
After that many ML algorithm

814
00:58:23.775 –> 00:58:25.815
classification algorithms, we are coming.

815
00:58:27.705 –> 00:58:31.315
Okay, so the LLM is going

816
00:58:31.315 –> 00:58:33.675
to come next semester. Then

817
00:58:34.315 –> 00:58:36.875
I think next semester is agent ai.

818
00:58:37.535 –> 00:58:40.155
No, third semester agent is in fourth.

819
00:58:40.695 –> 00:58:44.075
Fourth it is. No. Then LLM is in third semester, right?

820
00:58:44.335 –> 00:58:45.715
We need I need to in

821
00:58:45.715 –> 00:58:46.715
Third. It’s in third.

822
00:58:46.715 –> 00:58:50.195
Yeah.

823
00:58:52.035 –> 00:58:54.175
So in that case, whatever I’m teaching is going

824
00:58:54.175 –> 00:58:56.455
to be more advanced for you guys then.

825
00:59:01.795 –> 00:59:03.615
Yes, sir. It is looking like that only

826
00:59:03.615 –> 00:59:06.215
because we are kind of,

827
00:59:06.675 –> 00:59:09.615
You are not, uh, in that stage to catch this thing.

828
00:59:11.155 –> 00:59:15.205
Yeah. But anyhow, I don’t know about all other courses.

829
00:59:15.305 –> 00:59:19.635
That’s whatever the standard, uh, syllabus,

830
00:59:20.195 –> 00:59:22.165
standard topic for person app teaching them.

831
00:59:24.545 –> 00:59:28.125
Okay? Uh,

832
00:59:31.155 –> 00:59:35.125
another course like, uh, autonomous system is going

833
00:59:35.125 –> 00:59:36.165
to be in which semester?

834
00:59:36.195 –> 00:59:38.245
Next se Third semester. Fourth semester.

835
00:59:41.995 –> 00:59:44.235
I need to check actually,

836
00:59:47.645 –> 00:59:50.325
I think autonomous system is not there in this

837
00:59:50.325 –> 00:59:51.685
service, but I need to check.

838
00:59:52.185 –> 00:59:53.765
Yes, sir. We don’t remember this.

839
00:59:54.225 –> 00:59:58.645
Yes, I don’t think it’s there. Okay.

840
01:00:00.925 –> 01:00:04.745
No, actually the like, uh,

841
01:00:06.175 –> 01:00:09.225
your deep learning after that must have

842
01:00:10.765 –> 01:00:11.955
generative ai.

843
01:00:12.065 –> 01:00:15.715
Then only you get into LLM.

844
01:00:16.125 –> 01:00:17.715
After you get into LLM,

845
01:00:17.715 –> 01:00:19.995
then you must get into reinforcement learning.

846
01:00:26.305 –> 01:00:29.585
Yes. Okay.

847
01:00:31.775 –> 01:00:34.105
Okay. So if you don’t have any question,

848
01:00:34.105 –> 01:00:36.225
then I can close to this class.

849
01:00:36.925 –> 01:00:38.385
So 24 is the time now,

850
01:00:38.605 –> 01:00:40.025
Sir, you’ll give us this link.

851
01:00:40.635 –> 01:00:43.265
We’ll try to read. We don’t guarantee

852
01:00:43.265 –> 01:00:44.385
whether we’ll understand.

853
01:00:44.965 –> 01:00:48.585
Ah, I’ll give this, uh,

854
01:00:48.855 –> 01:00:50.225
I’ll give you the chat window.

855
01:00:50.285 –> 01:00:51.545
Huh? You stored everything?

856
01:00:53.055 –> 01:00:53.745
Yeah, sir.

857
01:00:59.715 –> 01:01:01.975
The next sim we have prompt engineering, LLM

858
01:01:01.975 –> 01:01:03.655
and uh, generate two a applications

859
01:01:05.645 –> 01:01:09.455
Prompting LM add, Generate two A applications.

860
01:01:10.445 –> 01:01:11.615
That is one credit course.

861
01:01:12.355 –> 01:01:15.335
Oh, okay. Okay. Both sim, we have agent K.

862
01:01:16.035 –> 01:01:20.835
Oh, and what about autonomous system? Is it there?

863
01:01:21.055 –> 01:01:22.395
No, no, no, sir.

864
01:01:32.155 –> 01:01:35.825
I was thinking it might be there.

865
01:01:35.845 –> 01:01:38.585
If it is there, then I may take it. That’s why I’m asking.

866
01:01:39.605 –> 01:01:43.815
But the problem is autonomous system is much more

867
01:01:45.105 –> 01:01:46.425
difficult course than this.

868
01:01:47.045 –> 01:01:50.185
All. What do you see itself is one

869
01:01:50.185 –> 01:01:53.705
of the toughest course then?

870
01:01:56.385 –> 01:02:00.775
Okay, I have sent you this, uh, link. Okay.

871
01:02:00.955 –> 01:02:04.015
You just, uh, save it on your everybody’s browser.

872
01:02:05.315 –> 01:02:07.655
Yes, sure, sir. Notebook. Also,

873
01:02:09.045 –> 01:02:10.255
This also has notebook.

874
01:02:11.665 –> 01:02:13.325
If you click it, they’ll give you notebook.

875
01:02:14.275 –> 01:02:14.845
Yeah, sure.

876
01:02:20.505 –> 01:02:24.625
China is doing amazing job. I am wondering.

877
01:02:25.095 –> 01:02:26.665
Yeah, sir.

878
01:02:27.205 –> 01:02:31.065
And now they’re accepting for their PhDs.

879
01:02:31.065 –> 01:02:33.545
They don’t want thesis, they want working products,

880
01:02:33.545 –> 01:02:36.145
which will improve China’s state. Ah,

881
01:02:36.245 –> 01:02:36.745
Yes. Yes.

882
01:02:37.125 –> 01:02:41.385
So yeah, Look at, uh huh

883
01:02:41.655 –> 01:02:43.945
They’re redoing their education system also,

884
01:02:43.945 –> 01:02:45.105
sir, from scratch.

885
01:02:45.485 –> 01:02:49.705
Mm-hmm. But look

886
01:02:49.705 –> 01:02:50.785
at how the, uh,

887
01:02:51.605 –> 01:02:55.225
And when 1989, yeah, they had a,

888
01:02:56.185 –> 01:03:00.085
I think oh three LA students has been

889
01:03:03.445 –> 01:03:05.375
crossed under the tanks

890
01:03:06.035 –> 01:03:09.135
and it’s called the man square massacre.

891
01:03:09.815 –> 01:03:12.555
Mm. In 1989, it happened

892
01:03:17.715 –> 01:03:19.215
not too much far away time.

893
01:03:20.325 –> 01:03:21.585
The, the man square,

894
01:03:21.775 –> 01:03:24.265
they had three left student has been crossed

895
01:03:24.285 –> 01:03:25.785
to death by tanks.

896
01:03:26.975 –> 01:03:29.185
Whether it is 1989, I’m not sure.

897
01:03:30.325 –> 01:03:33.225
Yes, sir. For pro, pro democracy protest,

898
01:03:34.075 –> 01:03:36.145
Which year it is 1989. Yeah.

899
01:03:36.405 –> 01:03:38.385
Uh, so look At, and now you see,

900
01:03:38.845 –> 01:03:41.225
And now if you see that after that, see,

901
01:03:41.225 –> 01:03:46.065
after that this America, western media, they have,

902
01:03:47.005 –> 01:03:51.695
uh, raised this bogey against China,

903
01:03:52.445 –> 01:03:56.695
that China is going back to, uh, uh, what is that called?

904
01:03:58.995 –> 01:04:01.525
Pre-human era and all, all this and whatever they can say.

905
01:04:03.415 –> 01:04:06.755
But within a span of 30 years, China has emerged

906
01:04:06.975 –> 01:04:08.635
to a strong contender of us.

907
01:04:12.595 –> 01:04:14.975
In 1944,

908
01:04:15.995 –> 01:04:18.285
when the second World War was in the peak,

909
01:04:18.815 –> 01:04:22.005
China was under the occupation of Japan, the eastern China,

910
01:04:22.385 –> 01:04:25.285
the most prosperous cities of China, adding the eastern side

911
01:04:25.285 –> 01:04:28.735
of China, Eastern in the south China.

912
01:04:28.755 –> 01:04:30.415
See, if you look at the Vietnam side,

913
01:04:32.475 –> 01:04:34.205
Vietnam and this side, right?

914
01:04:34.725 –> 01:04:36.765
Thailand, Cambodia, all those area,

915
01:04:37.625 –> 01:04:40.315
Eastern China is more economically prosperous

916
01:04:40.385 –> 01:04:41.515
than the Western China.

917
01:04:41.515 –> 01:04:45.775
Western China is basically Mongolia side, not part of China.

918
01:04:45.875 –> 01:04:49.635
Mon separate country. So China is a very past country.

919
01:04:49.865 –> 01:04:51.555
This is three times bigger than us,

920
01:04:53.265 –> 01:04:55.885
but they have done it very well.

921
01:04:56.355 –> 01:04:59.755
Whereas we are lagging further going down

922
01:05:00.265 –> 01:05:03.595
because of all constitutional

923
01:05:04.345 –> 01:05:06.035
reservation classism.

924
01:05:11.665 –> 01:05:16.175
Okay? So you go through this, uh, uh,

925
01:05:17.395 –> 01:05:21.605
blog, very, very well written blog by Lillian Wayne

926
01:05:22.225 –> 01:05:25.445
for every topic of machine learning to deep learning to

927
01:05:27.305 –> 01:05:29.545
LLM, everything will get it under this

928
01:05:29.545 –> 01:05:31.065
person’s Lillian Wayne.

929
01:05:35.095 –> 01:05:39.305
Okay? So we can, it’s specifically for policy gradient.

930
01:05:39.565 –> 01:05:43.585
So not this one is for policy gradient alone. Yes.

931
01:05:46.175 –> 01:05:47.865
This is for policy grad alone. Okay.

932
01:05:49.585 –> 01:05:51.765
You can type in Google the topic name.

933
01:05:51.765 –> 01:05:55.695
Lillian Wayne, you will get everything. Yes.

934
01:05:59.145 –> 01:06:00.625
Whatever. I have spoken in my mouth, right?

935
01:06:00.825 –> 01:06:02.105
I am reading their things.

936
01:06:02.105 –> 01:06:04.065
Then I’m explaining from their side on,

937
01:06:06.975 –> 01:06:09.305
okay, so we can end into this class.

938
01:06:11.745 –> 01:06:16.205
Yes, sure. Thank you. Thank you, sir. Thank you, sir.

939
01:06:16.735 –> 01:06:20.745
Thank you, sir. Thank you. Thank you.

[vimeo - [https://vimeo.com/1159504299?fl=pl&fe=vl](https://vimeo.com/1159504299?fl=pl&fe=vl)](https://www.notion.so/vimeo-https-vimeo-com-1159504299-fl-pl-fe-vl-30161edeb739802197cce118b8f05948?pvs=21)

---

# CLASS NOTES: RL01 - REINFORCE Algorithm

## Metadata

- **Video URL**: [https://vimeo.com/1159504299](https://vimeo.com/1159504299)
- **Duration**: 66:20 (MM:SS)
- **Date**: 28th & 31st January 2026
- **Topic**: Policy Gradient Methods - REINFORCE Algorithm
- **Environment**: CartPole (OpenAI Gym)
- **Framework**: PyTorch
- **Total Visuals Identified**: 21

---

## 1. Visual Index

| ID | Timestamp | Type | Description | Priority |
| --- | --- | --- | --- | --- |
| V1 | 12:15 | Website | Hugging Face Deep RL tutorial | High |
| V2 | 14:00 | Table | Value-based vs Policy-based comparison | High |
| V3 | 20:45 | Grid Diagram | Frozen Lake 4x4 with policy arrows | High |
| V4 | 26:00 | Diagram | Episode/Trajectory structure (τ) | High |
| V5 | 32:00 | Math | Policy gradient theorem derivation | High |
| V6 | 34:39 | Blog | Lillian Weng's blog on policy gradients | High |
| V7 | 38:31 | Pseudocode | REINFORCE algorithm steps | High |
| V8 | 40:27 | Code | PyTorch implementation | High |

---

## 2. Key Visual Reconstructions

### V2: Value-Based vs Policy-Based Methods (14:00)

**Comparison Table:**

| Aspect | Value-Based (DQN) | Policy-Based (PG) |
| --- | --- | --- |
| **Approach** | Learn Q(s,a) | Directly learn π(a |
| **Networks** | Policy + Target | Single policy network |
| **Target** | TD learning value | No separate target |
| **Objective** | Minimize TD error | Maximize expected return |
| **Update** | Gradient descent | Gradient ascent |

### V3: Frozen Lake Grid World (20:45)

```
Frozen Lake 4x4 Environment
┌────┬────┬────┬────┐
│  0 │  1 │  2 │  3 │  S = Start
│  ↓ │  → │  ↓ │  ← │  G = Goal
├────┼────┼────┼────┤  H = Hole
│  3 │  H │  5 │  H │
│  ← │    │  ↓ │    │  Policy shown
├────┼────┼────┼────┤  as arrows
│  6 │  7 │  8 │  9 │
│  → │  ↓ │  → │  ← │
├────┼────┼────┼────┤
│ 10 │ 11 │ 12 │  G │
│  → │  → │  ↓ │  ★ │
└────┴────┴────┴────┘
```

**Key Point**: Slippery environment - intended action succeeds ~33% of the time

### V4: Episode/Trajectory Structure (26:00)

**Episode τ:**

```
s₀ ─[a₀]→ s₁ ─[a₁]→ s₂ ─[a₂]→ ... ─→ s_T
 │       │       │              │
 r₀      r₁      r₂            r_T
```

**Multiple trajectories under same policy:**

- τ₁: Different path due to stochasticity → R(τ₁)
- τ₂: Another random path → R(τ₂)
- τ₃: Yet another path → R(τ₃)

**Expected return:** 𝔼[R(τ)] = average over all possible trajectories

### V5: Policy Gradient Objective (32:00)

**Objective Function:**

```
J(θ) = 𝔼_τ~πθ [R(τ)]
     = 𝔼_τ~πθ [∑_{t=0}^T γ^t r_t]

Goal: Maximize J(θ) via gradient ascent
      θ ← θ + α ∇_θ J(θ)
```

**Policy Gradient Theorem:**

```
∇_θ J(θ) = 𝔼_τ [∑_{t=0}^T ∇_θ log π_θ(a_t|s_t) · G_t]

where G_t = ∑_{k=t}^T γ^{k-t} r_k  (return from time t)
```

### V7: REINFORCE Algorithm (38:31)

**Algorithm Steps:**

1. **Initialize** policy parameters θ randomly
2. **For each episode:**
    - Generate trajectory τ = (s₀, a₀, r₀, ..., s_T) using π_θ
3. **For each time step t in episode:**
    - Calculate return: G_t = ∑_{k=t}^T γ^{k-t} r_k
4. **Update policy parameters:**
    
    ```
    θ ← θ + α ∑_{t=0}^T ∇_θ log π_θ(a_t|s_t) · G_t
    ```
    

**Key Insight**: REINFORCE uses Monte Carlo estimation (complete episodes)

### V8: PyTorch Implementation (40:27)

**Policy Network Architecture:**

```python
class Policy(nn.Module):
    def __init__(self, state_dim=4, hidden_dim=64, action_dim=2):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        action_probs = F.softmax(self.fc2(x), dim=-1)
        return action_probs
```

**Key Code Components:**

1. **Action Selection:**

```python
probs = policy(state)
dist = torch.distributions.Categorical(probs)
action = dist.sample()
log_prob = dist.log_prob(action)
```

1. **Return Calculation:**

```python
G_t = sum(gamma**k * rewards[t+k]
          for k in range(len(rewards)-t))
```

1. **Policy Loss:**

```python
loss = -log_prob * G_t  # Negative for gradient ascent
loss.backward()
optimizer.step()
```

---

## 3. Key Takeaways

1. **Policy Gradient vs Value-Based**: PG directly optimizes policy, no need for value function
2. **REINFORCE = Monte Carlo PG**: Uses complete episodes to estimate gradients
3. **Core Idea**: Increase probability of actions that lead to high returns
4. **Challenges**: High variance, slow convergence (addressed by Actor-Critic later)
5. **CartPole Environment**: 4D state space → 2 possible actions (left/right)

---

## 4. Resources

- **Hugging Face Tutorial**: Policy Gradient with PyTorch
- **Lillian Weng's Blog**: [Policy Gradient Algorithms](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)
- **Environment**: CartPole-v1 (OpenAI Gym)

---

## 5. Next Steps

- **Next Class**: Actor-Critic Methods (combines policy gradient + value function)
- **Then**: Proximal Policy Optimization (PPO) - used in ChatGPT training